@article{Grosmann2001b,
abstract = {Autonomous mobile robots should be able to learn incrementally and adapt to changes in the operating environment during their entire lifetime. This is referred to as continual learning. In this thesis, I propose an approach to continual learning which is based on adaptive state-space quantisation and reinforcement learning. Representational tools for continual learning should be constructive, as the learning task is not known a priori, and allow the transfer of information between related tasks. The proposed learning method uses constructive neural networks and instance-based learning to compute a task-dependent agent-internal state space that can be adapted to new learning tasks easily. The continual learning method has been applied to a mobile robot that had to learn navigation tasks in different environments. This scenario is challenging as the performance feedback is sparse and the interaction with the environment is non-deterministic. I provide a performance comparison of model-based and model-free reinforcement learning algorithms for the robot navigation task and evaluate the capability of the learning system to adapt to changes in the operating environment. Although only experiments dealing with change to the environment are reported, the methods developed in the thesis are intended for more general multi-task-learning. It is hoped that the learning method will lead to a speed-up in applications of reinforcement learning and enable an autonomous robot to adapt its behaviour during its entire lifetime.},
author = {Gro{\ss}mann, Axel},
number = {February},
title = {{Continual Learning for Mobile Robots}},
year = {2001}
}
@phdthesis{lomonaco2019a,
abstract = {Humans have the extraordinary ability to learn continually from experience. Not only we can apply previously learned knowledge and skills to new situations, we can also use these as the foundation for later learning. One of the grand goals of Artificial Intelligence (AI) is building an artificial “continual learning” agent that constructs a sophisticated understanding of the world from its own experience through the autonomous incremental development of ever more complex knowledge and skills. However, despite early speculations and few pioneering works, very little research and effort has been devoted to address this vision. Current AI systems greatly suffer from the exposure to new data or environments which even slightly differ from the ones for which they have been trained for. Moreover, the learning process is usually constrained on fixed datasets within narrow and isolated tasks which may hardly lead to the emergence of more complex and autonomous intelligent behaviors. In essence, continual learning and adaptation capabilities, while more than often thought as fundamental pillars of every intelligent agent, have been mostly left out of the main AI research focus. In this dissertation, we study the application of these ideas in light of the more recent advances in machine learning research and in the context of deep architectures for AI. We propose a comprehensive and unifying framework for continual learning, new metrics, benchmarks and algorithms, as well as providing substantial experimental evaluations in different supervised, unsupervised and reinforcement learning tasks.},
author = {Lomonaco, Vincenzo},
doi = {Lomonaco, Vincenzo (2019) Continual Learning with Deep Architectures, [Dissertation thesis], Alma Mater Studiorum Università di Bologna. Dottorato di ricerca in Computer science and engineering <http://amsdottorato.unibo.it/view/dottorati/DOT536/>, 31 Ciclo. DOI 10.6092/unibo/amsdottorato/9073.},
keywords = {[framework]},
language = {it},
mendeley-tags = {[framework]},
month = {apr},
school = {alma},
title = {{Continual Learning with Deep Architectures}},
url = {http://amsdottorato.unibo.it/9073/},
year = {2019}
}
@phdthesis{fayek2019a,
abstract = {Machine learning is one of several approaches to artificial intelligence. It allows us to build machines that can learn from experience as opposed to being explicitly programmed. Current machine learning formulations are mostly designed for learning and performing a particular task from a tabula rasa using data available for that task. For machine learning to converge to artificial intelligence, in addition to other desiderata, it must be in a state of continual learning, i.e., have the ability to be in a continuous learning process, such that when a new task is presented, the system can leverage prior knowledge from prior tasks, in learning and performing this new task, and augment the prior knowledge with the newly acquired knowledge without having a significant adverse effect on the prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence. Deep learning is a powerful general-purpose approach to machine learning that is able to solve numerous and various tasks with minimal modification. Deep learning extends machine learning, and specially neural networks, to learn multiple levels of distributed representations together with the required mapping function into a single composite function. The emergence of deep learning and neural networks as a generic approach to machine learning, coupled with their ability to learn versatile hierarchical representations, has paved the way for continual learning. The main aim of this thesis is the study and development of a structured approach to continual learning, leveraging the success of deep learning and neural networks. This thesis studies the application of deep learning to a number of supervised learning tasks, and in particular, classification tasks in machine perception, e.g., image recognition, automatic speech recognition, and speech emotion recognition. The relation between the systems developed for these tasks is investigated to illuminate the layer-wise relevance of features in deep networks trained for these tasks via transfer learning, and these independent systems are unified into continual learning systems. The main contribution of this thesis is the construction and formulation of a deep learning framework, denoted progressive learning, that allows a holistic and systematic approach to continual learning. Progressive learning comprises a number of procedures that address the continual learning desiderata. It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using less amounts of data and a smaller number of dedicated parameters, for the tasks studied in this thesis, by accumulating and leveraging knowledge learned across tasks in a continuous manner. It is envisioned that progressive learning is a step towards a fully general continual learning framework.},
author = {Fayek, Haytham M.},
keywords = {[audio],[cifar],[imagenet],[sparsity]},
mendeley-tags = {[cifar],[imagenet],[audio],[sparsity]},
school = {RMIT University},
title = {{Continual Deep Learning via Progressive Learning}},
url = {http://researchbank.rmit.edu.au/eserv/rmit:162646/Fayek.pdf},
year = {2019}
}
