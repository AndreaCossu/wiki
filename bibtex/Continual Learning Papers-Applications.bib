@inproceedings{sun2019a,
abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...},
author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
booktitle = {ICLR},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {sep},
shorttitle = {LAMOL},
title = {{LAMOL: LAnguage MOdeling for Lifelong Language Learning}},
url = {https://openreview.net/forum?id=Skgxcn4YDS},
year = {2020}
}
@misc{lee2018a,
abstract = {While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.},
annote = {arXiv: 1712.09943},
author = {Lee, Sungjin},
booktitle = {arXiv:1712.09943 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,[nlp],chatbot,conversation,conversational agent,ewc,lstm},
mendeley-tags = {[nlp]},
month = {jan},
title = {{Toward Continual Learning for Conversational Agents}},
url = {http://arxiv.org/abs/1712.09943},
year = {2018}
}
@misc{kruszewski2020a,
abstract = {Continual Learning has been often framed as the problem of training a model in a sequence of tasks. In this regard, Neural Networks have been attested to forget the solutions to previous task as they learn new ones. Yet, modelling human life-long learning does not necessarily require any crisp notion of tasks. In this work, we propose a benchmark based on language modelling in a multilingual and multidomain setting that prescinds of any explicit delimitation of training examples into distinct tasks, and propose metrics to study continual learning and catastrophic forgetting in this setting. Then, we introduce a simple Product of Experts learning system that performs strongly on this problem while displaying interesting properties, and investigate its merits for avoiding forgetting.},
annote = {arXiv: 2004.03340},
author = {Kruszewski, Germ{\'{a}}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
booktitle = {arXiv:2004.03340 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,[nlp],expert,mixture},
mendeley-tags = {[nlp]},
month = {apr},
title = {{Class-Agnostic Continual Learning of Alternating Languages and Domains}},
url = {http://arxiv.org/abs/2004.03340},
year = {2020}
}
