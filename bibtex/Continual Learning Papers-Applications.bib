
@article{bapna2019,
  title = {Non-{{Parametric Adaptation}} for {{Neural Machine Translation}}},
  author = {Bapna, Ankur and Firat, Orhan},
  year = {2019},
  url = {http://arxiv.org/abs/1903.00058},
  abstract = {Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.},
  annotation = {\_eprint: 1903.00058},
  journal = {arXiv},
  keywords = {[nlp]}
}

@article{baweja2018,
  title = {Towards Continual Learning in Medical Imaging},
  author = {Baweja, Chaitanya and Glocker, Ben and Kamnitsas, Konstantinos},
  year = {2018},
  pages = {1--4},
  doi = {arXiv:1811.02496v1},
  url = {http://arxiv.org/abs/1811.02496},
  abstract = {This work investigates continual learning of two segmentation tasks in brain MRI with neural networks. To explore in this context the capabilities of current methods for countering catastrophic forgetting of the first task when a new one is learned, we investigate elastic weight consolidation, a recently proposed method based on Fisher information, originally evaluated on reinforcement learning of Atari games. We use it to sequentially learn segmentation of normal brain structures and then segmentation of white matter lesions. Our findings show this recent method reduces catastrophic forgetting, while large room for improvement exists in these challenging settings for continual learning.},
  annotation = {\_eprint: 1811.02496},
  journal = {NeurIPS Workshop on Continual Learning},
  keywords = {[vision]}
}

@article{campo2020,
  title = {Continual {{Learning}} of {{Predictive Models}} in {{Video Sequences}} via {{Variational Autoencoders}}},
  author = {Campo, Damian and Slavic, Giulia and Baydoun, Mohamad and Marcenaro, Lucio and Regazzoni, Carlo},
  year = {2020},
  url = {http://arxiv.org/abs/2006.01945},
  abstract = {This paper proposes a method for performing continual learning of predictive models that facilitate the inference of future frames in video sequences. For a first given experience, an initial Variational Autoencoder, together with a set of fully connected neural networks are utilized to respectively learn the appearance of video frames and their dynamics at the latent space level. By employing an adapted Markov Jump Particle Filter, the proposed method recognizes new situations and integrates them as predictive models avoiding catastrophic forgetting of previously learned tasks. For evaluating the proposed method, this article uses video sequences from a vehicle that performs different tasks in a controlled environment.},
  annotation = {\_eprint: 2006.01945},
  journal = {arXiv},
  keywords = {[vision]}
}

@inproceedings{dautume2019,
  title = {Episodic {{Memory}} in {{Lifelong Language Learning}}},
  booktitle = {{{NeurIPS}}},
  author = {D'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
  year = {2019},
  url = {http://arxiv.org/abs/1906.01076},
  abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({$\sim$}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
  annotation = {\_eprint: 1906.01076},
  keywords = {[nlp]}
}

@article{delange2020,
  title = {Unsupervised {{Model Personalization}} While {{Preserving Privacy}} and {{Scalability}}: {{An Open Problem}}},
  author = {De Lange, Matthias and Jia, Xu and Parisot, Sarah and Leonardis, Ales and Slabaugh, Gregory and Tuytelaars, Tinne},
  year = {2020},
  pages = {14451--14460},
  doi = {10.1109/cvpr42600.2020.01447},
  url = {http://arxiv.org/abs/2003.13296},
  abstract = {This work investigates the task of unsupervised model personalization, adapted to continually evolving, unlabeled local user images. We consider the practical scenario where a high capacity server interacts with a myriad of resource-limited edge devices, imposing strong requirements on scalability and local data privacy. We aim to address this challenge within the continual learning paradigm and provide a novel Dual User-Adaptation framework (DUA) to explore the problem. This framework flexibly disentangles user-adaptation into model personalization on the server and local data regularization on the user device, with desirable properties regarding scalability and privacy constraints. First, on the server, we introduce incremental learning of task-specific expert models, subsequently aggregated using a concealed unsupervised user prior. Aggregation avoids retraining, whereas the user prior conceals sensitive raw user data, and grants unsupervised adaptation. Second, local user-adaptation incorporates a domain adaptation point of view, adapting regularizing batch normalization parameters to the user data. We explore various empirical user configurations with different priors in categories and a tenfold of transforms for MIT Indoor Scene recognition, and classify numbers in a combined MNIST and SVHN setup. Extensive experiments yield promising results for data-driven local adaptation and elicit user priors for server adaptation to depend on the model rather than user data. Hence, although user-adaptation remains a challenging open problem, the DUA framework formalizes a principled foundation for personalizing both on server and user device, while maintaining privacy and scalability.},
  annotation = {\_eprint: 2003.13296},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  keywords = {[framework],[mnist],[vision]}
}

@article{fu2020,
  title = {Incremental {{Learning}} for {{End}}-to-{{End Automatic Speech Recognition}}},
  author = {Fu, Li and Li, Xiaoxiao and Zi, Libo},
  year = {2020},
  url = {https://arxiv.org/abs/2005.04288},
  abstract = {\textdagger{} We propose an incremental learning for end-to-end Automatic Speech Recognition (ASR) to extend the model's capacity on a new task while retaining the performance on existing ones. The proposed method is effective without accessing to the old dataset to address the issues of high training cost and old dataset unavailability. To achieve this, knowledge distillation is applied as a guidance to retain the recognition ability from the previous model, which is then combined with the new ASR task for model optimization. With an ASR model pre-trained on 12,000h Mandarin speech, we test our proposed method on 300h new scenario task and 1h new named entities task. Experiments show that our method yields 3.25\% and 0.88\% absolute Character Error Rate (CER) reduction on the new scenario, when compared with the pre-trained model and the full-data retraining baseline, respectively. It even yields a surprising 0.37\% absolute CER reduction on the new scenario than the fine-tuning. For the new named entities task, our method significantly improves the accuracy compared with the pre-trained model, i.e. 16.95\% absolute CER reduction. For both of the new task adaptions, the new models still maintain a same accuracy with the baseline on the old tasks.},
  journal = {arXiv},
  keywords = {[audio],end-to-end,incremental learning,Index Terms: automatic speech recognition,knowledge distillation}
}

@inproceedings{gupta2020,
  title = {Neural {{Topic Modeling}} with {{Continual Lifelong Learning}}},
  booktitle = {{{ICML}}},
  author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Sch{\"u}tze, Hinrich},
  year = {2020},
  url = {http://arxiv.org/abs/2006.10909},
  abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.},
  annotation = {\_eprint: 2006.10909},
  keywords = {[nlp]}
}

@inproceedings{hawkins2019,
  title = {Continual Adaptation for Efficient Machine Communication},
  booktitle = {Proceedings of the {{ICML Workshop}} on {{Adaptive}} \{\$\textbackslash backslash\$\&\} {{Multitask Learning}}: {{Algorithms}} \{\$\textbackslash backslash\$\&\} {{Systems}}},
  author = {Hawkins, Robert D and Kwon, Minae and Sadigh, Dorsa and Goodman, Noah D},
  year = {2019},
  url = {http://arxiv.org/abs/1911.09896},
  abstract = {To communicate with new partners in new contexts, humans rapidly form new linguistic conventions. Recent language models trained with deep neural networks are able to comprehend and produce the existing conventions present in their training data, but are not able to flexibly and interactively adapt those conventions on the fly as humans do. We introduce a repeated reference task as a benchmark for models of adaptation in communication and propose a regularized continual learning framework that allows an artificial agent initialized with a generic language model to more accurately and efficiently communicate with a partner over time. We evaluate this framework through simulations on COCO and in real-time reference game experiments with human partners.},
  annotation = {\_eprint: 1911.09896}
}

@incollection{kapoor2009,
  title = {Principles of {{Lifelong Learning}} for {{Predictive User Modeling}}},
  booktitle = {User {{Modeling}} 2007},
  author = {Kapoor, Ashish and Horvitz, Eric},
  year = {2009},
  pages = {37--46},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  issn = {03029743},
  doi = {10.1007/978-3-540-73078-1_7},
  url = {http://link.springer.com/10.1007/978-3-540-73078-1_7},
  isbn = {978-3-540-73077-4},
  pmid = {16717005}
}

@article{kiyasseh2020,
  title = {{{CLOPS}}: {{Continual Learning}} of {{Physiological Signals}}},
  author = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A},
  year = {2020},
  url = {http://arxiv.org/abs/2004.09578},
  abstract = {Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.},
  annotation = {\_eprint: 2004.09578},
  journal = {arXiv}
}

@article{kruszewski2020,
  title = {Class-{{Agnostic Continual Learning}} of {{Alternating Languages}} and {{Domains}}},
  author = {Kruszewski, Germ{\'a}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
  year = {2020},
  url = {http://arxiv.org/abs/2004.03340},
  abstract = {Continual Learning has been often framed as the problem of training a model in a sequence of tasks. In this regard, Neural Networks have been attested to forget the solutions to previous task as they learn new ones. Yet, modelling human life-long learning does not necessarily require any crisp notion of tasks. In this work, we propose a benchmark based on language modelling in a multilingual and multidomain setting that prescinds of any explicit delimitation of training examples into distinct tasks, and propose metrics to study continual learning and catastrophic forgetting in this setting. Then, we introduce a simple Product of Experts learning system that performs strongly on this problem while displaying interesting properties, and investigate its merits for avoiding forgetting.},
  journal = {arXiv},
  keywords = {[nlp],Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,expert,mixture},
  note = {arXiv: 2004.03340}
}

@article{lee2018,
  title = {Toward {{Continual Learning}} for {{Conversational Agents}}},
  author = {Lee, Sungjin},
  year = {2018},
  url = {http://arxiv.org/abs/1712.09943},
  abstract = {While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.},
  journal = {arXiv},
  keywords = {[nlp],chatbot,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,conversation,conversational agent,ewc,lstm},
  note = {arXiv: 1712.09943}
}

@article{lee2020,
  title = {Clinical Applications of Continual Learning Machine Learning},
  author = {Lee, Cecilia S and Lee, Aaron Y},
  year = {2020},
  volume = {2},
  pages = {e279--e281},
  issn = {25897500},
  doi = {10.1016/S2589-7500(20)30102-3},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2589750020301023},
  journal = {The Lancet Digital Health},
  number = {6}
}

@article{lenga2020,
  title = {Continual {{Learning}} for {{Domain Adaptation}} in {{Chest X}}-Ray {{Classification}}},
  author = {Lenga, Matthias and Schulz, Heinrich and Saalbach, Axel},
  year = {2020},
  pages = {1--11},
  url = {http://arxiv.org/abs/2001.05922},
  abstract = {Over the last years, Deep Learning has been successfully applied to a broad range of medical applications. Especially in the context of chest X-ray classification, results have been reported which are on par, or even superior to experienced radiologists. Despite this success in controlled experimental environments, it has been noted that the ability of Deep Learning models to generalize to data from a new domain (with potentially different tasks) is often limited. In order to address this challenge, we investigate techniques from the field of Continual Learning (CL) including Joint Training (JT), Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that these methods provide promising options to improve the performance of Deep Learning models on a target domain and to mitigate effectively catastrophic forgetting for the source domain. To this end, the best overall performance was obtained using JT, while for LWF competitive results could be achieved - even without accessing data from the source domain.},
  annotation = {\_eprint: 2001.05922},
  journal = {arXiv},
  keywords = {[vision],catastrophic forgetting,chest x-ray,chestx-ray14,continual learning,convolutional neural networks,elastic weight consolidation,joint training,learning without forgetting,mimic-cxr}
}

@inproceedings{liu2019,
  title = {Continual {{Learning}} for {{Sentence Representations Using Conceptors}}},
  booktitle = {{{NAACL}}},
  author = {Liu, Tianlin and Ungar, Lyle and Sedoc, Jo{\~a}o},
  year = {2019},
  url = {http://arxiv.org/abs/1904.09187},
  abstract = {Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.},
  annotation = {\_eprint: 1904.09187},
  keywords = {[nlp]}
}

@inproceedings{mazumder2019,
  title = {Lifelong and {{Interactive Learning}} of {{Factual Knowledge}} in {{Dialogues}}},
  booktitle = {Proceedings of the 20th {{Annual SIGdial Meeting}} on {{Discourse}} and {{Dialogue}}},
  author = {Mazumder, Sahisnu and Liu, Bing and Wang, Shuai and Ma, Nianzu},
  year = {2019},
  pages = {21--31},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}},
  doi = {10.18653/v1/W19-5903},
  url = {http://arxiv.org/abs/1907.13295 https://www.aclweb.org/anthology/W19-5903},
  abstract = {Dialogue systems are increasingly using knowledge bases (KBs) storing real-world facts to help generate quality responses. However, as the KBs are inherently incomplete and remain fixed during conversation, it limits dialogue systems' ability to answer questions and to handle questions involving entities or relations that are not in the KB. In this paper, we make an attempt to propose an engine for Continuous and Interactive Learning of Knowledge (CILK) for dialogue systems to give them the ability to continuously and interactively learn and infer new knowledge during conversations. With more knowledge accumulated over time, they will be able to learn better and answer more questions. Our empirical evaluation shows that CILK is promising.},
  annotation = {\_eprint: 1907.13295},
  keywords = {[nlp]}
}

@article{ozgun2020,
  title = {Importance {{Driven Continual Learning}} for {{Segmentation Across Domains}}},
  author = {{\"O}zg{\"u}n, Sinan {\"O}zg{\"u}r and Rickmann, Anne-Marie and Roy, Abhijit Guha and Wachinger, Christian},
  year = {2020},
  pages = {1--10},
  url = {http://arxiv.org/abs/2005.00079},
  abstract = {The ability of neural networks to continuously learn and adapt to new tasks while retaining prior knowledge is crucial for many applications. However, current neural networks tend to forget previously learned tasks when trained on new ones, i.e., they suffer from Catastrophic Forgetting (CF). The objective of Continual Learning (CL) is to alleviate this problem, which is particularly relevant for medical applications, where it may not be feasible to store and access previously used sensitive patient data. In this work, we propose a Continual Learning approach for brain segmentation, where a single network is consecutively trained on samples from different domains. We build upon an importance driven approach and adapt it for medical image segmentation. Particularly, we introduce learning rate regularization to prevent the loss of the network's knowledge. Our results demonstrate that directly restricting the adaptation of important network parameters clearly reduces Catastrophic Forgetting for segmentation across domains.},
  annotation = {\_eprint: 2005.00079},
  journal = {arXiv},
  keywords = {[vision]}
}

@inproceedings{sun2020,
  title = {{{LAMOL}}: {{LAnguage MOdeling}} for {{Lifelong Language Learning}}},
  shorttitle = {{{LAMOL}}},
  booktitle = {{{ICLR}}},
  author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
  year = {2020},
  url = {https://openreview.net/forum?id=Skgxcn4YDS},
  abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...},
  keywords = {[nlp]}
}

@inproceedings{thompson2019,
  title = {Overcoming {{Catastrophic Forgetting During Domain Adaptation}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},
  year = {2019},
  pages = {2062--2068},
  url = {https://www.aclweb.org/anthology/N19-1209.pdf},
  abstract = {Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)-a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable .},
  keywords = {[nlp]}
}

@article{xue2019,
  title = {A {{Multi}}-{{Task Learning Framework}} for {{Overcoming}} the {{Catastrophic Forgetting}} in {{Automatic Speech Recognition}}},
  author = {Xue, Jiabin and Han, Jiqing and Zheng, Tieran and Gao, Xiang and Guo, Jiaxing},
  year = {2019},
  url = {http://arxiv.org/abs/1904.08039},
  abstract = {Recently, data-driven based Automatic Speech Recognition (ASR) systems have achieved state-of-the-art results. And transfer learning is often used when those existing systems are adapted to the target domain, e.g., fine-tuning, retraining. However, in the processes, the system parameters may well deviate too much from the previously learned parameters. Thus, it is difficult for the system training process to learn knowledge from target domains meanwhile not forgetting knowledge from the previous learning process, which is called as catastrophic forgetting (CF). In this paper, we attempt to solve the CF problem with the lifelong learning and propose a novel multi-task learning (MTL) training framework for ASR. It considers reserving original knowledge and learning new knowledge as two independent tasks, respectively. On the one hand, we constrain the new parameters not to deviate too far from the original parameters and punish the new system when forgetting original knowledge. On the other hand, we force the new system to solve new knowledge quickly. Then, a MTL mechanism is employed to get the balance between the two tasks. We applied our method to an End2End ASR task and obtained the best performance in both target and original datasets.},
  annotation = {\_eprint: 1904.08039},
  journal = {arXiv},
  keywords = {[audio],[rnn]}
}

@article{zhai2019,
  title = {Lifelong {{Learning}} for {{Scene Recognition}} in {{Remote Sensing Images}}},
  author = {Zhai, Min and Liu, Huaping and Sun, Fuchun},
  year = {2019},
  volume = {16},
  pages = {1472--1476},
  publisher = {{IEEE}},
  issn = {1545-598X},
  doi = {10.1109/LGRS.2019.2897652},
  url = {https://ieeexplore.ieee.org/document/8662768/},
  abstract = {The development of visual sensing technologies has made it possible to obtain some high resolution and to gather many high-resolution satellite images. To make the best use of these images, it is essential to be able to recognize and retrieve their intrinsic scene information. The problem of scene recognition in remote sensing images has recently aroused considerable interest, mainly due to the great success achieved by deep learning methods in generic image classification. Nevertheless, such methods usually require large amounts of labeled data. By contrast, remote sensing images are relatively scarce and expensive to obtain. Moreover, data sets from different aerospace research institutions exhibit large disparities. In order to address these problems, we propose a model based on a meta-learning method with the ability of learning a classifier from just few-shot samples. With the proposed model, the knowledge learned from one data set can be easily adapted to a new data set, which, in turn, would serve in the lifelong few-shot learning. Scene-level image recognition experiments, on public high-resolution remote sensing image data sets, validate our proposed lifelong few-shot learning model.},
  journal = {IEEE Geoscience and Remote Sensing Letters},
  keywords = {[vision]},
  number = {9}
}


