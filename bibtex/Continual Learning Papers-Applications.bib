@article{Zhai2019a,
abstract = {The development of visual sensing technologies has made it possible to obtain some high resolution and to gather many high-resolution satellite images. To make the best use of these images, it is essential to be able to recognize and retrieve their intrinsic scene information. The problem of scene recognition in remote sensing images has recently aroused considerable interest, mainly due to the great success achieved by deep learning methods in generic image classification. Nevertheless, such methods usually require large amounts of labeled data. By contrast, remote sensing images are relatively scarce and expensive to obtain. Moreover, data sets from different aerospace research institutions exhibit large disparities. In order to address these problems, we propose a model based on a meta-learning method with the ability of learning a classifier from just few-shot samples. With the proposed model, the knowledge learned from one data set can be easily adapted to a new data set, which, in turn, would serve in the lifelong few-shot learning. Scene-level image recognition experiments, on public high-resolution remote sensing image data sets, validate our proposed lifelong few-shot learning model.},
author = {Zhai, Min and Liu, Huaping and Sun, Fuchun},
doi = {10.1109/LGRS.2019.2897652},
issn = {1545-598X},
journal = {IEEE Geoscience and Remote Sensing Letters},
keywords = {[vision]},
mendeley-tags = {[vision]},
month = {sep},
number = {9},
pages = {1472--1476},
publisher = {IEEE},
title = {{Lifelong Learning for Scene Recognition in Remote Sensing Images}},
url = {https://ieeexplore.ieee.org/document/8662768/},
volume = {16},
year = {2019}
}
@article{Lenga2020a,
abstract = {Over the last years, Deep Learning has been successfully applied to a broad range of medical applications. Especially in the context of chest X-ray classification, results have been reported which are on par, or even superior to experienced radiologists. Despite this success in controlled experimental environments, it has been noted that the ability of Deep Learning models to generalize to data from a new domain (with potentially different tasks) is often limited. In order to address this challenge, we investigate techniques from the field of Continual Learning (CL) including Joint Training (JT), Elastic Weight Consolidation (EWC) and Learning Without Forgetting (LWF). Using the ChestX-ray14 and the MIMIC-CXR datasets, we demonstrate empirically that these methods provide promising options to improve the performance of Deep Learning models on a target domain and to mitigate effectively catastrophic forgetting for the source domain. To this end, the best overall performance was obtained using JT, while for LWF competitive results could be achieved - even without accessing data from the source domain.},
archivePrefix = {arXiv},
arxivId = {2001.05922},
author = {Lenga, Matthias and Schulz, Heinrich and Saalbach, Axel},
eprint = {2001.05922},
journal = {arXiv},
keywords = {catastrophic forgetting,chest x-ray,chestx-ray14,continual learning,convolutional neural networks,elastic weight consolidation,joint training,learning without forgetting,mimic-cxr},
pages = {1--11},
title = {{Continual Learning for Domain Adaptation in Chest X-ray Classification}},
url = {http://arxiv.org/abs/2001.05922},
year = {2020}
}
@article{Liu2019a,
abstract = {Distributed representations of sentences have become ubiquitous in natural language processing tasks. In this paper, we consider a continual learning scenario for sentence representations: Given a sequence of corpora, we aim to optimize the sentence encoder with respect to the new corpus while maintaining its accuracy on the old corpora. To address this problem, we propose to initialize sentence encoders with the help of corpus-independent features, and then sequentially update sentence encoders using Boolean operations of conceptor matrices to learn corpus-dependent features. We evaluate our approach on semantic textual similarity tasks and show that our proposed sentence encoder can continually learn features from new corpora while retaining its competence on previously encountered corpora.},
archivePrefix = {arXiv},
arxivId = {1904.09187},
author = {Liu, Tianlin and Ungar, Lyle and Sedoc, Jo{\~{a}}o},
eprint = {1904.09187},
journal = {NAACL},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
title = {{Continual Learning for Sentence Representations Using Conceptors}},
url = {http://arxiv.org/abs/1904.09187},
year = {2019}
}
@inproceedings{Thompson2019a,
abstract = {Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)-a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable .},
author = {Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp},
booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics},
file = {::},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
pages = {2062--2068},
title = {{Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation}},
url = {https://www.aclweb.org/anthology/N19-1209.pdf},
year = {2019}
}
@article{DAutume2019a,
abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ($\sim$50-90%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
archivePrefix = {arXiv},
arxivId = {1906.01076},
author = {D'Autume, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
eprint = {1906.01076},
journal = {NeurIPS},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {jun},
title = {{Episodic Memory in Lifelong Language Learning}},
url = {http://arxiv.org/abs/1906.01076},
year = {2019}
}
@article{Kiyasseh2020,
abstract = {Deep learning algorithms are known to experience destructive interference when instances violate the assumption of being independent and identically distributed (i.i.d). This violation, however, is ubiquitous in clinical settings where data are streamed temporally and from a multitude of physiological sensors. To overcome this obstacle, we propose CLOPS, a healthcare-specific replay-based continual learning strategy. In three continual learning scenarios based on three publically-available datasets, we show that CLOPS can outperform its multi-task learning counterpart. Moreover, we propose end-to-end trainable parameters, which we term task-instance parameters, that can be used to quantify task difficulty and similarity. This quantification yields insights into both network interpretability and clinical applications, where task difficulty is poorly quantified.},
archivePrefix = {arXiv},
arxivId = {2004.09578},
author = {Kiyasseh, Dani and Zhu, Tingting and Clifton, David A},
eprint = {2004.09578},
journal = {arXiv},
title = {{CLOPS: Continual Learning of Physiological Signals}},
url = {http://arxiv.org/abs/2004.09578},
year = {2020}
}
@article{Xue2019a,
abstract = {Recently, data-driven based Automatic Speech Recognition (ASR) systems have achieved state-of-the-art results. And transfer learning is often used when those existing systems are adapted to the target domain, e.g., fine-tuning, retraining. However, in the processes, the system parameters may well deviate too much from the previously learned parameters. Thus, it is difficult for the system training process to learn knowledge from target domains meanwhile not forgetting knowledge from the previous learning process, which is called as catastrophic forgetting (CF). In this paper, we attempt to solve the CF problem with the lifelong learning and propose a novel multi-task learning (MTL) training framework for ASR. It considers reserving original knowledge and learning new knowledge as two independent tasks, respectively. On the one hand, we constrain the new parameters not to deviate too far from the original parameters and punish the new system when forgetting original knowledge. On the other hand, we force the new system to solve new knowledge quickly. Then, a MTL mechanism is employed to get the balance between the two tasks. We applied our method to an End2End ASR task and obtained the best performance in both target and original datasets.},
archivePrefix = {arXiv},
arxivId = {1904.08039},
author = {Xue, Jiabin and Han, Jiqing and Zheng, Tieran and Gao, Xiang and Guo, Jiaxing},
eprint = {1904.08039},
journal = {arXiv},
keywords = {[audio],[rnn]},
mendeley-tags = {[audio],[rnn]},
title = {{A Multi-Task Learning Framework for Overcoming the Catastrophic Forgetting in Automatic Speech Recognition}},
url = {http://arxiv.org/abs/1904.08039},
year = {2019}
}
@inproceedings{Pankaj2020a,
abstract = {Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel selective data augmentation, co-training and topic regularization approaches. Given a stream of document collections, we apply the proposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three sparse document collections as future tasks and demonstrate improved performance quantified by perplexity, topic coherence and information retrieval task.},
archivePrefix = {arXiv},
arxivId = {2006.10909},
author = {Gupta, Pankaj and Chaudhary, Yatin and Runkler, Thomas and Sch{\"{u}}tze, Hinrich},
booktitle = {ICML},
eprint = {2006.10909},
file = {::},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {jun},
title = {{Neural Topic Modeling with Continual Lifelong Learning}},
url = {http://arxiv.org/abs/2006.10909},
year = {2020}
}
@article{kruszewski2020a,
abstract = {Continual Learning has been often framed as the problem of training a model in a sequence of tasks. In this regard, Neural Networks have been attested to forget the solutions to previous task as they learn new ones. Yet, modelling human life-long learning does not necessarily require any crisp notion of tasks. In this work, we propose a benchmark based on language modelling in a multilingual and multidomain setting that prescinds of any explicit delimitation of training examples into distinct tasks, and propose metrics to study continual learning and catastrophic forgetting in this setting. Then, we introduce a simple Product of Experts learning system that performs strongly on this problem while displaying interesting properties, and investigate its merits for avoiding forgetting.},
annote = {arXiv: 2004.03340},
author = {Kruszewski, Germ{\'{a}}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
journal = {arXiv},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,[nlp],expert,mixture},
mendeley-tags = {[nlp]},
month = {apr},
title = {{Class-Agnostic Continual Learning of Alternating Languages and Domains}},
url = {http://arxiv.org/abs/2004.03340},
year = {2020}
}
@article{Bapna2019a,
abstract = {Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The semi-parametric nature of our approach opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.},
archivePrefix = {arXiv},
arxivId = {1903.00058},
author = {Bapna, Ankur and Firat, Orhan},
eprint = {1903.00058},
journal = {arXiv},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
title = {{Non-Parametric Adaptation for Neural Machine Translation}},
url = {http://arxiv.org/abs/1903.00058},
year = {2019}
}
@inproceedings{sun2019a,
abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...},
author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
booktitle = {ICLR},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {sep},
shorttitle = {LAMOL},
title = {{LAMOL: LAnguage MOdeling for Lifelong Language Learning}},
url = {https://openreview.net/forum?id=Skgxcn4YDS},
year = {2020}
}
@article{Fu2020a,
abstract = {† We propose an incremental learning for end-to-end Automatic Speech Recognition (ASR) to extend the model's capacity on a new task while retaining the performance on existing ones. The proposed method is effective without accessing to the old dataset to address the issues of high training cost and old dataset unavailability. To achieve this, knowledge distillation is applied as a guidance to retain the recognition ability from the previous model, which is then combined with the new ASR task for model optimization. With an ASR model pre-trained on 12,000h Mandarin speech, we test our proposed method on 300h new scenario task and 1h new named entities task. Experiments show that our method yields 3.25% and 0.88% absolute Character Error Rate (CER) reduction on the new scenario, when compared with the pre-trained model and the full-data retraining baseline, respectively. It even yields a surprising 0.37% absolute CER reduction on the new scenario than the fine-tuning. For the new named entities task, our method significantly improves the accuracy compared with the pre-trained model, i.e. 16.95% absolute CER reduction. For both of the new task adaptions, the new models still maintain a same accuracy with the baseline on the old tasks.},
author = {Fu, Li and Li, Xiaoxiao and Zi, Libo},
file = {::},
journal = {arXiv},
keywords = {Index Terms: automatic speech recognition,[audio],end-to-end,incremental learning,knowledge distillation},
mendeley-tags = {[audio]},
title = {{Incremental Learning for End-to-End Automatic Speech Recognition}},
url = {https://arxiv.org/abs/2005.04288},
year = {2020}
}
@article{Campo2020a,
abstract = {This paper proposes a method for performing continual learning of predictive models that facilitate the inference of future frames in video sequences. For a first given experience, an initial Variational Autoencoder, together with a set of fully connected neural networks are utilized to respectively learn the appearance of video frames and their dynamics at the latent space level. By employing an adapted Markov Jump Particle Filter, the proposed method recognizes new situations and integrates them as predictive models avoiding catastrophic forgetting of previously learned tasks. For evaluating the proposed method, this article uses video sequences from a vehicle that performs different tasks in a controlled environment.},
archivePrefix = {arXiv},
arxivId = {2006.01945},
author = {Campo, Damian and Slavic, Giulia and Baydoun, Mohamad and Marcenaro, Lucio and Regazzoni, Carlo},
eprint = {2006.01945},
file = {::},
journal = {arXiv},
keywords = {[vision]},
mendeley-tags = {[vision]},
month = {jun},
title = {{Continual Learning of Predictive Models in Video Sequences via Variational Autoencoders}},
url = {http://arxiv.org/abs/2006.01945},
year = {2020}
}
@article{lee2018a,
abstract = {While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.},
annote = {arXiv: 1712.09943},
author = {Lee, Sungjin},
journal = {arXiv},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,[nlp],chatbot,conversation,conversational agent,ewc,lstm},
mendeley-tags = {[nlp]},
month = {jan},
title = {{Toward Continual Learning for Conversational Agents}},
url = {http://arxiv.org/abs/1712.09943},
year = {2018}
}
