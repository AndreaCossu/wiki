@misc{Caccia2020a,
abstract = {Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.},
annote = {arXiv: 2003.05856},
author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},
booktitle = {arXiv:2003.05856 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,MAML,OSAKA,[fashion],[framework],[mnist],continual meta learning,framework,meta continual learning},
mendeley-tags = {[fashion],[framework],[mnist]},
month = {mar},
shorttitle = {Online Fast Adaptation and Knowledge Accumulation},
title = {{Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning}},
url = {http://arxiv.org/abs/2003.05856},
year = {2020}
}
@article{Parisi2019a,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
annote = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.},
author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation,[framework]},
language = {en},
mendeley-tags = {[framework]},
month = {may},
pages = {54--71},
shorttitle = {Continual lifelong learning with neural networks},
title = {{Continual lifelong learning with neural networks: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},
volume = {113},
year = {2019}
}
@article{Lesort2020a,
abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.},
annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},
author = {Lesort, Timoth{\'{e}}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and D{\'{i}}az-Rodr{\'{i}}guez, Natalia},
doi = {10.1016/j.inffus.2019.12.004},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,[framework]},
language = {en},
mendeley-tags = {[framework]},
month = {jun},
pages = {52--68},
shorttitle = {Continual learning for robotics},
title = {{Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges}},
url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},
volume = {58},
year = {2020}
}
@misc{Lesort2020b,
abstract = {In most machine learning algorithms, training data are assumed independent and identically distributed (iid). Otherwise, the algorithms' performances are challenged. A famous phenomenon with non-iid data distribution is known as \say{catastrophic forgetting}. Algorithms dealing with it are gathered in the \textit{Continual Learning} research field. In this article, we study the \textit{regularization} based approaches to continual learning. We show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: class-incremental setting. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments.},
annote = {arXiv: 1912.03049},
author = {Lesort, Timoth{\'{e}}e and Stoian, Andrei and Filliat, David},
booktitle = {arXiv:1912.03049 [cs, stat]},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[fashion],[mnist],class incremental,regularization},
mendeley-tags = {[fashion],[mnist]},
month = {feb},
title = {{Regularization Shortcomings for Continual Learning}},
url = {http://arxiv.org/abs/1912.03049},
year = {2020}
}
@inproceedings{Farquhar2019a,
abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
annote = {arXiv: 1805.09733},
author = {Farquhar, Sebastian and Gal, Yarin},
booktitle = {Privacy in Machine Learning and Artificial Intelligence workshop, ICML},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[fashion],[framework],critique,evaluation,metrics},
mendeley-tags = {[fashion],[framework]},
month = {jun},
title = {{Towards Robust Evaluations of Continual Learning}},
url = {http://arxiv.org/abs/1805.09733},
year = {2019}
}
@inproceedings{VanDeVen2018a,
abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
annote = {Comment: Extended version of work presented at the NeurIPS Continual Learning workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635
arXiv: 1904.07734},
author = {van de Ven, Gido M and Tolias, Andreas S},
booktitle = {Continual Learning Workshop NeurIPS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning,[framework],[mnist]},
mendeley-tags = {[framework],[mnist]},
title = {{Three scenarios for continual learning}},
url = {http://arxiv.org/abs/1904.07734},
year = {2018}
}
