@article{harrison2019a,
abstract = {Meta-learning is a promising strategy for learning to efficiently learn within new tasks, using data gathered from a distribution of tasks. However, the meta-learning literature thus far has focused on the task segmented setting, where at train-time, offline data is assumed to be split according to the underlying task, and at test-time, the algorithms are optimized to learn in a single task. In this work, we enable the application of generic meta-learning algorithms to settings where this task segmentation is unavailable, such as continual online learning with a time-varying task. We present meta-learning via online changepoint analysis (MOCA), an approach which augments a meta-learning algorithm with a differentiable Bayesian changepoint detection scheme. The framework allows both training and testing directly on time series data without segmenting it into discrete tasks. We demonstrate the utility of this approach on a nonlinear meta-regression benchmark as well as two meta-image-classification benchmarks.},
archivePrefix = {arXiv},
arxivId = {1912.08866},
author = {Harrison, James and Sharma, Apoorva and Finn, Chelsea and Pavone, Marco},
eprint = {1912.08866},
journal = {arXiv},
keywords = {[imagenet],[mnist]},
mendeley-tags = {[imagenet],[mnist]},
title = {{Continuous meta-learning without tasks}},
url = {https://arxiv.org/abs/1912.08866},
year = {2019}
}
@misc{He2019a,
abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering â€“ i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.},
annote = {arXiv: 1906.05201},
author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
booktitle = {arXiv:1906.05201 [cs, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist]},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Task Agnostic Continual Learning via Meta Learning}},
url = {http://arxiv.org/abs/1906.05201},
year = {2019}
}
@article{finn2019a,
abstract = {A central capability of intelligent systems is the ability to continuously build upon previous experiences to speed up and enhance learning of new tasks. Two distinct research paradigms have studied this question. Meta-learning views this problem as learning a prior over model parameters that is amenable for fast adaptation on a new task, but typically assumes the set of tasks are available together as a batch. In contrast, online (regret based) learning considers a sequential setting in which problems are revealed one after the other, but conventionally train only a single model without any task-specific adaptation. This work introduces an online meta-learning setting, which merges ideas from both the aforementioned paradigms to better capture the spirit and practice of continual lifelong learning. We propose the follow the meta leader (FTML) algorithm which extends the MAML algorithm to this setting. Theoretically, this work provides an O(log T) regret guarantee with one additional higher order smoothness assumption (in comparison to the standard online setting). Our experimental evaluation on three different large-scale tasks suggest that the proposed algorithm significantly outperforms alternatives based on traditional online learning approaches.},
archivePrefix = {arXiv},
arxivId = {1902.08438},
author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
eprint = {1902.08438},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Online meta-learning}},
url = {https://arxiv.org/abs/1902.08438},
year = {2019}
}
@article{caccia2020a,
abstract = {Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.},
archivePrefix = {arXiv},
arxivId = {2003.05856},
author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},
eprint = {2003.05856},
file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Caccia et al. - 2020 - Online Fast Adaptation and Knowledge Accumulation a New Approach to Continual Learning(2).pdf:pdf},
journal = {arXiv},
keywords = {[fashion],[framework],[mnist]},
mendeley-tags = {[fashion],[framework],[mnist]},
month = {mar},
title = {{Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning}},
url = {http://arxiv.org/abs/2003.05856},
year = {2020}
}
@inproceedings{jerfel2019a,
abstract = {Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.},
author = {Jerfel, Ghassen and Grant, Erin and Griffiths, Tom and Heller, Katherine A},
booktitle = {Advances in Neural Information Processing Systems},
keywords = {[bayes],[vision]},
mendeley-tags = {[bayes],[vision]},
pages = {9122--9133},
title = {{Reconciling meta-learning and continual learning with online mixtures of tasks}},
url = {http://papers.nips.cc/paper/9112-reconciling-meta-learning-and-continual-learning-with-online-mixtures-of-tasks},
year = {2019}
}
