@inproceedings{Li2016a,
abstract = {When building a uniﬁed vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and ﬁne-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace ﬁne-tuning with similar old and new task datasets for improved new task performance.},
annote = {Comment: Conference version appears in ECCV 2016; updated with journal version
arXiv: 1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
booktitle = {European Conference on Computer Vision},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning,[imagenet]},
language = {en},
mendeley-tags = {[imagenet]},
pages = {614--629},
series = {Springer},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@inproceedings{Lee2017a,
abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
archivePrefix = {arXiv},
arxivId = {1703.08475},
author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1703.08475},
file = {::},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {mar},
pages = {4653--4663},
publisher = {Neural information processing systems foundation},
title = {{Overcoming Catastrophic Forgetting by Incremental Moment Matching}},
url = {http://arxiv.org/abs/1703.08475},
volume = {2017-Decem},
year = {2017}
}
@article{Ritter2018a,
abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90{\%} test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
archivePrefix = {arXiv},
arxivId = {1805.07810},
author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
eprint = {1805.07810},
journal = {arXiv},
keywords = {[bayes],[mnist]},
mendeley-tags = {[bayes],[mnist]},
title = {{Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting}},
url = {http://arxiv.org/abs/1805.07810},
year = {2018}
}
@inproceedings{Liu2018a,
abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
annote = {ISSN: 1051-4651},
author = {Liu, Xialei and Masana, Marc and Herranz, Luis and de Weijer, Joost and L{\'{o}}pez, Antonio M and Bagdanov, Andrew D},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545895},
keywords = {Computer vision,Data models,Fisher Information Matrix,Neural networks,Standards,Stanford-40 datasets,Task analysis,Training,Training data,[cifar],[mnist],ewc,fisher,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,sequential tasks,standard elastic weight consolidation},
mendeley-tags = {[cifar],[mnist]},
month = {aug},
pages = {2262--2268},
shorttitle = {Rotate your Networks},
title = {{Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting}},
year = {2018}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},
mendeley-tags = {[mnist]},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@inproceedings{Oswal2019a,
abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...},
author = {von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~{a}}o and Grewe, Benjamin F},
booktitle = {International Conference on Learning Representations},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {sep},
title = {{Continual learning with hypernetworks}},
url = {https://openreview.net/forum?id=SJgwNerKvB},
year = {2020}
}
@article{Ramapuram2017a,
abstract = {Lifelong learning is the problem of learning multiple consecutive tasks in an online manner and is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on learning a lifelong approach to generative modeling whereby we continuously incorporate newly observed distributions into our model representation. We utilize two models, aptly named the student and the teacher, in order to aggregate information about all past distributions without the preservation of any of the past data or previous models. The teacher is utilized as a form of compressed memory in order to allow for the student model to learn over the past as well as present data. We demonstrate why a naive approach to lifelong generative modeling fails and introduce a regularizer with which we demonstrate learning across a long range of distributions.},
archivePrefix = {arXiv},
arxivId = {1705.09847},
author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},
eprint = {1705.09847},
journal = {arXiv},
keywords = {[fashion],[generative],[mnist]},
mendeley-tags = {[fashion],[generative],[mnist]},
number = {2010},
pages = {1--14},
title = {{Lifelong Generative Modeling}},
url = {http://arxiv.org/abs/1705.09847},
year = {2017}
}
@article{Parshotam2020a,
abstract = {We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.},
archivePrefix = {arXiv},
arxivId = {2004.10862},
author = {Parshotam, Kishan and Kilickaya, Mert},
eprint = {2004.10862},
journal = {CVPR 2020: Workshop on Continual Learning in Computer Vision},
keywords = {[vision]},
mendeley-tags = {[vision]},
title = {{Continual Learning of Object Instances}},
url = {http://arxiv.org/abs/2004.10862},
year = {2020}
}
@article{Pomponi2020a,
abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
doi = {10.1016/j.neucom.2020.01.093},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions,[cifar],[mnist]},
language = {en},
mendeley-tags = {[cifar],[mnist]},
month = {feb},
title = {{Efficient continual learning in neural networks with embedding regularization}},
url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},
year = {2020}
}
@inproceedings{Aljundi2018a,
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {The European Conference on Computer Vision (ECCV)},
keywords = {[vision]},
mendeley-tags = {[vision]},
month = {sep},
title = {{Memory Aware Synapses: Learning what (not) to forget}},
year = {2018}
}
@inproceedings{Zenke2017a,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {International Conference on Machine Learning},
keywords = {[cifar],[mnist],mnist},
language = {en},
mendeley-tags = {[cifar],[mnist]},
month = {jul},
pages = {3987--3995},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {http://proceedings.mlr.press/v70/zenke17a.html},
year = {2017}
}
@inproceedings{Serra2018a,
abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting , cutting current rates by 45 to 80%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
author = {Serr{\`{a}}, Joan and Sur{\'{i}}s, D{\'{i}}dac and Miron, Marius and Karatzoglou, Alexandros},
booktitle = {ICML},
file = {::},
keywords = {[cifar],[fashion],[mnist],serra2018a},
mendeley-tags = {[cifar],[fashion],[mnist]},
title = {{Overcoming Catastrophic Forgetting with Hard Attention to the Task}},
url = {https://github.com/joansj/hat},
year = {2018}
}
