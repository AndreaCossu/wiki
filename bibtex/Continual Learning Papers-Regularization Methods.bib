Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Aljundi2018a,
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {sep},
title = {{Memory Aware Synapses: Learning what (not) to forget}},
year = {2018}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,annotated,ewc},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@inproceedings{Zenke2017a,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {International Conference on Machine Learning},
keywords = {mnist},
language = {en},
month = {jul},
pages = {3987--3995},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {http://proceedings.mlr.press/v70/zenke17a.html},
year = {2017}
}
@inproceedings{Liu2018a,
abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
annote = {ISSN: 1051-4651},
author = {Liu, Xialei and Masana, Marc and Herranz, Luis and de Weijer, Joost and L{\'{o}}pez, Antonio M and Bagdanov, Andrew D},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545895},
keywords = {Computer vision,Data models,Fisher Information Matrix,Neural networks,Standards,Stanford-40 datasets,Task analysis,Training,Training data,ewc,fisher,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,sequential tasks,standard elastic weight consolidation},
month = {aug},
pages = {2262--2268},
shorttitle = {Rotate your Networks},
title = {{Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting}},
year = {2018}
}
@article{Pomponi2020a,
abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
doi = {10.1016/j.neucom.2020.01.093},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions},
language = {en},
month = {feb},
title = {{Efficient continual learning in neural networks with embedding regularization}},
url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},
year = {2020}
}
@article{Li2016a,
abstract = {When building a uniﬁed vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and ﬁne-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace ﬁne-tuning with similar old and new task datasets for improved new task performance.},
annote = {Comment: Conference version appears in ECCV 2016; updated with journal version
arXiv: 1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
journal = {European Conference on Computer Vision},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning},
language = {en},
pages = {614--629},
series = {Springer},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
