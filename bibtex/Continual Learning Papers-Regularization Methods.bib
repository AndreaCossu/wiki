@article{Shmelkov2017a,
abstract = {Despite their success for object detection, convolutional neural networks are ill-equipped for incremental learning, i.e., adapting the original model trained on a set of classes to additionally detect objects of new classes, in the absence of the initial training data. They suffer from "catastrophic forgetting" - an abrupt degradation of performance on the original set of classes, when the training objective is adapted to the new classes. We present a method to address this issue, and learn object detectors incrementally, when neither the original training data nor annotations for the original classes in the new training set are available. The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes and a new distillation loss which minimizes the discrepancy between responses for old classes from the original and the updated networks. This incremental learning can be performed multiple times, for a new set of classes in each step, with a moderate drop in performance compared to the baseline network trained on the ensemble of data. We present object detection results on the PASCAL VOC 2007 and COCO datasets, along with a detailed empirical analysis of the approach.},
archivePrefix = {arXiv},
arxivId = {1708.06977},
author = {Shmelkov, Konstantin and Schmid, Cordelia and Alahari, Karteek},
doi = {10.1109/ICCV.2017.368},
eprint = {1708.06977},
isbn = {9781538610329},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
month = {aug},
pages = {3420--3429},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Incremental Learning of Object Detectors without Catastrophic Forgetting}},
url = {http://arxiv.org/abs/1708.06977},
volume = {2017-Octob},
year = {2017}
}
@inproceedings{michieli2019a,
abstract = {Deep learning architectures exhibit a critical drop of performance due to catastrophic forgetting when they are required to incrementally learn new tasks. Contemporary incremental learning frameworks focus on image classification and object detection while in this work we formally introduce the incremental learning problem for semantic segmentation in which a pixel-wise labeling is considered. To tackle this task we propose to distill the knowledge of the previous model to retain the information about previously learned classes, whilst updating the current model to learn the new ones. We propose various approaches working both on the output logits and on intermediate features. In opposition to some recent frameworks, we do not store any image from previously learned classes and only the last model is needed to preserve high accuracy on these classes. The experimental evaluation on the Pascal VOC2012 dataset shows the effectiveness of the proposed approaches.},
archivePrefix = {arXiv},
arxivId = {1907.13372},
author = {Michieli, Umberto and Zanuttigh, Pietro},
booktitle = {Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019},
doi = {10.1109/ICCVW.2019.00400},
eprint = {1907.13372},
file = {::},
isbn = {9781728150239},
keywords = {Catastrophic forgetting,Incremental learning,Knowledge distillation,Knowledge transfer,Semantic segmentation},
month = {oct},
pages = {3205--3212},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Incremental learning techniques for semantic segmentation}},
url = {http://arxiv.org/abs/1907.13372},
year = {2019}
}
@article{cermelli2020a,
abstract = {Despite their effectiveness in a wide range of tasks, deep architectures suffer from some important limitations. In particular, they are vulnerable to catastrophic forgetting, i.e. they perform poorly when they are required to update their model as new classes are available but the original training set is not retained. This paper addresses this problem in the context of semantic segmentation. Current strategies fail on this task because they do not consider a peculiar aspect of semantic segmentation: since each training step provides annotation only for a subset of all possible classes, pixels of the background class (i.e. pixels that do not belong to any other classes) exhibit a semantic distribution shift. In this work we revisit classical incremental learning methods, proposing a new distillation-based framework which explicitly accounts for this shift. Furthermore, we introduce a novel strategy to initialize classifier's parameters, thus preventing biased predictions toward the background class. We demonstrate the effectiveness of our approach with an extensive evaluation on the Pascal-VOC 2012 and ADE20K datasets, significantly outperforming state of the art incremental learning methods.},
archivePrefix = {arXiv},
arxivId = {2002.00718},
author = {Cermelli, Fabio and Mancini, Massimiliano and Bul{\`{o}}, Samuel Rota and Ricci, Elisa and Caputo, Barbara},
eprint = {2002.00718},
journal = {CVPR},
month = {feb},
pages = {9233--9242},
title = {{Modeling the Background for Incremental Learning in Semantic Segmentation}},
url = {http://arxiv.org/abs/2002.00718},
year = {2020}
}
@article{Seff2017a,
abstract = {Developments in deep generative models have allowed for tractable learning of high-dimensional data distributions. While the employed learning procedures typically assume that training data is drawn i.i.d. from the distribution of interest, it may be desirable to model distinct distributions which are observed sequentially, such as when different classes are encountered over time. Although conditional variations of deep generative models permit multiple distributions to be modeled by a single network in a disentangled fashion, they are susceptible to catastrophic forgetting when the distributions are encountered sequentially. In this paper, we adapt recent work in reducing catastrophic forgetting to the task of training generative adversarial networks on a sequence of distinct distributions, enabling continual generative modeling.},
archivePrefix = {arXiv},
arxivId = {1705.08395},
author = {Seff, Ari and Beatson, Alex and Suo, Daniel and Liu, Han},
eprint = {1705.08395},
journal = {arXiv},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
month = {may},
pages = {1--9},
title = {{Continual Learning in Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1705.08395},
year = {2017}
}
@article{Titsias2019a,
abstract = {We introduce a novel approach for supervised continual learning based on approximate Bayesian inference over function space rather than the parameters of a deep neural network. We use a Gaussian process obtained by treating the weights of the last layer of a neural network as random and Gaussian distributed. Functional regularisation for continual learning naturally arises by applying the variational sparse GP inference method in a sequential fashion as new tasks are encountered. At each step of the process, a summary is constructed for the current task that consists of (i) inducing inputs and (ii) a posterior distribution over the function values at these inputs. This summary then regularises learning of future tasks, through Kullback-Leibler regularisation terms that appear in the variational lower bound, and reduces the effects of catastrophic forgetting. We fully develop the theory of the method and we demonstrate its effectiveness in classification datasets, such as Split-MNIST, Permuted-MNIST and Omniglot.},
archivePrefix = {arXiv},
arxivId = {1901.11356},
author = {Titsias, Michalis K and Schwarz, Jonathan and Matthews, Alexander G de G and Pascanu, Razvan and Teh, Yee Whye},
eprint = {1901.11356},
journal = {arXiv},
keywords = {[mnist],[omniglot]},
mendeley-tags = {[mnist],[omniglot]},
title = {{Functional Regularisation for Continual Learning using Gaussian Processes}},
url = {http://arxiv.org/abs/1901.11356},
year = {2019}
}
@inproceedings{ahn2019a,
abstract = {We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at https://github.com/csm9493/UCL.},
author = {Ahn, Hongjoon and Cha, Sungmin and Lee, Donggyu and Moon, Taesup},
booktitle = {NeurIPS},
file = {::},
keywords = {[bayes],[cifar],[mnist]},
mendeley-tags = {[bayes],[cifar],[mnist]},
pages = {4392--4402},
title = {{Uncertainty-based Continual Learning with Adaptive Regularization}},
url = {https://papers.nips.cc/paper/8690-uncertainty-based-continual-learning-with-adaptive-regularization.pdf},
year = {2019}
}
@inproceedings{dhar2019a,
abstract = {Incremental learning (IL) is an important task aimed at increasing the capability of a trained model, in terms of the number of classes recognizable by the model. The key problem in this task is the requirement of storing data (e.g. images) associated with existing classes, while teaching the classifier to learn new classes. However, this is impractical as it increases the memory requirement at every incre-mental step, which makes it impossible to implement IL algorithms on edge devices with limited memory. Hence, we propose a novel approach, called 'Learning without Memorizing (LwM)', to preserve the information about existing (base) classes, without storing any of their data, while making the classifier progressively learn the new classes. In LwM, we present an information preserving penalty: Attention Distillation Loss (L AD), and demonstrate that penalizing the changes in classifiers' attention maps helps to retain information of the base classes, as new classes are added. We show that adding L AD to the distillation loss which is an existing information preserving loss consistently outperforms the state-of-the-art performance in the iILSVRC-small and iCIFAR-100 datasets in terms of the overall accuracy of base and incrementally learned classes.},
author = {Dhar, Prithviraj and {Vikram Singh}, Rajat and Peng, Kuan-Chuan and Wu, Ziyan and Chellappa, Rama},
booktitle = {CVPR},
file = {::},
keywords = {[cifar]},
mendeley-tags = {[cifar]},
title = {{Learning without Memorizing}},
url = {https://openaccess.thecvf.com/content{\_}CVPR{\_}2019/papers/Dhar{\_}Learning{\_}Without{\_}Memorizing{\_}CVPR{\_}2019{\_}paper.pdf},
year = {2019}
}
@inproceedings{Parshotam2020a,
abstract = {We propose continual instance learning - a method that applies the concept of continual learning to the task of distinguishing instances of the same object category. We specifically focus on the car object, and incrementally learn to distinguish car instances from each other with metric learning. We begin our paper by evaluating current techniques. Establishing that catastrophic forgetting is evident in existing methods, we then propose two remedies. Firstly, we regularise metric learning via Normalised Cross-Entropy. Secondly, we augment existing models with synthetic data transfer. Our extensive experiments on three large-scale datasets, using two different architectures for five different continual learning methods, reveal that Normalised cross-entropy and synthetic transfer leads to less forgetting in existing techniques.},
archivePrefix = {arXiv},
arxivId = {2004.10862},
author = {Parshotam, Kishan and Kilickaya, Mert},
booktitle = {CVPR 2020: Workshop on Continual Learning in Computer Vision},
eprint = {2004.10862},
keywords = {[vision]},
mendeley-tags = {[vision]},
title = {{Continual Learning of Object Instances}},
url = {http://arxiv.org/abs/2004.10862},
year = {2020}
}
@inproceedings{Lee2017a,
abstract = {Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.},
archivePrefix = {arXiv},
arxivId = {1703.08475},
author = {Lee, Sang-Woo and Kim, Jin-Hwa and Jun, Jaehyun and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1703.08475},
file = {::},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {mar},
pages = {4653--4663},
publisher = {Neural information processing systems foundation},
title = {{Overcoming Catastrophic Forgetting by Incremental Moment Matching}},
url = {http://arxiv.org/abs/1703.08475},
volume = {2017-Decem},
year = {2017}
}
@article{Ritter2018a,
abstract = {We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90{\{}$\backslash${\%}{\}} test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.},
archivePrefix = {arXiv},
arxivId = {1805.07810},
author = {Ritter, Hippolyt and Botev, Aleksandar and Barber, David},
eprint = {1805.07810},
journal = {arXiv},
keywords = {[bayes],[mnist]},
mendeley-tags = {[bayes],[mnist]},
title = {{Online Structured Laplace Approximations For Overcoming Catastrophic Forgetting}},
url = {http://arxiv.org/abs/1805.07810},
year = {2018}
}
@article{Ramapuram2017a,
abstract = {Lifelong learning is the problem of learning multiple consecutive tasks in an online manner and is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on learning a lifelong approach to generative modeling whereby we continuously incorporate newly observed distributions into our model representation. We utilize two models, aptly named the student and the teacher, in order to aggregate information about all past distributions without the preservation of any of the past data or previous models. The teacher is utilized as a form of compressed memory in order to allow for the student model to learn over the past as well as present data. We demonstrate why a naive approach to lifelong generative modeling fails and introduce a regularizer with which we demonstrate learning across a long range of distributions.},
archivePrefix = {arXiv},
arxivId = {1705.09847},
author = {Ramapuram, Jason and Gregorova, Magda and Kalousis, Alexandros},
eprint = {1705.09847},
journal = {arXiv},
keywords = {[fashion],[generative],[mnist]},
mendeley-tags = {[fashion],[generative],[mnist]},
number = {2010},
pages = {1--14},
title = {{Lifelong Generative Modeling}},
url = {http://arxiv.org/abs/1705.09847},
year = {2017}
}
@inproceedings{Serra2018a,
abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting , cutting current rates by 45 to 80{\%}. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
author = {Serr{\`{a}}, Joan and Sur{\'{i}}s, D{\'{i}}dac and Miron, Marius and Karatzoglou, Alexandros},
booktitle = {ICML},
file = {::},
keywords = {[cifar],[fashion],[mnist],serra2018a},
mendeley-tags = {[cifar],[fashion],[mnist]},
title = {{Overcoming Catastrophic Forgetting with Hard Attention to the Task}},
url = {https://arxiv.org/pdf/1801.01423.pdf},
year = {2018}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},
mendeley-tags = {[mnist]},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@inproceedings{Li2016a,
abstract = {When building a uniﬁed vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and ﬁne-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace ﬁne-tuning with similar old and new task datasets for improved new task performance.},
annote = {Comment: Conference version appears in ECCV 2016; updated with journal version
arXiv: 1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
booktitle = {European Conference on Computer Vision},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning,[imagenet]},
language = {en},
mendeley-tags = {[imagenet]},
pages = {614--629},
series = {Springer},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@inproceedings{Zenke2017a,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {International Conference on Machine Learning},
keywords = {[cifar],[mnist],mnist},
language = {en},
mendeley-tags = {[cifar],[mnist]},
month = {jul},
pages = {3987--3995},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {http://proceedings.mlr.press/v70/zenke17a.html},
year = {2017}
}
@inproceedings{Aljundi2018a,
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {The European Conference on Computer Vision (ECCV)},
keywords = {[vision]},
mendeley-tags = {[vision]},
month = {sep},
title = {{Memory Aware Synapses: Learning what (not) to forget}},
url = {https://openaccess.thecvf.com/content{\_}ECCV{\_}2018/papers/Rahaf{\_}Aljundi{\_}Memory{\_}Aware{\_}Synapses{\_}ECCV{\_}2018{\_}paper.pdf},
year = {2018}
}
@inproceedings{Liu2018a,
abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
annote = {ISSN: 1051-4651},
author = {Liu, Xialei and Masana, Marc and Herranz, Luis and {Van de Weijer}, Joost and Lopez, Antonio M. and Bagdanov, Andrew D},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545895},
isbn = {978-1-5386-3788-3},
keywords = {Computer vision,Data models,Fisher Information Matrix,Neural networks,Standards,Stanford-40 datasets,Task analysis,Training,Training data,[cifar],[mnist],ewc,fisher,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,sequential tasks,standard elastic weight consolidation},
mendeley-tags = {[cifar],[mnist]},
month = {aug},
pages = {2262--2268},
publisher = {IEEE},
shorttitle = {Rotate your Networks},
title = {{Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting}},
url = {https://ieeexplore.ieee.org/document/8545895/},
year = {2018}
}
@article{Pomponi2020a,
abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
doi = {10.1016/j.neucom.2020.01.093},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions,[cifar],[mnist]},
language = {en},
mendeley-tags = {[cifar],[mnist]},
month = {feb},
title = {{Efficient continual learning in neural networks with embedding regularization}},
url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},
year = {2020}
}
@inproceedings{Oswal2019a,
abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...},
author = {von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~{a}}o and Grewe, Benjamin F},
booktitle = {International Conference on Learning Representations},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {sep},
title = {{Continual learning with hypernetworks}},
url = {https://openreview.net/forum?id=SJgwNerKvB},
year = {2020}
}
