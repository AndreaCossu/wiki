@inproceedings{Aljundi2016a,
abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision of which expert to deploy at test time. We introduce a gating autoencoder that learns a representation for the task at hand, and is used at test time to automatically forward the test sample to the relevant expert. This has the added advantage of being memory efficient as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert with fine-tuning or learning-without-forgetting can be selected. We evaluate our system on image classification and video prediction problems.},
archivePrefix = {arXiv},
arxivId = {1611.06194},
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1611.06194},
keywords = {[vision]},
mendeley-tags = {[vision]},
title = {{Expert Gate: Lifelong Learning with a Network of Experts}},
url = {http://arxiv.org/abs/1611.06194},
year = {2017}
}
@inproceedings{Srivastava2019a,
abstract = {The problem of a deep learning model losing performance on a previously learned task when fine-tuned to a new one is a phenomenon known as Catastrophic forgetting. There are two major ways to mitigate this problem: either preserving activations of the initial network during training with a new task; or restricting the new network activations to remain close to the initial ones. The latter approach falls under the denomination of lifelong learning, where the model is updated in a way that it performs well on both old and new tasks, without having access to the old task's training samples anymore. Recently, approaches like pruning networks for freeing network capacity during sequential learning of tasks have been gaining in popularity. Such approaches allow learning small networks while making redundant parameters available for the next tasks. The common problem encountered with these approaches is that the pruning percentage is hard-coded, irrespective of the number of samples, of the complexity of the learning task and of the number of classes in the dataset. We propose a method based on Bayesian optimization to perform adaptive compression/pruning of the network and show its effectiveness in lifelong learning. Our method learns to perform heavy pruning for small and/or simple datasets while using milder compression rates for large and/or complex data. Experiments on classification and semantic segmentation demonstrate the applicability of learning network compression, where we are able to effectively preserve performances along sequences of tasks of varying complexity.},
archivePrefix = {arXiv},
arxivId = {1907.09695},
author = {Srivastava, Shivangi and Berman, Maxim and Blaschko, Matthew B and Tuia, Devis},
booktitle = {BMVC},
eprint = {1907.09695},
keywords = {[imagenet],[sparsity]},
mendeley-tags = {[imagenet],[sparsity]},
month = {jul},
title = {{Adaptive Compression-based Lifelong Learning}},
url = {http://arxiv.org/abs/1907.09695},
year = {2019}
}
@inproceedings{Ashfahani2019a,
abstract = {The feasibility of deep neural networks (DNNs) to address data stream problems still requires intensive study because of the static and offline nature of conventional deep learning approaches. A deep continual learning algorithm, namely autonomous deep learning (ADL), is proposed in this paper. Unlike traditional deep learning methods, ADL features a flexible structure where its network structure can be constructed from scratch with the absence of initial network structure via the self-constructing network structure. ADL specifically addresses catastrophic forgetting by having a different-depth structure which is capable of achieving a trade-off between plasticity and stability. Network significance (NS) formula is proposed to drive the hidden nodes growing and pruning mechanism. Drift detection scenario (DDS) is put forward to signal distributional changes in data streams which induce the creation of a new hidden layer. Maximum information compression index (MICI) method plays an important role as a complexity reduction module eliminating redundant layers. The efficacy of ADL is numerically validated under the prequential test-then-train procedure in lifelong environments using nine popular data stream problems. The numerical results demonstrate that ADL consistently outperforms recent continual learning methods while characterizing the automatic construction of network structures.},
address = {Philadelphia, PA},
archivePrefix = {arXiv},
arxivId = {1810.07348},
author = {Ashfahani, Andri and Pratama, Mahardhika},
booktitle = {Proceedings of the 2019 SIAM International Conference on Data Mining},
doi = {10.1137/1.9781611975673.75},
eprint = {1810.07348},
isbn = {9781611975673},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
month = {may},
pages = {666--674},
publisher = {Society for Industrial and Applied Mathematics},
title = {{Autonomous Deep Learning: Continual Learning Approach for Dynamic Environments}},
url = {https://epubs.siam.org/doi/10.1137/1.9781611975673.75},
year = {2019}
}
@inproceedings{Zhu2020a,
abstract = {Training a neural network model can be a lifelong learning process and is a computationally intensive one. A severe adverse effect that may occur in deep neural network models is that they can suffer from catastrophic forgetting during retraining on new data. To avoid such disruptions in the continuous learning, one appealing property is the additive nature of ensemble models. In this paper, we propose two generic ensemble approaches, gradient boosting and meta-learning, to solve the catastrophic forgetting problem in tuning pre-trained neural network models.},
archivePrefix = {arXiv},
arxivId = {2001.01829},
author = {Zhu, Xiaofeng and Liu, Feng and Trajcevski, Goce and Wang, Dingding},
booktitle = {2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)},
doi = {10.1109/ICMLA.2019.00094},
eprint = {2001.01829},
isbn = {978-1-7281-4550-1},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {dec},
number = {1},
pages = {506--510},
publisher = {IEEE},
title = {{Frosting Weights for Better Continual Training}},
url = {https://ieeexplore.ieee.org/document/8999083/},
year = {2019}
}
@inproceedings{Terekhov2015a,
abstract = {Although deep neural networks (DNNs) have demonstrated impressive results during the last decade, they remain highly specialized tools, which are trained – often from scratch – to solve each particular task. The human brain, in contrast, significantly re-uses existing capacities when learning to solve new tasks. In the current study we explore a block-modular architecture for DNNs, which allows parts of the existing network to be re-used to solve a new task without a decrease in performance when solving the original task. We show that networks with such architectures can outperform networks trained from scratch, or perform comparably, while having to learn nearly 10 times fewer weights than the networks trained from scratch.},
archivePrefix = {arXiv},
arxivId = {1908.08017},
author = {Terekhov, Alexander V. and Montone, Guglielmo and O'Regan, J. Kevin},
booktitle = {Conference on Biomimetic and Biohybrid Systems},
doi = {10.1007/978-3-319-22979-9_27},
eprint = {1908.08017},
file = {::},
isbn = {9783319229782},
issn = {16113349},
keywords = {Deep learning,Knowledge transfer,Modular,Neural networks,[vision]},
mendeley-tags = {[vision]},
pages = {268--279},
publisher = {Springer Verlag},
title = {{Knowledge transfer in deep block-modular neural networks}},
url = {http://lpp.psycho.univ-paris5.fr/feel},
volume = {9222},
year = {2015}
}
@inproceedings{Valkov2018a,
abstract = {We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochas-tic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search.},
author = {Valkov, Lazar and Chaudhari, Dipak and Srivastava, Akash and Sutton, Charles and Chaudhuri, Swarat},
booktitle = {NeurIPS},
file = {::},
pages = {8687--8698},
title = {{HOUDINI: Lifelong Learning as Program Synthesis}},
url = {http://papers.nips.cc/paper/8086-houdini-lifelong-learning-as-program-synthesis.pdf},
year = {2018}
}
@inproceedings{Hung2019a,
abstract = {Continual lifelong learning is essential to many applications. In this paper, we propose a simple but effective approach to continual deep learning. Our approach leverages the principles of deep model compression, critical weights selection, and progressive networks expansion. By enforcing their integration in an iterative manner, we introduce an incremental learning method that is scalable to the number of sequential tasks in a continual learning process. Our approach is easy to implement and owns several favorable characteristics. First, it can avoid forgetting (i.e., learn new tasks while remembering all previous tasks). Second, it allows model expansion but can maintain the model compactness when handling sequential tasks. Besides, through our compaction and selection/expansion mechanism, we show that the knowledge accumulated through learning previous tasks is helpful to build a better model for the new tasks compared to training the models independently with tasks. Experimental results show that our approach can incrementally learn a deep model tackling multiple tasks without forgetting, while the model compactness is maintained with the performance more satisfiable than individual task training.},
author = {Hung, Steven C Y and Tu, Cheng-Hao and Wu, Cheng-En and Chen, Chien-Hung and Chan, Yi-Ming and Chen, Chu-Song},
booktitle = {NeurIPS},
file = {::},
keywords = {[cifar],[imagenet]},
mendeley-tags = {[cifar],[imagenet]},
pages = {13669--13679},
title = {{Compacting, Picking and Growing for Unforgetting Continual Learning}},
url = {http://papers.nips.cc/paper/9518-compacting-picking-and-growing-for-unforgetting-continual-learning.pdf},
year = {2019}
}
@article{Garg2017a,
abstract = {We address the problem of online model adaptation when learning representations from non-stationary data streams. Specifically, we focus here on online dictionary learning (i.e. sparse linear autoencoder), and propose a simple but effective online modelselection approach involving "birth" (addition) and "death" (removal) of hidden units representing dictionary elements, in response to changing inputs; we draw inspiration from the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with better adaptation to new environments. Empirical evaluation on real-life datasets (images and text), as well as on synthetic data, demonstrates that the proposed approach can considerably outperform the state-of-art non-adaptive online sparse coding of [Mairal et al., 2009] in the presence of non-stationary data. Moreover, we identify certain data- and model properties associated with such improvements.},
archivePrefix = {arXiv},
arxivId = {1701.06106},
author = {Garg, Sahil and Rish, Irina and Cecchi, Guillermo and Lozano, Aurelie},
doi = {10.24963/ijcai.2017/235},
eprint = {1701.06106},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {[nlp],[vision]},
mendeley-tags = {[nlp],[vision]},
pages = {1696--1702},
title = {{Neurogenesis-inspired dictionary learning: Online model adaption in a changing world}},
url = {https://arxiv.org/abs/1701.06106},
year = {2017}
}
@inproceedings{rao2019a,
abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
booktitle = {NeurIPS},
file = {::},
keywords = {[mnist],[omniglot]},
mendeley-tags = {[mnist],[omniglot]},
title = {{Continual Unsupervised Representation Learning}},
url = {https://papers.nips.cc/paper/8981-continual-unsupervised-representation-learning.pdf},
year = {2019}
}
@inproceedings{xu2018a,
abstract = {Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.},
author = {Xu, Ju and Zhu, Zhanxing},
booktitle = {Advances in Neural Information Processing Systems},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
pages = {899--908},
title = {{Reinforced continual learning}},
url = {http://papers.nips.cc/paper/7369-reinforced-continual-learning},
year = {2018}
}
@article{li2019a,
abstract = {Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting},
archivePrefix = {arXiv},
arxivId = {1904.00310},
author = {Li, Xilai and Zhou, Yingbo and Wu, Tianfu and Socher, Richard and Xiong, Caiming},
eprint = {1904.00310},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Learn to grow: A continual structure learning framework for overcoming catastrophic forgetting}},
url = {https://arxiv.org/pdf/1904.00310.pdf},
year = {2019}
}
@inproceedings{adel2020a,
abstract = {Approaches to continual learning aim to successfully learn a set of related tasks that arrive in an online manner. Recently, several frameworks have been developed which enable deep learning to be deployed in this learning scenario. A key modelling decision is to what extent the architecture should be shared across tasks. On the one hand, separately modelling each task avoids catastrophic forgetting but it does not support transfer learning and leads to large models. On the other hand, rigidly specifying a shared component and a task-specific part enables task transfer and limits the model size, but it is vulnerable to catastrophic forgetting and restricts the form of task-transfer that can occur. Ideally, the network should adaptively identify which parts of the network to share in a data driven way. Here we introduce such an approach called Continual Learning with Adaptive Weights (CLAW), which is based on probabilistic modelling and variational inference. Experiments show that CLAW achieves state-of-the-art performance on six benchmarks in terms of overall continual learning performance, as measured by classification accuracy, and in terms of addressing catastrophic forgetting.},
author = {Adel, Tameem and Zhao, Han and Turner, Richard E},
booktitle = {International Conference on Learning Representations},
keywords = {[cifar],[mnist],[omniglot]},
mendeley-tags = {[cifar],[mnist],[omniglot]},
title = {{Continual Learning with Adaptive Weights (CLAW)}},
url = {https://openreview.net/forum?id=Hklso24Kwr},
year = {2020}
}
@inproceedings{Luders2016a,
abstract = {Continual learning, i.e. the ability to sequentially learn tasks without catastrophicforgetting of previously learned ones, is an important open challenge in machinelearning. In this paper we take a step in this direction by showing that the recentlyproposedEvolving Neural Turing Machine(ENTM) approach is able to performone-shot learningin a reinforcement learning task without catastrophic forgettingof previously stored associations.},
author = {{Luders, Benno and Schlager, Mikkel and Risi}, Sebastia},
booktitle = {NIPS 2016 Workshop on Continual Learning and Deep Networks},
title = {{Continual learning through evolvable neural turing machines}},
url = {https://core.ac.uk/reader/84859350},
year = {2016}
}
@inproceedings{Gidaris2018a,
abstract = {The human visual system has the remarkably ability to be able to effortlessly learn novel concepts from only a few examples. Mimicking the same behavior on machine learning vision systems is an interesting and very challenging research problem with many practical advantages on real world vision applications. In this context, the goal of our work is to devise a few-shot visual learning system that during test time it will be able to efficiently learn novel categories from only a few training data while at the same time it will not forget the initial categories on which it was trained (here called base categories). To achieve that goal we propose (a) to extend an object recognition system with an attention based few-shot classification weight generator, and (b) to redesign the classifier of a ConvNet model as the cosine similarity function between feature representations and classification weight vectors. The latter, apart from unifying the recognition of both novel and base categories, it also leads to feature representations that generalize better on 'unseen' categories. We extensively evaluate our approach on Mini-ImageNet where we manage to improve the prior state-of-the-art on few-shot recognition (i.e., we achieve 56.20{\%} and 73.00{\%} on the 1-shot and 5-shot settings respectively) while at the same time we do not sacrifice any accuracy on the base categories, which is a characteristic that most prior approaches lack. Finally, we apply our approach on the recently introduced few-shot benchmark of Bharath and Girshick [4] where we also achieve state-of-the-art results.},
archivePrefix = {arXiv},
arxivId = {1804.09458},
author = {Gidaris, Spyros and Komodakis, Nikos},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2018.00459},
eprint = {1804.09458},
isbn = {9781538664209},
issn = {10636919},
keywords = {[imagenet],[vision]},
mendeley-tags = {[imagenet],[vision]},
pages = {4367--4375},
title = {{Dynamic Few-Shot Visual Learning Without Forgetting}},
url = {http://openaccess.thecvf.com/content{\_}cvpr{\_}2018/html/Gidaris{\_}Dynamic{\_}Few-Shot{\_}Visual{\_}CVPR{\_}2018{\_}paper.html},
year = {2018}
}
@inproceedings{Shen2019a,
abstract = {Semantic slot filling is one of the major tasks in spoken language understanding (SLU). After a slot filling model is trained on pre-collected data, it is crucial to continually improve the model after deployment to learn users' new expressions. As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions. In this paper, we introduce a novel progressive slot filling model, ProgModel. ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data. As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions. Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24{\%} and 3.03{\%} on two benchmark datasets.},
author = {Shen, Yilin and Zeng, Xiangyu and Jin, Hongxia},
booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing},
file = {::},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
pages = {1279--1284},
publisher = {Association for Computational Linguistics},
title = {{A Progressive Model to Enable Continual Learning for Semantic Slot Filling}},
url = {https://www.aclweb.org/anthology/D19-1126.pdf},
year = {2019}
}
@article{Madrid2018a,
abstract = {Research progress in AutoML has lead to state of the art solutions that can cope quite wellwith supervised learning task, e.g., classification with AutoSklearn. However, so far thesesystems do not take into account the changing nature of evolving data over time (i.e., theystill assume i.i.d. data); even when this sort of domains are increasingly available in realapplications (e.g., spam filtering, user preferences, etc.). We describe a first attempt to de-velop an AutoML solution for scenarios in which data distribution changes relatively slowlyover time and in which the problem is approached in a lifelong learning setting. We extendAuto-Sklearn with sound and intuitive mechanisms that allow it to cope with this sort ofproblems. The extended Auto-Sklearn is combined with concept drift detection techniquesthat allow it to automatically determine when the initial models have to be adapted. Wereport experimental results in benchmark data from AutoML competitions that adhere tothis scenario. Results demonstrate the effectiveness of the proposed methodology.},
archivePrefix = {arXiv},
arxivId = {1907.10772},
author = {Madrid, Jorge G. and Escalante, Hugo Jair and Morales, Eduardo F. and Tu, Wei-Wei and Yu, Yang and Sun-Hosoya, Lisheng and Guyon, Isabelle and Sebag, Michele},
eprint = {1907.10772},
file = {::},
journal = {arXiv},
month = {jul},
title = {{Towards AutoML in the presence of Drift: first results}},
url = {http://arxiv.org/abs/1907.10772},
year = {2019}
}
@article{Mehta2020a,
abstract = {Naively trained neural networks tend to experience catastrophic forgetting in sequential task settings, where data from previous tasks are unavailable. A number of methods, using various model expansion strategies, have been proposed recently as possible solutions. However, determining how much to expand the model is left to the practitioner, and typically a constant schedule is chosen for simplicity, regardless of how complex the incoming task is. Instead, we propose a principled Bayesian nonparametric approach based on the Indian Buffet Process (IBP) prior, letting the data determine how much to expand the model complexity. We pair this with a factorization of the neural network's weight matrices. Such an approach allows us to scale the number of factors of each weight matrix to the complexity of the task, while the IBP prior imposes weight factor sparsity and encourages factor reuse, promoting positive knowledge transfer between tasks. We demonstrate the effectiveness of our method on a number of continual learning benchmarks and analyze how weight factors are allocated and reused throughout the training.},
archivePrefix = {arXiv},
arxivId = {2004.10098},
author = {Mehta, Nikhil and Liang, Kevin J and Carin, Lawrence},
eprint = {2004.10098},
journal = {arXiv},
keywords = {[bayes],[cifar],[mnist],[sparsity]},
mendeley-tags = {[bayes],[cifar],[mnist],[sparsity]},
pages = {1--17},
title = {{Bayesian Nonparametric Weight Factorization for Continual Learning}},
url = {http://arxiv.org/abs/2004.10098},
year = {2020}
}
@article{marsland2002a,
abstract = {The ability to grow extra nodes is a potentially useful facility for a self-organising neural network. A network that can add nodes into its map space can approximate the input space more accurately, and often more parsimoniously, than a network with predefined structure and size, such as the Self-Organising Map. In addition, a growing network can deal with dynamic input distributions. Most of the growing networks that have been proposed in the literature add new nodes to support the node that has accumulated the highest error during previous iterations or to support topological structures. This usually means that new nodes are added only when the number of iterations is an integer multiple of some pre-defined constant, $\lambda$. This paper suggests a way in which the learning algorithm can add nodes whenever the network in its current state does not sufficiently match the input. In this way the network grows very quickly when new data is presented, but stops growing once the network has matched the data. This is particularly important when we consider dynamic data sets, where the distribution of inputs can change to a new regime after some time. We also demonstrate the preservation of neighbourhood relations in the data by the network. The new network is compared to an existing growing network, the Growing Neural Gas (GNG), on a artificial dataset, showing how the network deals with a change in input distribution after some time. Finally, the new network is applied to several novelty detection tasks and is compared with both the GNG and an unsupervised form of the Reduced Coulomb Energy network on a robotic inspection task and with a Support Vector Machine on two benchmark novelty detection tasks. {\textcopyright} 2002 Elsevier Science Ltd. All rights reserved.},
author = {Marsland, Stephen and Shapiro, Jonathan and Nehmzow, Ulrich},
doi = {10.1016/S0893-6080(02)00078-3},
issn = {08936080},
journal = {Neural Networks},
keywords = {Dimensionality reduction,Growing networks,Inspection,Mobile robotics,Novelty detection,Self-organisation,Topology preservation,Unsupervised learning,[som]},
mendeley-tags = {[som]},
month = {oct},
number = {8-9},
pages = {1041--1058},
publisher = {Pergamon},
title = {{A self-organising network that grows when required}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608002000783},
volume = {15},
year = {2002}
}
@inproceedings{Yoon2018,
abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efﬁciently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only signiﬁcantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network ﬁne-tuned on all tasks obtained siginﬁcantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the ﬁrst place.},
annote = {The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.},
author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
booktitle = {ICLR},
keywords = {[cifar],[mnist],[sparsity],disadvantages,lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[cifar],[mnist],[sparsity]},
pages = {11},
title = {{Lifelong Learning With Dynamically Expandable Networks}},
url = {https://arxiv.org/abs/1708.01547},
year = {2018}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
journal = {arXiv},
keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@inproceedings{Draelos2017a,
abstract = {Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.},
annote = {The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.},
author = {Draelos, Timothy John and Miner, Nadine E and Lamb, Christopher and Cox, Jonathan A and Vineyard, Craig Michael and Carlson, Kristofor David and Severa, William Mark and James, Conrad D and Aimone, James Bradley},
booktitle = {IJCNN},
keywords = {[mnist],autoencoder,autoencoders,neurogenesis,reconstruction},
language = {English},
mendeley-tags = {[mnist]},
title = {{Neurogenesis Deep Learning}},
url = {https://www.osti.gov/biblio/1424868},
year = {2017}
}
@inproceedings{Asghar2019a,
abstract = {This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in...},
annote = {The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.},
author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A and Pantasdo, Kevin D and Poupart, Pascal and Jiang, Xin},
booktitle = {International Conference on Learning Representations},
keywords = {[nlp],[rnn]},
mendeley-tags = {[nlp],[rnn]},
month = {sep},
title = {{Progressive Memory Banks for Incremental Domain Adaptation}},
url = {https://openreview.net/forum?id=BkepbpNFwr},
year = {2019}
}
@inproceedings{Cossu2020a,
abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
annote = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.},
author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
booktitle = {Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020)},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist],[rnn]},
mendeley-tags = {[mnist],[rnn]},
month = {apr},
title = {{Continual Learning with Gated Incremental Memories for sequential data processing}},
url = {http://arxiv.org/abs/2004.04077},
year = {2020}
}
