@inproceedings{Draelos2017a,
abstract = {Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.},
annote = {The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.},
author = {Draelos, Timothy John and Miner, Nadine E and Lamb, Christopher and Cox, Jonathan A and Vineyard, Craig Michael and Carlson, Kristofor David and Severa, William Mark and James, Conrad D and Aimone, James Bradley},
booktitle = {IJCNN},
keywords = {[mnist],autoencoder,autoencoders,neurogenesis,reconstruction},
language = {English},
mendeley-tags = {[mnist]},
title = {{Neurogenesis Deep Learning}},
url = {https://www.osti.gov/biblio/1424868},
year = {2017}
}
@inproceedings{Yoon2018,
abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efﬁciently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only signiﬁcantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network ﬁne-tuned on all tasks obtained siginﬁcantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the ﬁrst place.},
annote = {The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.},
author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
booktitle = {ICLR},
keywords = {[cifar],[mnist],[sparsity],disadvantages,lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[cifar],[mnist],[sparsity]},
pages = {11},
title = {{Lifelong Learning With Dynamically Expandable Networks}},
year = {2018}
}
@inproceedings{Cossu2020a,
abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
annote = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.},
author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
booktitle = {Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020)},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist],[rnn]},
mendeley-tags = {[mnist],[rnn]},
month = {apr},
title = {{Continual Learning with Gated Incremental Memories for sequential data processing}},
url = {http://arxiv.org/abs/2004.04077},
year = {2020}
}
@inproceedings{Asghar2019a,
abstract = {This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in...},
annote = {The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.},
author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A and Pantasdo, Kevin D and Poupart, Pascal and Jiang, Xin},
booktitle = {International Conference on Learning Representations},
keywords = {[nlp],[rnn]},
mendeley-tags = {[nlp],[rnn]},
month = {sep},
title = {{Progressive Memory Banks for Incremental Domain Adaptation}},
url = {https://openreview.net/forum?id=BkepbpNFwr},
year = {2019}
}
@misc{Golkar2019a,
abstract = {We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.},
annote = {Comment: 12 pages, 5 figures, 3 tables
arXiv: 1903.04476},
author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
booktitle = {arXiv:1903.04476 [cs, q-bio, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning,[cifar],[mnist],[sparsity]},
mendeley-tags = {[cifar],[mnist],[sparsity]},
month = {mar},
title = {{Continual Learning via Neural Pruning}},
url = {http://arxiv.org/abs/1903.04476},
year = {2019}
}
@misc{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
booktitle = {arXiv: 1606.04671 [cs]},
keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
