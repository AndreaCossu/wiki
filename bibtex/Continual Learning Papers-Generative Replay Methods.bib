@article{vandenven2018a,
abstract = {A major obstacle to developing artificial intelligence applications capable of true lifelong learning is that artificial neural networks quickly or catastrophically forget previously learned tasks when trained on a new one. Numerous methods for alleviating catastrophic forgetting are currently being proposed, but differences in evaluation protocols make it difficult to directly compare their performance. To enable more meaningful comparisons, here we identified three distinct scenarios for continual learning based on whether task identity is known and, if it is not, whether it needs to be inferred. Performing the split and permuted MNIST task protocols according to each of these scenarios, we found that regularization-based approaches (e.g., elastic weight consolidation) failed when task identity needed to be inferred. In contrast, generative replay combined with distillation (i.e., using class probabilities as "soft targets") achieved superior performance in all three scenarios. Addressing the issue of efficiency, we reduced the computational cost of generative replay by integrating the generative model into the main model by equipping it with generative feedback or backward connections. This Replay-through-Feedback approach substantially shortened training time with no or negligible loss in performance. We believe this to be an important first step towards making the powerful technique of generative replay scalable to real-world continual learning applications.},
archivePrefix = {arXiv},
arxivId = {1809.10635},
author = {van de Ven, Gido M. and Tolias, Andreas S.},
eprint = {1809.10635},
journal = {arXiv},
keywords = {[framework],[generative],[mnist]},
mendeley-tags = {[framework],[generative],[mnist]},
month = {sep},
title = {{Generative replay with feedback connections as a general strategy for continual learning}},
url = {https://arxiv.org/abs/1809.10635},
year = {2018}
}
@article{Ven2020a,
abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as ‘generative replay', which can successfully – and surprisingly efficiently – prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on CIFAR-100) without storing data, and it provides a novel model for replay in the brain.},
annote = {The paper shows a generative form of replay in which a VAE, conditioned on the current task, is able to generate pseudosamples and, when used as a classifier, to address new tasks. The idea is that the generative model is inspired by the hyppocampus, which sits hierarchically on top of the cortex (often thought as the classifier). In this way, replay is fed-back by the same model used to predict the class. Forgetting is prevented both on VAE and on the classification component through replay. It also shows that regularization approaches fail in class-incremental setting.},
author = {van de Ven, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
doi = {10.1038/s41467-020-17866-2},
journal = {Nature Communications},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Brain-inspired replay for continual learning with artificial neural networks}},
url = {https://www.nature.com/articles/s41467-020-17866-2},
volume = {11},
year = {2020}
}
@article{ayub2020b,
abstract = {The two main challenges faced by continual learning approaches are catastrophic forgetting and memory limitations on the storage of data. To cope with these challenges, we propose a novel, cognitively-inspired approach which trains autoencoders with Neural Style Transfer to encode and store images. Reconstructed images from encoded episodes are replayed when training the classifier model on a new task to avoid catastrophic forgetting. The loss function for the reconstructed images is weighted to reduce its effect during classifier training to cope with image degradation. When the system runs out of memory the encoded episodes are converted into centroids and covariance matrices, which are used to generate pseudo-images during classifier training, keeping classifier performance stable with less memory. Our approach increases classification accuracy by 13-17{\%} over state-of-the-art methods on benchmark datasets, while requiring 78{\%} less storage space.},
archivePrefix = {arXiv},
arxivId = {2007.06637},
author = {Ayub, Ali and Wagner, Alan R.},
eprint = {2007.06637},
journal = {arXiv},
keywords = {[generative],[imagenet],[mnist],catastrophic forgetting,continual learning},
mendeley-tags = {[generative],[imagenet],[mnist]},
month = {jun},
title = {{Storing Encoded Episodes as Concepts for Continual Learning}},
url = {https://arxiv.org/abs/2007.06637 http://arxiv.org/abs/2007.06637},
year = {2020}
}
@article{Rostami2019a,
abstract = {Despite huge success, deep networks are unable to learn effectively in sequential multitask learning settings as they forget the past learned tasks after learning new tasks. Inspired from complementary learning systems theory, we address this challenge by learning a generative model that couples the current task to the past learned tasks through a discriminative embedding space. We learn an abstract level generative distribution in the embedding that allows the generation of data points to represent the experience. We sample from this distribution and utilize experience replay to avoid forgetting and simultaneously accumulate new knowledge to the abstract distribution in order to couple the current task with past experience. We demonstrate theoretically and empirically that our framework learns a distribution in the embedding that is shared across all task and as a result tackles catastrophic forgetting.},
archivePrefix = {arXiv},
arxivId = {1903.04566},
author = {Rostami, Mohammad and Kolouri, Soheil and Pilly, Praveen K},
eprint = {1903.04566},
journal = {arXiv},
month = {mar},
title = {{Complementary Learning for Overcoming Catastrophic Forgetting Using Experience Replay}},
url = {http://arxiv.org/abs/1903.04566},
year = {2019}
}
@article{Wang2019a,
abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4{\%} of the size of all previous training data matches the performance of refining the classifier keeping 20{\%} of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
annote = {arXiv: 1906.00654},
author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
journal = {arXiv},
keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,Statistics - Machine Learning,[audio],audio,sequence,sequences,time series},
mendeley-tags = {[audio]},
month = {jun},
title = {{Continual Learning of New Sound Classes using Generative Replay}},
url = {http://arxiv.org/abs/1906.00654},
year = {2019}
}
@inproceedings{Shin2017a,
author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
pages = {2990--2999},
publisher = {Curran Associates, Inc.},
title = {{Continual Learning with Deep Generative Replay}},
url = {http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf},
year = {2017}
}
@article{Parisi2018a,
abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.},
author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
doi = {10.3389/fnbot.2018.00078},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {CLS,Incremental Learning,Lifelong learning,Memory,Self-organizing Network,[core50],[dual],[som],object recognition systems},
language = {English},
mendeley-tags = {[core50],[dual],[som]},
title = {{Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization}},
url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},
volume = {12},
year = {2018}
}
