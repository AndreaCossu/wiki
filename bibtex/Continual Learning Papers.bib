@inproceedings{Pfulb2018a,
abstract = {We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.},
author = {Pf{\"{u}}lb, B and Gepperth, A},
booktitle = {ICLR},
file = {::},
keywords = {fashion,mnist},
mendeley-tags = {fashion,mnist},
month = {sep},
title = {{A comprehensive, application-oriented study of catastrophic forgetting in DNNs}},
url = {https://gitlab.informatik.hs-fulda.de/ML-Projects/CF{\_}in{\_}DNNs},
year = {2018}
}
@inproceedings{Aljundi2018a,
author = {Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {The European Conference on Computer Vision (ECCV)},
month = {sep},
title = {{Memory Aware Synapses: Learning what (not) to forget}},
year = {2018}
}
@misc{Golkar2019a,
abstract = {We introduce Continual Learning via Neural Pruning (CLNP), a new method aimed at lifelong learning in fixed capacity models based on neuronal model sparsification. In this method, subsequent tasks are trained using the inactive neurons and filters of the sparsified network and cause zero deterioration to the performance of previous tasks. In order to deal with the possible compromise between model sparsity and performance, we formalize and incorporate the concept of graceful forgetting: the idea that it is preferable to suffer a small amount of forgetting in a controlled manner if it helps regain network capacity and prevents uncontrolled loss of performance during the training of future tasks. CLNP also provides simple continual learning diagnostic tools in terms of the number of free neurons left for the training of future tasks as well as the number of neurons that are being reused. In particular, we see in experiments that CLNP verifies and automatically takes advantage of the fact that the features of earlier layers are more transferable. We show empirically that CLNP leads to significantly improved results over current weight elasticity based methods.},
annote = {Comment: 12 pages, 5 figures, 3 tables
arXiv: 1903.04476},
author = {Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
booktitle = {arXiv:1903.04476 [cs, q-bio, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning,cifar,mnist,sparsity},
mendeley-tags = {cifar,mnist,sparsity},
month = {mar},
title = {{Continual Learning via Neural Pruning}},
url = {http://arxiv.org/abs/1903.04476},
year = {2019}
}
@inproceedings{riemer2019a,
abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
booktitle = {ICLR},
file = {::},
month = {sep},
title = {{Learning to learn without forgetting by maximizing transfer and minimizing interference}},
year = {2019}
}
@inproceedings{Coop2013a,
abstract = {Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.},
address = {Dallas, TX, USA},
author = {Coop, Robert and Arel, Itamar},
booktitle = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2013.6707047},
isbn = {978-1-4673-6129-3 978-1-4673-6128-6},
keywords = {fel,mnist,recurrent fel,rnn,sparsity},
language = {en},
mendeley-tags = {mnist,rnn,sparsity},
month = {aug},
pages = {1--7},
publisher = {IEEE},
title = {{Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer}},
url = {http://ieeexplore.ieee.org/document/6707047/},
year = {2013}
}
@misc{Diaz-Rodriguez2018a,
abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
annote = {arXiv: 1810.13166},
author = {D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
booktitle = {arXiv: 1810.13166 [cs]},
keywords = {68T05,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,cifar,cs.AI,cs.CV,cs.LG,cs.NE,framework,stat.ML},
mendeley-tags = {cifar,framework},
month = {oct},
shorttitle = {Don't forget, there is more than forgetting},
title = {{Don't forget, there is more than forgetting: new metrics for Continual Learning}},
url = {http://arxiv.org/abs/1810.13166},
year = {2018}
}
@incollection{Kobayashi2019a,
abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be ﬁred corresponding to each task, since only readout weights are updated according to the degree of ﬁring of neurons. We therefore propose the way to design reservoir computing such that the ﬁring neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely diﬀerent networks.},
address = {Cham},
annote = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.},
author = {Kobayashi, Taisuke and Sugino, Toshiki},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions},
doi = {10.1007/978-3-030-30493-5_4},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30492-8 978-3-030-30493-5},
keywords = {fractals,rc,reinforcement,reservoir computing,rnn},
language = {en},
mendeley-tags = {rnn},
pages = {35--47},
publisher = {Springer International Publishing},
title = {{Continual Learning Exploiting Structure of Fractal Reservoir Computing}},
url = {http://link.springer.com/10.1007/978-3-030-30493-5{\_}4},
volume = {11731},
year = {2019}
}
@article{Lopez-Paz2017a,
abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
annote = {Comment: Published at NIPS 2017
arXiv: 1706.08840},
author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
journal = {NIPS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,gem},
title = {{Gradient Episodic Memory for Continual Learning}},
url = {http://arxiv.org/abs/1706.08840},
year = {2017}
}
@article{Farquhar2019a,
abstract = {Experiments used in current continual learning research do not faithfully assess fundamental challenges of learning continually. Instead of assessing performance on challenging and representative experiment designs, recent research has focused on increased dataset difficulty, while still using flawed experiment set-ups. We examine standard evaluations and show why these evaluations make some continual learning approaches look better than they are. We introduce desiderata for continual learning evaluations and explain why their absence creates misleading comparisons. Based on our desiderata we then propose new experiment designs which we demonstrate with various continual learning approaches and datasets. Our analysis calls for a reprioritization of research effort by the community.},
annote = {arXiv: 1805.09733},
author = {Farquhar, Sebastian and Gal, Yarin},
journal = {Privacy in Machine Learning and Artificial Intelligence workshop, ICML},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,critique,evaluation,metrics},
month = {jun},
title = {{Towards Robust Evaluations of Continual Learning}},
url = {http://arxiv.org/abs/1805.09733},
year = {2019}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,annotated,ewc},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@article{Parisi2018a,
abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.},
author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
doi = {10.3389/fnbot.2018.00078},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {CLS,Incremental Learning,Lifelong learning,Memory,Self-organizing Network,core50,dual,object recognition systems,som},
language = {English},
mendeley-tags = {core50,dual,som},
title = {{Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization}},
url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},
volume = {12},
year = {2018}
}
@misc{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
booktitle = {arXiv: 1606.04671 [cs]},
keywords = {Computer Science - Machine Learning,lifelong learning,mnist,modular,progressive},
language = {en},
mendeley-tags = {mnist},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@article{Wang2019a,
abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4{\%} of the size of all previous training data matches the performance of refining the classifier keeping 20{\%} of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
annote = {arXiv: 1906.00654},
author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
journal = {arXiv: 1906.00654 [cs, eess, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,Statistics - Machine Learning,audio,sequence,sequences,time series},
month = {jun},
title = {{Continual Learning of New Sound Classes using Generative Replay}},
url = {http://arxiv.org/abs/1906.00654},
year = {2019}
}
@article{Cui2016a,
abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
annote = {Publisher: MIT Press},
author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
doi = {10.1162/NECO_a_00893},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {htm,spiking},
mendeley-tags = {spiking},
month = {sep},
number = {11},
pages = {2474--2504},
title = {{Continuous Online Sequence Learning with an Unsupervised Neural Network Model}},
url = {https://doi.org/10.1162/NECO{\_}a{\_}00893},
volume = {28},
year = {2016}
}
@misc{Ororbia2020a,
abstract = {For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.},
annote = {Comment: Revised version of manuscript – includes updated experimental results
arXiv: 1908.08655},
author = {Ororbia, Alexander},
booktitle = {arXiv:1908.08655 [cs, q-bio]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition},
month = {jan},
title = {{Spiking Neural Predictive Coding for Continual Learning from Data Streams}},
url = {http://arxiv.org/abs/1908.08655},
year = {2020}
}
@article{He2019a,
abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering – i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.},
annote = {arXiv: 1906.05201},
author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
journal = {arXiv:1906.05201 [cs, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning},
month = {jun},
title = {{Task Agnostic Continual Learning via Meta Learning}},
url = {http://arxiv.org/abs/1906.05201},
year = {2019}
}
@inproceedings{Kemker2018a,
abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.},
author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {audioset,kemker,mnist,review,survey},
language = {en},
mendeley-tags = {mnist},
month = {apr},
title = {{Measuring Catastrophic Forgetting in Neural Networks}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410},
year = {2018}
}
@article{French1997a,
abstract = {In order to solve the “sensitivity-stability” problem — and its immediate correlate, the problem of sequential learning — it is crucial to develop connectionist architectures that are simultaneously sensitive to, but not excessively disrupted by, new input. French (1992) suggested that to alleviate a particularly severe form of this disruption, catastrophic forgetting, it was necessary for networks to dynamically separate their internal representations during learning. McClelland, McNaughton, {\&} O'Reilly (1995) went even further. They suggested that nature's way of implementing this obligatory separation was the evolution of two separate areas of the brain, the hippocampus and the neocortex. In keeping with this idea of radical separation, a “pseudo-recurrent” memory model is presented here that partitions a connectionist network into two functionally distinct, but continually interacting areas. One area serves as a final-storage area for representations; the other is an early-processing area where new representations are first learned by the system. The final-storage area continually supplies internally generated patterns (pseudopatterns, Robins (1995)), which are approximations of its content, to the early-processing area, where they are interleaved with the new patterns to be learned. Transfer of the new learning is done either by weight-copying from the early-processing area to the final-storage area or by pseudopattern transfer. A number of experiments are presented that demonstrate the effectiveness of this approach, allowing, in particular, effective sequential learning with gradual forgetting in the presence of new input. Finally, it is shown that the two interacting areas automatically produce representational compaction and it is suggested that similar representational streamlining may exist in the brain.},
annote = {In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon},
author = {French, Robert},
doi = {10.1080/095400997116595},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
keywords = {Catastrophic Interference,Dual Memory,Keywords: Pseudopatterns,Semi-distributed Representations,Sensitivity-stability Transfer,dilemma,plasticity,stability},
language = {en},
month = {dec},
number = {4},
pages = {353--380},
shorttitle = {Pseudo-recurrent Connectionist Networks},
title = {{Pseudo-recurrent Connectionist Networks: An Approach to the 'Sensitivity-Stability' Dilemma}},
url = {http://www.tandfonline.com/doi/abs/10.1080/095400997116595},
volume = {9},
year = {1997}
}
@inproceedings{Miconi2019a,
abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
booktitle = {ICLR},
file = {::},
keywords = {fashion,mnist,spiking},
mendeley-tags = {fashion,mnist,spiking},
month = {sep},
title = {{Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity}},
year = {2019}
}
@article{Miconi2016a,
author = {Miconi, Thomas},
journal = {NIPS Workshop - Continual Learning},
keywords = {hebbian,workshop},
language = {en},
pages = {5},
title = {{Backpropagation of Hebbian plasticity for continual learning}},
year = {2016}
}
@phdthesis{lomonaco2019a,
abstract = {Humans have the extraordinary ability to learn continually from experience. Not only we can apply previously learned knowledge and skills to new situations, we can also use these as the foundation for later learning. One of the grand goals of Artificial Intelligence (AI) is building an artificial “continual learning” agent that constructs a sophisticated understanding of the world from its own experience through the autonomous incremental development of ever more complex knowledge and skills. However, despite early speculations and few pioneering works, very little research and effort has been devoted to address this vision. Current AI systems greatly suffer from the exposure to new data or environments which even slightly differ from the ones for which they have been trained for. Moreover, the learning process is usually constrained on fixed datasets within narrow and isolated tasks which may hardly lead to the emergence of more complex and autonomous intelligent behaviors. In essence, continual learning and adaptation capabilities, while more than often thought as fundamental pillars of every intelligent agent, have been mostly left out of the main AI research focus. In this dissertation, we study the application of these ideas in light of the more recent advances in machine learning research and in the context of deep architectures for AI. We propose a comprehensive and unifying framework for continual learning, new metrics, benchmarks and algorithms, as well as providing substantial experimental evaluations in different supervised, unsupervised and reinforcement learning tasks.},
author = {Lomonaco, Vincenzo},
doi = {Lomonaco, Vincenzo (2019) Continual Learning with Deep Architectures, [Dissertation thesis], Alma Mater Studiorum Università di Bologna. Dottorato di ricerca in Computer science and engineering <http://amsdottorato.unibo.it/view/dottorati/DOT536/>, 31 Ciclo. DOI 10.6092/unibo/amsdottorato/9073.},
language = {it},
month = {apr},
school = {alma},
title = {{Continual Learning with Deep Architectures}},
url = {http://amsdottorato.unibo.it/9073/},
year = {2019}
}
@inproceedings{Zenke2017a,
abstract = {While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biologica...},
author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
booktitle = {International Conference on Machine Learning},
keywords = {mnist},
language = {en},
month = {jul},
pages = {3987--3995},
title = {{Continual Learning Through Synaptic Intelligence}},
url = {http://proceedings.mlr.press/v70/zenke17a.html},
year = {2017}
}
@inproceedings{khurram2019a,
abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite-they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. 1},
author = {Javed, Khurram and White, Martha},
booktitle = {NeurIPS},
file = {::},
title = {{Meta-Learning Representations for Continual Learning}},
url = {https://github.com/khurramjaved96/mrcl},
year = {2019}
}
@misc{Parisi2020a,
abstract = {Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate $\backslash$textit{\{}catastrophic forgetting{\}} in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.},
annote = {Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896
arXiv: 2003.09114},
author = {Parisi, German I and Lomonaco, Vincenzo},
booktitle = {arXiv:2003.09114 [cs]},
doi = {10.1007/978-3-030-43883-8_8},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi},
month = {mar},
title = {{Online Continual Learning on Sequences}},
url = {http://arxiv.org/abs/2003.09114},
year = {2020}
}
@misc{lee2018a,
abstract = {While end-to-end neural conversation models have led to promising advances in reducing hand-crafted features and errors induced by the traditional complex system architecture, they typically require an enormous amount of data due to the lack of modularity. Previous studies adopted a hybrid approach with knowledge-based components either to abstract out domain-specific information or to augment data to cover more diverse patterns. On the contrary, we propose to directly address the problem using recent developments in the space of continual learning for neural models. Specifically, we adopt a domain-independent neural conversational model and introduce a novel neural continual learning algorithm that allows a conversational agent to accumulate skills across different tasks in a data-efficient way. To the best of our knowledge, this is the first work that applies continual learning to conversation systems. We verified the efficacy of our method through a conversational skill transfer from either synthetic dialogs or human-human dialogs to human-computer conversations in a customer support domain.},
annote = {arXiv: 1712.09943},
author = {Lee, Sungjin},
booktitle = {arXiv:1712.09943 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,chatbot,conversation,conversational agent,ewc,lstm,nlp},
mendeley-tags = {nlp},
month = {jan},
title = {{Toward Continual Learning for Conversational Agents}},
url = {http://arxiv.org/abs/1712.09943},
year = {2018}
}
@article{Lesort2020b,
abstract = {In most machine learning algorithms, training data are assumed independent and identically distributed (iid). Otherwise, the algorithms' performances are challenged. A famous phenomenon with non-iid data distribution is known as $\backslash$say{\{}catastrophic forgetting{\}}. Algorithms dealing with it are gathered in the $\backslash$textit{\{}Continual Learning{\}} research field. In this article, we study the $\backslash$textit{\{}regularization{\}} based approaches to continual learning. We show that those approaches can not learn to discriminate classes from different tasks in an elemental continual benchmark: class-incremental setting. We make theoretical reasoning to prove this shortcoming and illustrate it with examples and experiments.},
annote = {arXiv: 1912.03049},
author = {Lesort, Timoth{\'{e}}e and Stoian, Andrei and Filliat, David},
journal = {arXiv:1912.03049 [cs, stat]},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,class incremental,regularization},
month = {feb},
title = {{Regularization Shortcomings for Continual Learning}},
url = {http://arxiv.org/abs/1912.03049},
year = {2020}
}
@inproceedings{Toneva2018a,
abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a...},
annote = {An interesting aspect of this paper is related to the study of unforgettable patterns and how they influence performance in terms of forgetting.},
author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
booktitle = {International Conference on Learning Representations},
keywords = {cifar,mnist},
mendeley-tags = {cifar,mnist},
month = {sep},
title = {{An Empirical Study of Example Forgetting during Deep Neural Network Learning}},
url = {https://openreview.net/forum?id=BJlxm30cKm},
year = {2019}
}
@inproceedings{khurram2019a,
abstract = {A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite-they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. 1},
author = {Javed, Khurram and White, Martha},
booktitle = {NeurIPS},
title = {{Meta-Learning Representations for Continual Learning}},
url = {https://github.com/khurramjaved96/mrcl},
year = {2019}
}
@inproceedings{Aljundi2019c,
abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...},
annote = {The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.},
author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {ICLR},
keywords = {cifar,mnist,sparsity},
mendeley-tags = {cifar,mnist,sparsity},
title = {{Selfless Sequential Learning}},
url = {https://openreview.net/forum?id=Bkxbrn0cYX},
year = {2019}
}
@inproceedings{Schak2019a,
abstract = {We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).},
address = {Cham},
author = {Schak, Monika and Gepperth, Alexander},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning},
doi = {10.1007/978-3-030-30484-3_56},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30484-3},
keywords = {Catastrophic Forgetting,LSTM,rnn,sequential},
language = {en},
mendeley-tags = {rnn},
pages = {714--728},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{A Study on Catastrophic Forgetting in Deep LSTM Networks}},
year = {2019}
}
@article{VanDeVen2018a,
abstract = {Standard artificial neural networks suffer from the well-known issue of catastrophic forgetting, making continual or lifelong learning difficult for machine learning. In recent years, numerous methods have been proposed for continual learning, but due to differences in evaluation protocols it is difficult to directly compare their performance. To enable more structured comparisons, we describe three continual learning scenarios based on whether at test time task identity is provided and–in case it is not–whether it must be inferred. Any sequence of well-defined tasks can be performed according to each scenario. Using the split and permuted MNIST task protocols, for each scenario we carry out an extensive comparison of recently proposed continual learning methods. We demonstrate substantial differences between the three scenarios in terms of difficulty and in terms of how efficient different methods are. In particular, when task identity must be inferred (i.e., class incremental learning), we find that regularization-based approaches (e.g., elastic weight consolidation) fail and that replaying representations of previous experiences seems required for solving this scenario.},
annote = {Comment: Extended version of work presented at the NeurIPS Continual Learning workshop (2018); 18 pages, 5 figures, 6 tables. Related to arXiv:1809.10635
arXiv: 1904.07734},
author = {van de Ven, Gido M and Tolias, Andreas S},
journal = {Continual Learning Workshop NeurIPS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning},
title = {{Three scenarios for continual learning}},
url = {http://arxiv.org/abs/1904.07734},
year = {2018}
}
@incollection{French2019a,
abstract = {It is well known that when a connectionist network is trained on one set of patterns and then attempts to add new patterns to its repertoire, catastrophic interference may result. The use of sparse, orthogonal hidden-layer representations has been shown to reduce catastrophic interference. The author demonstrates that the use of sparse representations may, in certain cases, actually result in worse performance on catastrophic interference. This paper argues for the necessity of maintaining hidden-layer representations that are both as highly distributed and as highly orthogonal as possible. The author presents a learning algorithm, called context-biasing, that dynamically solves the problem of constraining hiddenlayer representations to simultaneously produce good orthogonality and distributedness. On the data tested for this study, context-biasing is shown to reduce catastrophic interference by more than 50{\%} compared to standard backpropagation. In particular, this technique succeeds in reducing catastrophic interference on data where sparse, orthogonal distributions failed to produce any improvement.},
author = {French, Robert M},
booktitle = {Proceedings of the Sixteenth Annual Conference of the Cognitive Science Society},
doi = {10.4324/9781315789354-58},
edition = {1},
editor = {Ram, Ashwin and Eiselt, Kurt},
isbn = {978-1-315-78935-4},
keywords = {sparsity},
language = {en},
mendeley-tags = {sparsity},
month = {may},
pages = {335--340},
publisher = {Routledge},
title = {{Dynamically constraining connectionist networks to produce distributed, orthogonal representations to reduce catastrophic interference}},
url = {https://www.taylorfrancis.com/books/9781317729266/chapters/10.4324/9781315789354-58},
year = {2019}
}
@article{Lesort2020a,
abstract = {Continual learning (CL) is a particular machine learning paradigm where the data distribution and learning objective change through time, or where all the training data and objective criteria are never available at once. The evolution of the learning process is modeled by a sequence of learning experiences where the goal is to be able to learn new skills all along the sequence without forgetting what has been previously learned. CL can be seen as an online learning where knowledge fusion needs to take place in order to learn from streams of data presented sequentially in time. Continual learning also aims at the same time at optimizing the memory, the computation power and the speed during the learning process. An important challenge for machine learning is not necessarily finding solutions that work in the real world but rather finding stable algorithms that can learn in real world. Hence, the ideal approach would be tackling the real world in a embodied platform: an autonomous agent. Continual learning would then be effective in an autonomous agent or robot, which would learn autonomously through time about the external world, and incrementally develop a set of complex skills and knowledge.Robotic agents have to learn to adapt and interact with their environment using a continuous stream of observations. Some recent approaches aim at tackling continual learning for robotics, but most recent papers on continual learning only experiment approaches in simulation or with static datasets. Unfortunately, the evaluation of those algorithms does not provide insights on whether their solutions may help continual learning in the context of robotics. This paper aims at reviewing the existing state of the art of continual learning, summarizing existing benchmarks and metrics, and proposing a framework for presenting and evaluating both robotics and non robotics approaches in a way that makes transfer between both fields easier. We put light on continual learning in the context of robotics to create connections between fields and normalize approaches.},
annote = {Overview of a CL framework for applications in robotic, together with discussion of existing CL strategies and techniques.},
author = {Lesort, Timoth{\'{e}}e and Lomonaco, Vincenzo and Stoian, Andrei and Maltoni, Davide and Filliat, David and D{\'{i}}az-Rodr{\'{i}}guez, Natalia},
doi = {10.1016/j.inffus.2019.12.004},
issn = {1566-2535},
journal = {Information Fusion},
keywords = {Catastrophic Forgetting,Continual Learning,Deep Learning,Lifelong Learning,Reinforcement Learning,Robotics,framework},
language = {en},
mendeley-tags = {framework},
month = {jun},
pages = {52--68},
shorttitle = {Continual learning for robotics},
title = {{Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges}},
url = {http://www.sciencedirect.com/science/article/pii/S1566253519307377},
volume = {58},
year = {2020}
}
@article{French1999a,
author = {French, Robert},
doi = {10.1016/S1364-6613(99)01294-2},
issn = {1364-6613, 1879-307X},
journal = {Trends in Cognitive Sciences},
keywords = {Catastrophic forgetting,Connectionism,Connectionist networks,Interference,Learning,Memory,Neuroscience,biology},
language = {English},
month = {apr},
number = {4},
pages = {128--135},
pmid = {10322466},
title = {{Catastrophic forgetting in connectionist networks}},
url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2},
volume = {3},
year = {1999}
}
@inproceedings{Liu2018a,
abstract = {In this paper we propose an approach to avoiding catastrophic forgetting in sequential task learning scenarios. Our technique is based on a network reparameterization that approximately diagonalizes the Fisher Information Matrix of the network parameters. This reparameterization takes the form of a factorized rotation of parameter space which, when used in conjunction with Elastic Weight Consolidation (which assumes a diagonal Fisher Information Matrix), leads to significantly better performance on lifelong learning of sequential tasks. Experimental results on the MNIST, CIFAR-100, CUB-200 and Stanford-40 datasets demonstrate that we significantly improve the results of standard elastic weight consolidation, and that we obtain competitive results when compared to the state-of-the-art in lifelong learning without forgetting.},
annote = {ISSN: 1051-4651},
author = {Liu, Xialei and Masana, Marc and Herranz, Luis and de Weijer, Joost and L{\'{o}}pez, Antonio M and Bagdanov, Andrew D},
booktitle = {2018 24th International Conference on Pattern Recognition (ICPR)},
doi = {10.1109/ICPR.2018.8545895},
keywords = {Computer vision,Data models,Fisher Information Matrix,Neural networks,Standards,Stanford-40 datasets,Task analysis,Training,Training data,ewc,fisher,image classification,learning (artificial intelligence),matrix algebra,network parameters,network reparameterization,sequential tasks,standard elastic weight consolidation},
month = {aug},
pages = {2262--2268},
shorttitle = {Rotate your Networks},
title = {{Rotate your Networks: Better Weight Consolidation and Less Catastrophic Forgetting}},
year = {2018}
}
@inproceedings{beaulieu2020a,
abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
archivePrefix = {arXiv},
arxivId = {2002.09571},
author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
booktitle = {ECAI},
eprint = {2002.09571},
month = {feb},
title = {{Learning to Continually Learn}},
url = {http://arxiv.org/abs/2002.09571},
year = {2020}
}
@inproceedings{Schwarz2018a,
abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
booktitle = {International Conference on Machine Learning},
keywords = {ewc,normalized ewc,online ewc},
language = {en},
month = {jul},
pages = {4528--4537},
shorttitle = {Progress {\&} Compress},
title = {{Progress {\&} Compress: A scalable framework for continual learning}},
url = {http://proceedings.mlr.press/v80/schwarz18a.html},
year = {2018}
}
@inproceedings{beaulieu2020a,
abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables context-dependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
archivePrefix = {arXiv},
arxivId = {2002.09571},
author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
booktitle = {ECAI},
eprint = {2002.09571},
file = {::},
month = {feb},
title = {{Learning to Continually Learn}},
url = {http://arxiv.org/abs/2002.09571},
year = {2020}
}
@article{Pomponi2020a,
abstract = {Continual learning of deep neural networks is a key requirement for scaling them up to more complex applicative scenarios and for achieving real lifelong learning of these architectures. Previous approaches to the problem have considered either the progressive increase in the size of the networks, or have tried to regularize the network behavior to equalize it with respect to previously observed tasks. In the latter case, it is essential to understand what type of information best represents this past behavior. Common techniques include regularizing the past outputs, gradients, or individual weights. In this work, we propose a new, relatively simple and efficient method to perform continual learning by regularizing instead the network internal embeddings. To make the approach scalable, we also propose a dynamic sampling strategy to reduce the memory footprint of the required external storage. We show that our method performs favorably with respect to state-of-the-art approaches in the literature, while requiring significantly less space in memory and computational time. In addition, inspired by to recent works, we evaluate the impact of selecting a more flexible model for the activation functions inside the network, evaluating the impact of catastrophic forgetting on the activation functions themselves.},
author = {Pomponi, Jary and Scardapane, Simone and Lomonaco, Vincenzo and Uncini, Aurelio},
doi = {10.1016/j.neucom.2020.01.093},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Catastrophic forgetting,Continual learning,Embedding,Regularization,Trainable activation functions},
language = {en},
month = {feb},
title = {{Efficient continual learning in neural networks with embedding regularization}},
url = {http://www.sciencedirect.com/science/article/pii/S092523122030151X},
year = {2020}
}
@inproceedings{Oswal2019a,
abstract = {Artificial neural networks suffer from catastrophic forgetting when they are sequentially trained on multiple tasks. To overcome this problem, we present a novel approach based on task-conditioned...},
author = {von Oswald, Johannes and Henning, Christian and Sacramento, Jo{\~{a}}o and Grewe, Benjamin F},
booktitle = {International Conference on Learning Representations},
month = {sep},
title = {{Continual learning with hypernetworks}},
url = {https://openreview.net/forum?id=SJgwNerKvB},
year = {2020}
}
@inproceedings{Draelos2017a,
abstract = {Neural machine learning methods, such as deep neural networks (DNN), have achieved remarkable success in a number of complex data processing tasks. These methods have arguably had their strongest impact on tasks such as image and audio processing - data processing domains in which humans have long held clear advantages over conventional algorithms. In contrast to biological neural systems, which are capable of learning continuously, deep artificial networks have a limited ability for incorporating new information in an already trained network. As a result, methods for continuous learning are potentially highly impactful in enabling the application of deep networks to dynamic data sets. Here, inspired by the process of adult neurogenesis in the hippocampus, we explore the potential for adding new neurons to deep layers of artificial neural networks in order to facilitate their acquisition of novel information while preserving previously trained data representations. Our results on the MNIST handwritten digit dataset and the NIST SD 19 dataset, which includes lower and upper case letters and digits, demonstrate that neurogenesis is well suited for addressing the stability-plasticity dilemma that has long challenged adaptive machine learning algorithms.},
annote = {The neurogenesis algorithm selectively expand the original multi-layer autoencoder at the neuron level depending on its reconstruction performance measured at each layer. The model is capable of maintaining plasticity while mitigating forgetting through replay of old samples.},
author = {Draelos, Timothy John and Miner, Nadine E and Lamb, Christopher and Cox, Jonathan A and Vineyard, Craig Michael and Carlson, Kristofor David and Severa, William Mark and James, Conrad D and Aimone, James Bradley},
booktitle = {IJCNN},
keywords = {autoencoder,autoencoders,neurogenesis,reconstruction},
language = {English},
title = {{Neurogenesis Deep Learning}},
url = {https://www.osti.gov/biblio/1424868},
year = {2017}
}
@article{Maltoni2018a,
abstract = {It was recently shown that architectural, regularization and rehearsal strategies can be used to train deep models sequentially on a number of disjoint tasks without forgetting previously acquired knowledge. However, these strategies are still unsatisfactory if the tasks are not disjoint but constitute a single incremental task (e.g., class-incremental learning). In this paper we point out the differences between multi-task and single-incremental-task scenarios and show that well-known approaches such as LWF, EWC and SI are not ideal for incremental task scenarios. A new approach, denoted as AR1, combining architectural and regularization strategies is then speciﬁcally proposed. AR1 overhead (in terms of memory and computation) is very small thus making it suitable for online learning. When tested on CORe50 and iCIFAR-100, AR1 outperformed existing regularization strategies by a good margin.},
annote = {Comment: 26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4), several typos and minor mistakes corrected
arXiv: 1806.08568},
author = {Maltoni, Davide and Lomonaco, Vincenzo},
journal = {arXiv:1806.08568 [cs, stat]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Continuous learning,Deep learning,Incremental class learning,Lifelong learning,Object recognition,Single-incremental-task,Statistics - Machine Learning,ewc,incremental task,review},
language = {en},
month = {jun},
title = {{Continuous Learning in Single-Incremental-Task Scenarios}},
url = {http://arxiv.org/abs/1806.08568},
year = {2018}
}
@inproceedings{Yoon2018,
abstract = {We propose a novel deep network architecture for lifelong learning which we refer to as Dynamically Expandable Network (DEN), that can dynamically decide its network capacity as it trains on a sequence of tasks, to learn a compact overlapping knowledge sharing structure among tasks. DEN is efﬁciently trained in an online manner by performing selective retraining, dynamically expands network capacity upon arrival of each task with only the necessary number of units, and effectively prevents semantic drift by splitting/duplicating units and timestamping them. We validate DEN on multiple public datasets under lifelong learning scenarios, on which it not only signiﬁcantly outperforms existing lifelong learning methods for deep networks, but also achieves the same level of performance as the batch counterparts with substantially fewer number of parameters. Further, the obtained network ﬁne-tuned on all tasks obtained siginﬁcantly better performance over the batch models, which shows that it can be used to estimate the optimal network structure even when all tasks are available in the ﬁrst place.},
annote = {The authors propose a method to evaluate the importance of each neuron in the network through the use of sparse connections. The network is then expanded based on the neuron importance for each task.},
author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
booktitle = {ICLR},
keywords = {cifar,disadvantages,lifelong learning,mnist,modular,progressive,sparsity},
language = {en},
mendeley-tags = {cifar,mnist,sparsity},
pages = {11},
title = {{Lifelong Learning With Dynamically Expandable Networks}},
year = {2018}
}
@inproceedings{Cossu2020a,
abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
annote = {An evaluation of RNNs (LSTM and LMN) inspired by Progressive networks, leading to the Gated Incremental Memory approach to overcome catastrophic forgetting.},
author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
booktitle = {Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020)},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,mnist,rnn},
mendeley-tags = {mnist,rnn},
month = {apr},
title = {{Continual Learning with Gated Incremental Memories for sequential data processing}},
url = {http://arxiv.org/abs/2004.04077},
year = {2020}
}
@article{Parisi2019a,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for computational learning systems and autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. Although significant advances have been made in domain-specific learning with neural networks, extensive research efforts are required for the development of robust lifelong learning on autonomous agents and robots. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
annote = {A general survey of the most important and used CL techniques, with a characterisation of the catastrophic forgetting phenomenon.},
author = {Parisi, German I and Kemker, Ronald and Part, Jose L and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012},
issn = {0893-6080},
journal = {Neural Networks},
keywords = {Catastrophic forgetting,Continual learning,Developmental systems,Lifelong learning,Memory consolidation},
language = {en},
month = {may},
pages = {54--71},
shorttitle = {Continual lifelong learning with neural networks},
title = {{Continual lifelong learning with neural networks: A review}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608019300231},
volume = {113},
year = {2019}
}
@incollection{Aljundi2019a,
author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {11849--11860},
publisher = {Curran Associates, Inc.},
title = {{Online Continual Learning with Maximal Interfered Retrieval}},
url = {http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf},
year = {2019}
}
@article{Chaudhry2019a,
abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz {\&} Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
annote = {Comment: Published as a conference paper at ICLR 2019
arXiv: 1812.00420},
author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
journal = {ICLR},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
language = {en},
title = {{Efficient Lifelong Learning with A-GEM}},
url = {http://arxiv.org/abs/1812.00420},
year = {2019}
}
@inproceedings{Chaudhry2018a,
abstract = {Incremental learning (il) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the il problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of il. The main challenge for an il algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, il also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of il algorithms. Furthermore, we present RWalk, a generalization of ewc++ (our efficient version of ewc [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various il algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.},
archivePrefix = {arXiv},
arxivId = {1801.10112},
author = {Chaudhry, Arslan and Dokania, Puneet K and Ajanthan, Thalaiyasingam and Torr, Philip H.S.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01252-6_33},
eprint = {1801.10112},
isbn = {9783030012519},
issn = {16113349},
keywords = {ewc,ewc++,fisher,forgetting},
pages = {556--572},
shorttitle = {Riemannian Walk for Incremental Learning},
title = {{Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence}},
url = {http://openaccess.thecvf.com/content{\_}ECCV{\_}2018/html/Arslan{\_}Chaudhry{\_}{\_}Riemannian{\_}Walk{\_}ECCV{\_}2018{\_}paper.html},
volume = {11215 LNCS},
year = {2018}
}
@article{Ring1997a,
abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
author = {Ring, Mark B},
doi = {10.1023/A:1007331723572},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Continual learning,cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},
language = {en},
month = {jul},
number = {1},
pages = {77--104},
shorttitle = {CHILD},
title = {{CHILD: A First Step Towards Continual Learning}},
url = {https://doi.org/10.1023/A:1007331723572},
volume = {28},
year = {1997}
}
@article{Robins1995a,
abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. W e then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old inform a tion.},
author = {Robins, Anthony},
doi = {10.1080/09540099550039318},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
language = {en},
month = {jun},
number = {2},
pages = {123--146},
title = {{Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},
volume = {7},
year = {1995}
}
@inproceedings{Coop2012a,
abstract = {In this paper we present the fixed expansion layer (FEL) feedforward neural network designed for balancing plasticity and stability in the presence of non-stationary inputs. Catastrophic interference (or catastrophic forgetting) refers to the drastic loss of previously learned information when a neural network is trained on new or different information. The goal of the FEL network is to reduce the effect of catastrophic interference by augmenting a multilayer perceptron with a layer of sparse neurons with binary activations. We compare the FEL network's performance to that of other algorithms designed to combat the effects of catastrophic interference and demonstrate that the FEL network is able to retain information for significantly longer periods of time with substantially lower computational requirements.},
annote = {ISSN: 1548-3746},
author = {Coop, Robert and Arel, Itamar},
booktitle = {2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS)},
doi = {10.1109/MWSCAS.2012.6292123},
keywords = {Accuracy,Biological neural networks,Feedforward neural networks,Interference,Neurons,Training,binary activations,catastrophic forgetting,catastrophic interference,fixed expansion layer feedforward neural network,multilayer perceptron,multilayer perceptrons,non-stationary inputs,sparse neurons,sparsity},
mendeley-tags = {sparsity},
month = {aug},
pages = {726--729},
title = {{Mitigation of catastrophic interference in neural networks using a fixed expansion layer}},
year = {2012}
}
@inproceedings{Miconi2018a,
abstract = {How can we build agents that keep learning from experience, quickly and efficiently, after their initial training? Here we take inspiration from the main mechanism of learning in biological brains:...},
author = {Miconi, Thomas and Stanley, Kenneth and Clune, Jeff},
booktitle = {International Conference on Machine Learning},
keywords = {plasticity,recurrent},
language = {en},
month = {jul},
pages = {3559--3568},
shorttitle = {Differentiable plasticity},
title = {{Differentiable plasticity: training plastic neural networks with backpropagation}},
url = {http://proceedings.mlr.press/v80/miconi18a.html},
year = {2018}
}
@article{Sodhani2020a,
abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step toward developing true lifelong learning systems, we unify gradient episodic memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both models are proposed in the context of feedforward networks, and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
doi = {10.1162/neco_a_01246},
issn = {1530888X},
journal = {Neural Computation},
keywords = {rnn},
mendeley-tags = {rnn},
month = {jan},
number = {1},
pages = {1--35},
publisher = {MIT Press Journals},
title = {{Toward training recurrent neural networks for lifelong learning}},
volume = {32},
year = {2020}
}
@article{Caccia2020a,
abstract = {Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.},
annote = {arXiv: 2003.05856},
author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},
journal = {arXiv:2003.05856 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,MAML,OSAKA,continual meta learning,framework,meta continual learning},
month = {mar},
shorttitle = {Online Fast Adaptation and Knowledge Accumulation},
title = {{Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning}},
url = {http://arxiv.org/abs/2003.05856},
year = {2020}
}
@misc{Ororbia2019a,
abstract = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications. However, training these models often relies on back-propagation through time, which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of back-propagation itself does not permit the use of non-differentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the Parallel Temporal Neural Coding Network (P-TNCN), a biologically inspired model trained by the learning algorithm we call Local Representation Alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation through time. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure to other back-propagation through time alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can in some instances outperform full back-propagation through time as well as variants such as sparse attentive back-tracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new datasets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.},
annote = {Comment: Important revisions made throughout (additional items/results added, including a complexity analysis)
arXiv: 1810.07411},
author = {Ororbia, Alexander and Mali, Ankur and Giles, C Lee and Kifer, Daniel},
booktitle = {arXiv:1810.07411 [cs]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,credi assignment,mnist,spiking},
mendeley-tags = {mnist,spiking},
month = {aug},
title = {{Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations}},
url = {http://arxiv.org/abs/1810.07411},
year = {2019}
}
@article{kruszewski2020a,
abstract = {Continual Learning has been often framed as the problem of training a model in a sequence of tasks. In this regard, Neural Networks have been attested to forget the solutions to previous task as they learn new ones. Yet, modelling human life-long learning does not necessarily require any crisp notion of tasks. In this work, we propose a benchmark based on language modelling in a multilingual and multidomain setting that prescinds of any explicit delimitation of training examples into distinct tasks, and propose metrics to study continual learning and catastrophic forgetting in this setting. Then, we introduce a simple Product of Experts learning system that performs strongly on this problem while displaying interesting properties, and investigate its merits for avoiding forgetting.},
annote = {arXiv: 2004.03340},
author = {Kruszewski, Germ{\'{a}}n and Sorodoc, Ionut-Teodor and Mikolov, Tomas},
journal = {arXiv:2004.03340 [cs]},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,expert,mixture,nlp},
mendeley-tags = {nlp},
month = {apr},
title = {{Class-Agnostic Continual Learning of Alternating Languages and Domains}},
url = {http://arxiv.org/abs/2004.03340},
year = {2020}
}
@inproceedings{French1991a,
abstract = {In connectionist networks, newly-learned information destroys previously-learned information unless the network is continually retrained on the old information. This behavior, known as catastrophic forgetting, is unacceptable both for practical purposes and as a model of mind. This paper advances the claim that catastrophic forgetting is a direct consequence of the overlap of the system's distributed representations and can be reduced by reducing this overlap. A simple algorithm is presented that allows a standard feedforward backpropagation network to develop semi-distributed representations, thereby significantly reducing the problem of catastrophic forgetting. 1 Introduction Catastrophic forgetting is the inability of a neural network to retain old information in the presence of new. New information destroys old unless the old information is continually relearned by the net. McCloskey {\&} Cohen [1990] and Ratcliff [1989] have demonstrated that this is a serious problem with c...},
author = {French, Robert M},
booktitle = {In Proceedings of the 13th Annual Cognitive Science Society Conference},
keywords = {activation sharpening,sparsity},
mendeley-tags = {sparsity},
pages = {173--178},
publisher = {Erlbaum},
title = {{Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks}},
year = {1991}
}
@inproceedings{Asghar2019a,
abstract = {This paper addresses the problem of incremental domain adaptation (IDA) in natural language processing (NLP). We assume each domain comes one after another, and that we could only access data in...},
annote = {The authors leverage a Recurrent Neural Network with an explicit memory (memory banks) which grows when new computational capabilities are needed. Attention mechanisms are exploited in order to focus on specific component of previous memories.},
author = {Asghar, Nabiha and Mou, Lili and Selby, Kira A and Pantasdo, Kevin D and Poupart, Pascal and Jiang, Xin},
booktitle = {International Conference on Learning Representations},
keywords = {rnn},
mendeley-tags = {rnn},
month = {sep},
title = {{Progressive Memory Banks for Incremental Domain Adaptation}},
url = {https://openreview.net/forum?id=BkepbpNFwr},
year = {2019}
}
@inproceedings{Rebuffi2017a,
author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {jul},
title = {{iCaRL: Incremental Classifier and Representation Learning}},
year = {2017}
}
@inproceedings{sun2019a,
abstract = {Most research on lifelong learning applies to images or games, but not language. We present LAMOL, a simple yet effective method for lifelong language learning (LLL) based on language...},
author = {Sun, Fan-Keng and Ho, Cheng-Hao and Lee, Hung-Yi},
keywords = {nlp},
mendeley-tags = {nlp},
month = {sep},
shorttitle = {LAMOL},
title = {{LAMOL: LAnguage MOdeling for Lifelong Language Learning}},
url = {https://openreview.net/forum?id=Skgxcn4YDS},
year = {2019}
}
@article{caccia2020a,
abstract = {Learning from non-stationary data remains a great challenge for machine learning. Continual learning addresses this problem in scenarios where the learning agent faces a stream of changing tasks. In these scenarios, the agent is expected to retain its highest performance on previous tasks without revisiting them while adapting well to the new tasks. Two new recent continual-learning scenarios have been proposed. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting when trained on a sequence of tasks. In continual-meta learning, the goal is faster remembering, i.e., focusing on how quickly the agent recovers performance rather than measuring the agent's performance without any adaptation. Both scenarios have the potential to propel the field forward. Yet in their original formulations, they each have limitations. As a remedy, we propose a more general scenario where an agent must quickly solve (new) out-of-distribution tasks, while also requiring fast remembering. We show that current continual learning, meta learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. Accordingly, we propose a strong baseline: Continual-MAML, an online extension of the popular MAML algorithm. In our empirical experiments, we show that our method is better suited to the new scenario than the methodologies mentioned above, as well as standard continual learning and meta learning approaches.},
archivePrefix = {arXiv},
arxivId = {2003.05856},
author = {Caccia, Massimo and Rodriguez, Pau and Ostapenko, Oleksiy and Normandin, Fabrice and Lin, Min and Caccia, Lucas and Laradji, Issam and Rish, Irina and Lacoste, Alexande and Vazquez, David and Charlin, Laurent},
eprint = {2003.05856},
file = {::},
month = {mar},
title = {{Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning}},
url = {http://arxiv.org/abs/2003.05856},
year = {2020}
}
@incollection{Shrestha2018a,
author = {Shrestha, Sumit Bam and Orchard, Garrick},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
pages = {1412--1421},
publisher = {Curran Associates, Inc.},
shorttitle = {SLAYER},
title = {{SLAYER: Spike Layer Error Reassignment in Time}},
url = {http://papers.nips.cc/paper/7415-slayer-spike-layer-error-reassignment-in-time.pdf},
year = {2018}
}
@article{Abbott2000a,
abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them—synaptic scaling, spike-timing dependent plasticity and synaptic redistribution—and discuss their functional implications.},
author = {Abbott, L F and Nelson, Sacha B},
doi = {10.1038/81453},
issn = {1546-1726},
journal = {Nature Neuroscience},
language = {en},
month = {nov},
number = {11},
pages = {1178--1183},
shorttitle = {Synaptic plasticity},
title = {{Synaptic plasticity: taming the beast}},
url = {https://www.nature.com/articles/nn1100{\_}1178},
volume = {3},
year = {2000}
}
@incollection{Shin2017a,
author = {Shin, Hanul and Lee, Jung Kwon and Kim, Jaehong and Kim, Jiwon},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and Fergus, R and Vishwanathan, S and Garnett, R},
pages = {2990--2999},
publisher = {Curran Associates, Inc.},
title = {{Continual Learning with Deep Generative Replay}},
url = {http://papers.nips.cc/paper/6892-continual-learning-with-deep-generative-replay.pdf},
year = {2017}
}
@inproceedings{Thrun1996a,
author = {Thrun, Sebastian},
booktitle = {Advances in Neural Information Processing Systems 8},
editor = {Touretzky, D S and Mozer, M C and Hasselmo, M E},
keywords = {lifelong,lifelong learning},
pages = {640--646},
publisher = {MIT Press},
title = {{Is Learning The n-th Thing Any Easier Than Learning The First?}},
url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
year = {1996}
}
@article{Li2016a,
abstract = {When building a uniﬁed vision system or gradually adding new capabilities to a system, the usual assumption is that training data for all tasks is always available. However, as the number of tasks grows, storing and retraining on such data becomes infeasible. A new problem arises where we add new capabilities to a Convolutional Neural Network (CNN), but the training data for its existing capabilities are unavailable. We propose our Learning without Forgetting method, which uses only new task data to train the network while preserving the original capabilities. Our method performs favorably compared to commonly used feature extraction and ﬁne-tuning adaption techniques and performs similarly to multitask learning that uses original task data we assume unavailable. A more surprising observation is that Learning without Forgetting may be able to replace ﬁne-tuning with similar old and new task datasets for improved new task performance.},
annote = {Comment: Conference version appears in ECCV 2016; updated with journal version
arXiv: 1606.09282},
author = {Li, Zhizhong and Hoiem, Derek},
journal = {European Conference on Computer Vision},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Statistics - Machine Learning},
language = {en},
pages = {614--629},
series = {Springer},
title = {{Learning without Forgetting}},
url = {http://arxiv.org/abs/1606.09282},
year = {2016}
}
@inproceedings{Aljundi2019d,
author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d$\backslash$textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {11816--11825},
publisher = {Curran Associates, Inc.},
title = {{Gradient based sample selection for online continual learning}},
url = {http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf},
year = {2019}
}
@article{Wang2019a,
abstract = {Continual learning consists in incrementally training a model on a sequence of datasets and testing on the union of all datasets. In this paper, we examine continual learning for the problem of sound classification, in which we wish to refine already trained models to learn new sound classes. In practice one does not want to maintain all past training data and retrain from scratch, but naively updating a model with new data(sets) results in a degradation of already learned tasks, which is referred to as "catastrophic forgetting." We develop a generative replay procedure for generating training audio spectrogram data, in place of keeping older training datasets. We show that by incrementally refining a classifier with generative replay a generator that is 4{\%} of the size of all previous training data matches the performance of refining the classifier keeping 20{\%} of all previous training data. We thus conclude that we can extend a trained sound classifier to learn new classes without having to keep previously used datasets.},
annote = {arXiv: 1906.00654},
author = {Wang, Zhepei and Subakan, Cem and Tzinis, Efthymios and Smaragdis, Paris and Charlin, Laurent},
journal = {arXiv: 1906.00654 [cs, eess, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio,Statistics - Machine Learning,audio,sequence,sequences,time series},
month = {jun},
title = {{Continual Learning of New Sound Classes using Generative Replay}},
url = {http://arxiv.org/abs/1906.00654},
year = {2019}
}
@book{Hebb2005a,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology–a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
author = {Hebb, D O},
isbn = {978-1-135-63191-8},
keywords = {Psychology / Cognitive Psychology {\&} Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology},
language = {en},
month = {apr},
publisher = {Psychology Press},
shorttitle = {The Organization of Behavior},
title = {{The Organization of Behavior: A Neuropsychological Theory}},
year = {2005}
}
@inproceedings{riemer2019a,
abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
booktitle = {ICLR},
month = {sep},
title = {{Learning to learn without forgetting by maximizing transfer and minimizing interference}},
year = {2019}
}
