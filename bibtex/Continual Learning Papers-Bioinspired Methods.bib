@article{Abbott2000a,
abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them—synaptic scaling, spike-timing dependent plasticity and synaptic redistribution—and discuss their functional implications.},
author = {Abbott, L F and Nelson, Sacha B},
doi = {10.1038/81453},
issn = {1546-1726},
journal = {Nature Neuroscience},
keywords = {[hebbian]},
language = {en},
mendeley-tags = {[hebbian]},
month = {nov},
number = {11},
pages = {1178--1183},
shorttitle = {Synaptic plasticity},
title = {{Synaptic plasticity: taming the beast}},
url = {https://www.nature.com/articles/nn1100_1178},
volume = {3},
year = {2000}
}
@article{Ororbia2019b,
abstract = {In lifelong learning systems, especially those based on artificial neural networks, one of the biggest obstacles is the severe inability to retain old knowledge as new information is encountered. This phenomenon is known as catastrophic forgetting. In this paper, we present a new connectionist model, the Sequential Neural Coding Network, and its learning procedure, grounded in the neurocognitive theory of predictive coding. The architecture experiences significantly less forgetting as compared to standard neural models and outperforms a variety of previously proposed remedies and methods when trained across multiple task datasets in a stream-like fashion. The promising performance demonstrated in our experiments offers motivation that directly incorporating mechanisms prominent in real neuronal systems, such as competition, sparse activation patterns, and iterative input processing, can create viable pathways for tackling the challenge of lifelong machine learning.},
archivePrefix = {arXiv},
arxivId = {1905.10696},
author = {Ororbia, Alexander and Mali, Ankur and Kifer, Daniel and Giles, C Lee},
eprint = {1905.10696},
journal = {arXiv},
keywords = {[fashion],[mnist],[sparsity]},
mendeley-tags = {[fashion],[mnist],[sparsity]},
pages = {1--11},
title = {{Lifelong Neural Predictive Coding: Sparsity Yields Less Forgetting when Learning Cumulatively}},
url = {http://arxiv.org/abs/1905.10696},
year = {2019}
}
@inproceedings{Coop2012a,
abstract = {In this paper we present the fixed expansion layer (FEL) feedforward neural network designed for balancing plasticity and stability in the presence of non-stationary inputs. Catastrophic interference (or catastrophic forgetting) refers to the drastic loss of previously learned information when a neural network is trained on new or different information. The goal of the FEL network is to reduce the effect of catastrophic interference by augmenting a multilayer perceptron with a layer of sparse neurons with binary activations. We compare the FEL network's performance to that of other algorithms designed to combat the effects of catastrophic interference and demonstrate that the FEL network is able to retain information for significantly longer periods of time with substantially lower computational requirements.},
annote = {ISSN: 1548-3746},
author = {Coop, Robert and Arel, Itamar},
booktitle = {2012 IEEE 55th International Midwest Symposium on Circuits and Systems (MWSCAS)},
doi = {10.1109/MWSCAS.2012.6292123},
keywords = {Accuracy,Biological neural networks,Feedforward neural networks,Interference,Neurons,Training,[sparsity],binary activations,catastrophic forgetting,catastrophic interference,fixed expansion layer feedforward neural network,multilayer perceptron,multilayer perceptrons,non-stationary inputs,sparse neurons},
mendeley-tags = {[sparsity]},
month = {aug},
pages = {726--729},
title = {{Mitigation of catastrophic interference in neural networks using a fixed expansion layer}},
year = {2012}
}
@article{Cui2016a,
abstract = {The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams.},
annote = {Publisher: MIT Press},
author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
doi = {10.1162/NECO_a_00893},
issn = {0899-7667},
journal = {Neural Computation},
keywords = {[spiking],htm},
mendeley-tags = {[spiking]},
month = {sep},
number = {11},
pages = {2474--2504},
title = {{Continuous Online Sequence Learning with an Unsupervised Neural Network Model}},
url = {https://doi.org/10.1162/NECO_a_00893},
volume = {28},
year = {2016}
}
@inproceedings{Coop2013a,
abstract = {Catastrophic forgetting (or catastrophic interference) in supervised learning systems is the drastic loss of previously stored information caused by the learning of new information. While substantial work has been published on addressing catastrophic forgetting in memoryless supervised learning systems (e.g. feedforward neural networks), the problem has received limited attention in the context of dynamic systems, particularly recurrent neural networks. In this paper, we introduce a solution for mitigating catastrophic forgetting in RNNs based on enhancing the Fixed Expansion Layer (FEL) neural network which exploits sparse coding of hidden neuron activations. Simulation results on several non-stationary data sets clearly demonstrate the effectiveness of the proposed architecture.},
address = {Dallas, TX, USA},
author = {Coop, Robert and Arel, Itamar},
booktitle = {The 2013 International Joint Conference on Neural Networks (IJCNN)},
doi = {10.1109/IJCNN.2013.6707047},
isbn = {978-1-4673-6129-3 978-1-4673-6128-6},
keywords = {[mnist],[rnn],[sparsity],fel,recurrent fel},
language = {en},
mendeley-tags = {[mnist],[rnn],[sparsity]},
month = {aug},
pages = {1--7},
publisher = {IEEE},
title = {{Mitigation of catastrophic forgetting in recurrent neural networks using a Fixed Expansion Layer}},
url = {http://ieeexplore.ieee.org/document/6707047/},
year = {2013}
}
@article{Ororbia2020a,
abstract = {For energy-efficient computation in specialized neuromorphic hardware, we present the Spiking Neural Coding Network, an instantiation of a family of artificial neural models strongly motivated by the theory of predictive coding. The model, in essence, works by operating in a never-ending process of "guess-and-check", where neurons predict the activity values of one another and then immediately adjust their own activities to make better future predictions. The interactive, iterative nature of our neural system fits well into the continuous time formulation of data sensory stream prediction and, as we show, the model's structure yields a simple, local synaptic update rule, which could be used to complement or replace online spike-timing dependent plasticity. In this article, we experiment with an instantiation of our model that consists of leaky integrate-and-fire units. However, the general framework within which our model is situated can naturally incorporate more complex, formal neurons such as the Hodgkin-Huxley model. Our experimental results in pattern recognition demonstrate the potential of the proposed model when binary spike trains are the primary paradigm for inter-neuron communication. Notably, our model is competitive in terms of classification performance, can conduct online semi-supervised learning, naturally experiences less forgetting when learning from a sequence of tasks, and is more computationally economical and biologically-plausible than popular artificial neural networks.},
annote = {Comment: Revised version of manuscript – includes updated experimental results
arXiv: 1908.08655},
author = {Ororbia, Alexander},
journal = {arXiv},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Quantitative Biology - Neurons and Cognition,[spiking]},
mendeley-tags = {[spiking]},
month = {jan},
title = {{Spiking Neural Predictive Coding for Continual Learning from Data Streams}},
url = {http://arxiv.org/abs/1908.08655},
year = {2020}
}
@inproceedings{Aljundi2019c,
abstract = {Sequential learning, also called lifelong learning, studies the problem of learning tasks in a sequence with access restricted to only the data of the current task. In this paper we look at a...},
annote = {The authors combine multiple penalizations to (1) induce sparse activations through lateral inhibitions between neurons and to (2) penalize changes in most important weights in order to prevent forgetting.},
author = {Aljundi, Rahaf and Rohrbach, Marcus and Tuytelaars, Tinne},
booktitle = {ICLR},
keywords = {[cifar],[mnist],[sparsity]},
mendeley-tags = {[cifar],[mnist],[sparsity]},
title = {{Selfless Sequential Learning}},
url = {https://openreview.net/forum?id=Bkxbrn0cYX},
year = {2019}
}
@inproceedings{srivastava2013a,
abstract = {Local competition among neighboring neurons is common in biological neu-ral networks (NNs). In this paper, we apply the concept to gradient-based, backprop-trained artificial multilayer NNs. NNs with competing linear units tend to outperform those with non-competing nonlinear units, and avoid catastrophic forgetting when training sets change over time.},
author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Kazerounian, Sohrob and Gomez, Faustino and Schmidhuber, J{\"{u}}rgen},
booktitle = {Advances in Neural Information Processing Systems 26},
file = {::},
keywords = {[mnist],[sparsity]},
mendeley-tags = {[mnist],[sparsity]},
title = {{Compete to Compute}},
year = {2013}
}
@inproceedings{ven2020a,
abstract = {Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the replay of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay has been implemented in the form of ‘generative replay', which can successfully prevent catastrophic forgetting in a range of toy examples. Scaling up generative replay to problems with more complex inputs, however, turns out to be challenging. We propose a new, more brain-like variant of replay in which internal or hidden representations are replayed that are generated by the network's own, context-modulated feedback connections. In contrast to established continual learning methods, our method achieves acceptable performance on the challenging problem of class-incremental learning on natural images without relying on stored data.},
author = {van de Ven, Gido M. and Siegelmann, Hava T. and Tolias, Andreas S.},
booktitle = {International Conference on Learning Representations},
keywords = {[cifar]},
mendeley-tags = {[cifar]},
title = {{Brain-like Replay for Continual Learning with Artificial Neural Networks}},
url = {https://baicsworkshop.github.io/pdf/BAICS_8.pdf},
year = {2020}
}
@article{Parisi2018a,
abstract = {Artificial autonomous agents and robots interacting in complex environments are required to continually acquire and fine-tune knowledge over sustained periods of time. The ability to learn from continuous streams of information is referred to as lifelong learning and represents a long-standing challenge for neural network models due to catastrophic forgetting in which novel sensory experience interferes with existing representations and leads to abrupt decreases in the performance on previously acquired knowledge. Computational models of lifelong learning typically alleviate catastrophic forgetting in experimental scenarios with given datasets of static images and limited complexity, thereby differing significantly from the conditions artificial agents are exposed to. In more natural settings, sequential information may become progressively available over time and access to previous experience may be restricted. Therefore, specialized neural network mechanisms are required that adapt to novel sequential experience while preventing disruptive interference with existing representations. In this paper, we propose a dual-memory self-organizing architecture for lifelong learning scenarios. The architecture comprises two growing recurrent networks with the complementary tasks of learning object instances (episodic memory) and categories (semantic memory). Both growing networks can expand in response to novel sensory experience: the episodic memory learns fine-grained spatiotemporal representations of object instances in an unsupervised fashion while the semantic memory uses task-relevant signals to regulate structural plasticity levels and develop more compact representations from episodic experience. For the consolidation of knowledge in the absence of external sensory input, the episodic memory periodically replays trajectories of neural reactivations. We evaluate the proposed model on the CORe50 benchmark dataset for continuous object recognition, showing that we significantly outperform current methods of lifelong learning in three different incremental learning scenarios.},
author = {Parisi, German I and Tani, Jun and Weber, Cornelius and Wermter, Stefan},
doi = {10.3389/fnbot.2018.00078},
issn = {1662-5218},
journal = {Frontiers in Neurorobotics},
keywords = {CLS,Incremental Learning,Lifelong learning,Memory,Self-organizing Network,[core50],[dual],[som],object recognition systems},
language = {English},
mendeley-tags = {[core50],[dual],[som]},
title = {{Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization}},
url = {https://www.frontiersin.org/articles/10.3389/fnbot.2018.00078/full},
volume = {12},
year = {2018}
}
@article{Allred2020a,
abstract = {Stochastic gradient descent requires that training samples be drawn from a uniformly random distribution of the data. For a deployed system that must learn online from an uncontrolled and unknown environment, the ordering of input samples often fails to meet this criterion, making lifelong learning a difficult challenge. We exploit the locality of the unsupervised Spike Timing Dependent Plasticity (STDP) learning rule to target local representations in a Spiking Neural Network (SNN) to adapt to novel information while protecting essential information in the remainder of the SNN from catastrophic forgetting. In our Controlled Forgetting Networks (CFNs), novel information triggers stimulated firing and heterogeneously modulated plasticity, inspired by biological dopamine signals, to cause rapid and isolated adaptation in the synapses of neurons associated with outlier information. This targeting controls the forgetting process in a way that reduces the degradation of accuracy for older tasks while learning new tasks. Our experimental results on the MNIST dataset validate the capability of CFNs to learn successfully over time from an unknown, changing environment, achieving 95.24% accuracy, which we believe is the best unsupervised accuracy ever achieved by a fixed-size, single-layer SNN on a completely disjoint MNIST dataset.},
author = {Allred, Jason M. and Roy, Kaushik},
doi = {10.3389/fnins.2020.00007},
file = {::},
issn = {1662-453X},
journal = {Frontiers in Neuroscience},
keywords = {Spike Timing Dependent Plasticity,Spiking Neural Networks,[spiking],catastrophic forgetting,continual learning,controlled forgetting,dopaminergic learning,lifelong learning,stability-plasticity dilemma},
mendeley-tags = {[spiking]},
month = {jan},
pages = {7},
publisher = {Frontiers Media S.A.},
title = {{Controlled Forgetting: Targeted Stimulation and Dopaminergic Plasticity Modulation for Unsupervised Lifelong Learning in Spiking Neural Networks}},
url = {https://www.frontiersin.org/article/10.3389/fnins.2020.00007/full},
volume = {14},
year = {2020}
}
@article{Ororbia2019a,
abstract = {Temporal models based on recurrent neural networks have proven to be quite powerful in a wide variety of applications. However, training these models often relies on back-propagation through time, which entails unfolding the network over many time steps, making the process of conducting credit assignment considerably more challenging. Furthermore, the nature of back-propagation itself does not permit the use of non-differentiable activation functions and is inherently sequential, making parallelization of the underlying training process difficult. Here, we propose the Parallel Temporal Neural Coding Network (P-TNCN), a biologically inspired model trained by the learning algorithm we call Local Representation Alignment. It aims to resolve the difficulties and problems that plague recurrent networks trained by back-propagation through time. The architecture requires neither unrolling in time nor the derivatives of its internal activation functions. We compare our model and learning procedure to other back-propagation through time alternatives (which also tend to be computationally expensive), including real-time recurrent learning, echo state networks, and unbiased online recurrent optimization. We show that it outperforms these on sequence modeling benchmarks such as Bouncing MNIST, a new benchmark we denote as Bouncing NotMNIST, and Penn Treebank. Notably, our approach can in some instances outperform full back-propagation through time as well as variants such as sparse attentive back-tracking. Significantly, the hidden unit correction phase of P-TNCN allows it to adapt to new datasets even if its synaptic weights are held fixed (zero-shot adaptation) and facilitates retention of prior generative knowledge when faced with a task sequence. We present results that show the P-TNCN's ability to conduct zero-shot adaptation and online continual sequence modeling.},
annote = {Comment: Important revisions made throughout (additional items/results added, including a complexity analysis)
arXiv: 1810.07411},
author = {Ororbia, Alexander and Mali, Ankur and Giles, C Lee and Kifer, Daniel},
journal = {arXiv},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[mnist],[spiking],credi assignment},
mendeley-tags = {[mnist],[spiking]},
month = {aug},
title = {{Continual Learning of Recurrent Neural Networks by Locally Aligning Distributed Representations}},
url = {http://arxiv.org/abs/1810.07411},
year = {2019}
}
@inproceedings{kemker2018b,
abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
author = {Kemker, Ronald and Kanan, Christopher},
booktitle = {ICLR},
file = {::},
keywords = {[audio],[cifar],[generative]},
mendeley-tags = {[audio],[cifar],[generative]},
month = {feb},
title = {{FearNet: Brain-Inspired Model for Incremental Learning}},
url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},
year = {2018}
}
