Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@incollection{Kobayashi2019a,
abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be ﬁred corresponding to each task, since only readout weights are updated according to the degree of ﬁring of neurons. We therefore propose the way to design reservoir computing such that the ﬁring neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely diﬀerent networks.},
address = {Cham},
annote = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.},
author = {Kobayashi, Taisuke and Sugino, Toshiki},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions},
doi = {10.1007/978-3-030-30493-5_4},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30492-8 978-3-030-30493-5},
keywords = {fractals,rc,reinforcement,reservoir computing,rnn},
language = {en},
mendeley-tags = {rnn},
pages = {35--47},
publisher = {Springer International Publishing},
title = {{Continual Learning Exploiting Structure of Fractal Reservoir Computing}},
url = {http://link.springer.com/10.1007/978-3-030-30493-5{\_}4},
volume = {11731},
year = {2019}
}
@misc{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
booktitle = {arXiv: 1606.04671 [cs]},
keywords = {Computer Science - Machine Learning,lifelong learning,mnist,modular,progressive},
language = {en},
mendeley-tags = {mnist},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
