@inproceedings{Kobayashi2019a,
abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be ﬁred corresponding to each task, since only readout weights are updated according to the degree of ﬁring of neurons. We therefore propose the way to design reservoir computing such that the ﬁring neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely diﬀerent networks.},
address = {Cham},
annote = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.},
author = {Kobayashi, Taisuke and Sugino, Toshiki},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions},
doi = {10.1007/978-3-030-30493-5_4},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30492-8 978-3-030-30493-5},
keywords = {[rnn],fractals,rc,reinforcement,reservoir computing},
language = {en},
mendeley-tags = {[rnn]},
pages = {35--47},
publisher = {Springer International Publishing},
title = {{Continual Learning Exploiting Structure of Fractal Reservoir Computing}},
url = {http://link.springer.com/10.1007/978-3-030-30493-5_4},
volume = {11731},
year = {2019}
}
@article{Ring1997a,
abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
author = {Ring, Mark B},
doi = {10.1023/A:1007331723572},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Continual learning,[rnn],cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},
language = {en},
mendeley-tags = {[rnn]},
month = {jul},
number = {1},
pages = {77--104},
shorttitle = {CHILD},
title = {{CHILD: A First Step Towards Continual Learning}},
url = {https://doi.org/10.1023/A:1007331723572},
volume = {28},
year = {1997}
}
@inproceedings{Schwarz2018a,
abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
booktitle = {International Conference on Machine Learning},
keywords = {[vision],ewc,normalized ewc,online ewc},
language = {en},
mendeley-tags = {[vision]},
month = {jul},
pages = {4528--4537},
shorttitle = {Progress & Compress},
title = {{Progress & Compress: A scalable framework for continual learning}},
url = {http://proceedings.mlr.press/v80/schwarz18a.html},
year = {2018}
}
@misc{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
booktitle = {arXiv: 1606.04671 [cs]},
keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},
mendeley-tags = {[mnist]},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@inproceedings{riemer2019a,
abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
booktitle = {ICLR},
file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Riemer et al. - 2019 - Learning to learn without forgetting by maximizing transfer and minimizing interference(2).pdf:pdf},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
month = {sep},
title = {{Learning to learn without forgetting by maximizing transfer and minimizing interference}},
year = {2019}
}
@article{Pan2019a,
abstract = {Interference is a known problem when learning in online settings, such as continual learning or reinforcement learning. Interference occurs when updates, to improve performance for some inputs, degrades performance for others. Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In our approach, we design an activation function that naturally produces sparse representations, and so is much more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Leaky Tiling Activation (LTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We empirically investigate both value-based and policy gradient reinforcement learning algorithms that use neural networks with LTAs, in classic discrete-action control environments and Mujoco continuous-action environments. We show that, with LTAs, learning is faster, with more stable policies, without needing target networks.},
archivePrefix = {arXiv},
arxivId = {1911.08068},
author = {Pan, Yangchen and Banman, Kirby and White, Martha},
eprint = {1911.08068},
keywords = {[sparsity]},
mendeley-tags = {[sparsity]},
title = {{Leaky Tiling Activations: A Simple Approach to Learning Sparse Representations Online}},
url = {http://arxiv.org/abs/1911.08068},
year = {2019}
}
