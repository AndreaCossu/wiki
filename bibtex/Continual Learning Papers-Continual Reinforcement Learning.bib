@article{Mankowitz2018a,
abstract = {Some real-world domains are best characterized as a single task, but for others this perspective is limiting. Instead, some tasks continually grow in complexity, in tandem with the agent's competence. In continual learning, also referred to as lifelong learning, there are no explicit task boundaries or curricula. As learning agents have become more powerful, continual learning remains one of the frontiers that has resisted quick progress. To test continual learning capabilities we consider a challenging 3D domain with an implicit sequence of tasks and sparse rewards. We propose a novel agent architecture called Unicorn, which demonstrates strong continual learning and outperforms several baseline agents on the proposed domain. The agent achieves this by jointly representing and learning multiple policies efficiently, using a parallel off-policy learning setup.},
archivePrefix = {arXiv},
arxivId = {1802.08294},
author = {Mankowitz, Daniel J and {\v{Z}}{\'{i}}dek, Augustin and Barreto, Andr{\'{e}} and Horgan, Dan and Hessel, Matteo and Quan, John and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Schaul, Tom},
eprint = {1802.08294},
journal = {arXiv},
month = {feb},
pages = {1--17},
title = {{Unicorn: Continual Learning with a Universal, Off-policy Agent}},
url = {http://arxiv.org/abs/1802.08294},
year = {2018}
}
@inproceedings{Kaplanis2018b,
abstract = {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna {\&} Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.},
archivePrefix = {arXiv},
arxivId = {1802.07239},
author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
booktitle = {ICML},
eprint = {1802.07239},
month = {feb},
title = {{Continual Reinforcement Learning with Complex Synapses}},
url = {http://arxiv.org/abs/1802.07239},
year = {2018}
}
@article{Early2019a,
abstract = {A key stepping stone in the development of an artificial general intelligence (a machine that can perform any task), is the production of agents that can perform multiple tasks at once instead of just one. Unfortunately, canonical methods are very prone to catastrophic forgetting (CF) - the act of overwriting previous knowledge about a task when learning a new task. Recent efforts have developed techniques for overcoming CF in learning systems, but no attempt has been made to apply these new techniques to evolutionary systems. This research presents a novel technique, weight protection, for reducing CF in evolutionary systems by adapting a method from learning systems. It is used in conjunction with other evolutionary approaches for overcoming CF and is shown to be effective at alleviating CF when applied to a suite of reinforcement learning tasks. It is speculated that this work could indicate the potential for a wider application of existing learning-based approaches to evolutionary systems and that evolutionary techniques may be competitive with or better than learning systems when it comes to reducing CF.},
archivePrefix = {arXiv},
arxivId = {1904.03178},
author = {Early, Joseph},
eprint = {1904.03178},
journal = {arXiv},
month = {apr},
title = {{Reducing catastrophic forgetting when evolving neural networks}},
url = {http://arxiv.org/abs/1904.03178},
year = {2019}
}
@article{Isele2018a,
abstract = {Deep reinforcement learning has emerged as a powerful tool for a variety of learning tasks, however deep nets typically exhibit forgetting when learning multiple tasks in sequence. To mitigate forgetting, we propose an experience replay process that augments the standard FIFO buffer and selectively stores experiences in a long-term memory. We explore four strategies for selecting which experiences will be stored: favoring surprise, favoring reward, matching the global training distribution, and maximizing coverage of the state space. We show that distribution matching successfully prevents catastrophic forgetting, and is consistently the best approach on all domains tested. While distribution matching has better and more consistent performance, we identify one case in which coverage maximization is beneficial - when tasks that receive less trained are more important. Overall, our results show that selective experience replay, when suitable selection algorithms are employed, can prevent catastrophic forgetting.},
archivePrefix = {arXiv},
arxivId = {1802.10269},
author = {Isele, David and Cosgun, Akansel},
eprint = {1802.10269},
journal = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {Natural Language Processing and Machine Learning T},
month = {feb},
pages = {3302--3309},
title = {{Selective Experience Replay for Lifelong Learning}},
url = {http://arxiv.org/abs/1802.10269},
year = {2018}
}
@inproceedings{Mendez2018a,
abstract = {Methods for learning from demonstration (LfD) have shown success in acquiring behavior policies by imitating a user. However, even for a single task, LfD may require numerous demonstrations. For versatile agents that must learn many tasks via demonstration, this process would substantially burden the user if each task were learned in isolation. To address this challenge, we introduce the novel problem of lifelong learning from demonstration, which allows the agent to continually build upon knowledge learned from previously demonstrated tasks to accelerate the learning of new tasks, reducing the amount of demonstrations required. As one solution to this problem, we propose the first lifelong learning approach to inverse reinforcement learning, which learns consecutive tasks via demonstration, continually transferring knowledge between tasks to improve performance.},
author = {Mendez, Jorge A and Shivkumar, Shashank and Eaton, Eric},
booktitle = {NeurIPS},
file = {::},
pages = {4502--4513},
title = {{Lifelong Inverse Reinforcement Learning}},
url = {http://papers.nips.cc/paper/7702-lifelong-inverse-reinforcement-learning.pdf},
year = {2018}
}
@inproceedings{garcia2019a,
abstract = {In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed framework.},
author = {Garcia, Francisco M and Thomas, Philip S},
booktitle = {NeurIPS},
file = {::},
pages = {5691--5700},
title = {{A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning}},
url = {https://papers.nips.cc/paper/8806-a-meta-mdp-approach-to-exploration-for-lifelong-reinforcement-learning.pdf},
year = {2019}
}
@inproceedings{rolnick2019a,
abstract = {Interacting with a complex world involves continual learning, in which tasks and data distributions change over time. A continual learning system should demonstrate both plasticity (acquisition of new knowledge) and stability (preservation of old knowledge). Catastrophic forgetting is the failure of stability, in which new experience overwrites previous experience. In the brain, replay of past experience is widely believed to reduce forgetting, yet it has been largely overlooked as a solution to forgetting in deep reinforcement learning. Here, we introduce CLEAR, a replay-based method that greatly reduces catastrophic forgetting in multi-task reinforcement learning. CLEAR leverages off-policy learning and behavioral cloning from replay to enhance stability, as well as on-policy learning to preserve plasticity. We show that CLEAR performs better than state-of-the-art deep learning techniques for mitigating forgetting, despite being significantly less complicated and not requiring any knowledge of the individual tasks being learned.},
author = {Rolnick, David and Ahuja, Arun and Schwarz, Jonathan and Lillicrap, Timothy P and Wayne, Greg},
booktitle = {NeurIPS},
file = {:home/andrea/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rolnick et al. - 2019 - Experience Replay for Continual Learning.pdf:pdf},
pages = {350--360},
title = {{Experience Replay for Continual Learning}},
url = {http://papers.nips.cc/paper/8327-experience-replay-for-continual-learning.pdf},
year = {2019}
}
@inproceedings{Civelek2014a,
abstract = {It is well-known that opinions have targets. Extracting such targets is an important problem of opinion mining because without knowing the target of an opinion, the opinion is of limited use. So far many algorithms have been proposed to extract opinion targets. However, an opinion target can be an entity or an aspect (part or attribute) of an entity. An opinion about an entity is an opinion about the entity as a whole, while an opinion about an aspect is just an opinion about that specific attribute or aspect of an entity. Thus, opinion targets should be separated into entities and aspects before use because they represent very different things about opinions. This paper proposes a novel algorithm, called Lifelong-RL, to solve the problem based on lifelong machine learning and relaxation labeling. Extensive experiments show that the proposed algorithm Lifelong-RL outperforms baseline methods markedly.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Shu, Lei and Liu, Bing and Xu, Hu and Kim, Annice},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing},
doi = {10.1038/nrg3575.Systems},
eprint = {15334406},
isbn = {9781493973712},
issn = {1527-5418},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {nov},
number = {1},
pages = {225--235},
pmid = {29756130},
title = {{Lifelong-RL: Lifelong Relaxation Labeling for Separating Entities and Aspects in Opinion Targets.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29756130 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5947972},
volume = {2016},
year = {2016}
}
@article{kaplanis2019a,
abstract = {We propose a method for tackling catastrophic forgetting in deep reinforcement learning that is $\backslash$textit{\{}agnostic{\}} to the timescale of changes in the distribution of experiences, does not require knowledge of task boundaries, and can adapt in $\backslash$textit{\{}continuously{\}} changing environments. In our $\backslash$textit{\{}policy consolidation{\}} model, the policy network interacts with a cascade of hidden networks that simultaneously remember the agent's policy at a range of timescales and regularise the current policy by its own history, thereby improving its ability to learn without forgetting. We find that the model improves continual learning relative to baselines on a number of continuous control tasks in single-task, alternating two-task, and multi-agent competitive self-play settings.},
archivePrefix = {arXiv},
arxivId = {1902.00255},
author = {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
eprint = {1902.00255},
file = {::},
journal = {ICML},
month = {feb},
title = {{Policy Consolidation for Continual Reinforcement Learning}},
url = {http://arxiv.org/abs/1902.00255},
year = {2019}
}
@inproceedings{schlegel2017a,
abstract = {The objective of continual learning is to build agents that continually learn about their world, building on prior learning. In this paper, we explore an approach to continual learning based on making and updating many predictions formalized as general value functions (GVFs). The idea behind GVFs is simple: if we can cast the task of representing predictive knowledge as a prediction of future reward, then computationally efficient policy evaluation methods from reinforcement learning can be used to learn a large collection of predictions while the agent interacts with the world. We explore this idea further by analyzing how GVF predictions can be used as predictive features, and introduce two algorithmic techniques to ensure the stability of continual prediction learning. We illustrate these ideas with a small experiment in the cycle world domain.},
author = {Schlegel, Matthew and White, Adam and White, Martha},
booktitle = {Continual Learning and Deep Networks workshop at the Neural Information Processing System Conference},
title = {{Stable predictive representations with general value functions for continual learning}},
url = {https://sites.ualberta.ca/{~}amw8/cldl.pdf},
year = {2017}
}
@article{nagabandi2019a,
abstract = {Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances. Videos available at: https://sites.google.com/Berkeley.edu/onlineviameta.},
archivePrefix = {arXiv},
arxivId = {1812.07671},
author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
eprint = {1812.07671},
journal = {7th International Conference on Learning Representations, ICLR 2019},
title = {{Deep online learning via meta-learning: Continual adaptation for model-based RL}},
url = {https://arxiv.org/abs/1812.07671},
year = {2019}
}
@inproceedings{Luders2016a,
abstract = {Continual learning, i.e. the ability to sequentially learn tasks without catastrophicforgetting of previously learned ones, is an important open challenge in machinelearning. In this paper we take a step in this direction by showing that the recentlyproposedEvolving Neural Turing Machine(ENTM) approach is able to performone-shot learningin a reinforcement learning task without catastrophic forgettingof previously stored associations.},
author = {{Luders, Benno and Schlager, Mikkel and Risi}, Sebastia},
booktitle = {NIPS 2016 Workshop on Continual Learning and Deep Networks},
title = {{Continual learning through evolvable neural turing machines}},
url = {https://core.ac.uk/reader/84859350},
year = {2016}
}
@article{Pan2019a,
abstract = {Interference is a known problem when learning in online settings, such as continual learning or reinforcement learning. Interference occurs when updates, to improve performance for some inputs, degrades performance for others. Recent work has shown that sparse representations---where only a small percentage of units are active---can significantly reduce interference. Those works, however, relied on relatively complex regularization or meta-learning approaches, that have only been used offline in a pre-training phase. In our approach, we design an activation function that naturally produces sparse representations, and so is much more amenable to online training. The idea relies on the simple approach of binning, but overcomes the two key limitations of binning: zero gradients for the flat regions almost everywhere, and lost precision---reduced discrimination---due to coarse aggregation. We introduce a Leaky Tiling Activation (LTA) that provides non-negligible gradients and produces overlap between bins that improves discrimination. We empirically investigate both value-based and policy gradient reinforcement learning algorithms that use neural networks with LTAs, in classic discrete-action control environments and Mujoco continuous-action environments. We show that, with LTAs, learning is faster, with more stable policies, without needing target networks.},
archivePrefix = {arXiv},
arxivId = {1911.08068},
author = {Pan, Yangchen and Banman, Kirby and White, Martha},
eprint = {1911.08068},
journal = {arXiv},
keywords = {[sparsity]},
mendeley-tags = {[sparsity]},
title = {{Leaky Tiling Activations: A Simple Approach to Learning Sparse Representations Online}},
url = {http://arxiv.org/abs/1911.08068},
year = {2019}
}
@inproceedings{riemer2019a,
abstract = {Lack of performance when it comes to continual learning over non-stationary distributions of data remains a major challenge in scaling neural network learning to more human realistic settings. In this work we propose a new conceptualization of the continual learning problem in terms of a temporally symmetric trade-off between transfer and interference that can be optimized by enforcing gradient alignment across examples. We then propose a new algorithm, Meta-Experience Replay (MER), that directly exploits this view by combining experience replay with optimization based meta-learning. This method learns parameters that make interference based on future gradients less likely and transfer based on future gradients more likely. 1 We conduct experiments across continual lifelong supervised learning benchmarks and non-stationary reinforcement learning environments demonstrating that our approach consistently outperforms recently proposed baselines for continual learning. Our experiments show that the gap between the performance of MER and baseline algorithms grows both as the environment gets more non-stationary and as the fraction of the total experiences stored gets smaller.},
author = {Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
booktitle = {ICLR},
file = {::},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
month = {sep},
title = {{Learning to learn without forgetting by maximizing transfer and minimizing interference}},
url = {https://openreview.net/pdf?id=B1gTShAct7},
year = {2019}
}
@article{Rusu2016a,
abstract = {Learning to solve complex sequences of tasks—while both leveraging transfer and avoiding catastrophic forgetting—remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and ﬁnetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
annote = {The authors rely on a separate feedforward network (column) for each task the model is trained on. Each column is connected through adaptive connections to all the previous ones. The weights of previous columns are frozen once trained. At inference time, given a known task label, the network choose the appropriate column to produce the output, thus preventing forgetting by design.},
author = {Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
journal = {arXiv},
keywords = {Computer Science - Machine Learning,[mnist],lifelong learning,modular,progressive},
language = {en},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Progressive Neural Networks}},
url = {http://arxiv.org/abs/1606.04671},
year = {2016}
}
@article{Kirkpatrick2017a,
abstract = {The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Neural networks are not, in general, capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks which they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on the MNIST hand written digit dataset and by learning several Atari 2600 games sequentially.},
annote = {arXiv: 1612.00796},
author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
journal = {PNAS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,[mnist],annotated,ewc},
mendeley-tags = {[mnist]},
number = {13},
pages = {3521--3526},
title = {{Overcoming catastrophic forgetting in neural networks}},
url = {http://arxiv.org/abs/1612.00796},
volume = {114},
year = {2017}
}
@inproceedings{Schwarz2018a,
abstract = {We introduce a conceptually simple and scalable framework for continual learning domains where tasks are learned sequentially. Our method is constant in the number of parameters and is designed to ...},
author = {Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
booktitle = {International Conference on Machine Learning},
keywords = {[vision],ewc,normalized ewc,online ewc},
language = {en},
mendeley-tags = {[vision]},
month = {jul},
pages = {4528--4537},
shorttitle = {Progress {\&} Compress},
title = {{Progress {\&} Compress: A scalable framework for continual learning}},
url = {http://proceedings.mlr.press/v80/schwarz18a.html},
year = {2018}
}
@article{Ring1997a,
abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
author = {Ring, Mark B},
doi = {10.1023/A:1007331723572},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Continual learning,[rnn],cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},
language = {en},
mendeley-tags = {[rnn]},
month = {jul},
number = {1},
pages = {77--104},
shorttitle = {CHILD},
title = {{CHILD: A First Step Towards Continual Learning}},
url = {https://doi.org/10.1023/A:1007331723572},
volume = {28},
year = {1997}
}
@inproceedings{Kobayashi2019a,
abstract = {Neural network has a critical problem, called catastrophic forgetting, where memories for tasks already learned are easily overwritten with memories for a task additionally learned. This problem interferes with continual learning required for autonomous robots, which learn many tasks incrementally from daily activities. To mitigate the catastrophic forgetting, it is important for especially reservoir computing to clarify which neurons should be ﬁred corresponding to each task, since only readout weights are updated according to the degree of ﬁring of neurons. We therefore propose the way to design reservoir computing such that the ﬁring neurons are clearly distinguished from others according to the task to be performed. As a key design feature, we employ fractal network, which has modularity and scalability, to be reservoir layer. In particular, its modularity is fully utilized by designing input layer. As a result, simulations of control tasks using reinforcement learning show that our design mitigates the catastrophic forgetting even when random actions from reinforcement learning prompt parameters to be overwritten. Furthermore, learning multiple tasks with a single network suggests that knowledge for the other tasks can facilitate to learn a new task, unlike the case using completely diﬀerent networks.},
address = {Cham},
annote = {A reservoir computing approach with Echo State Networks is implemented in order to learn multiple tasks in reinforcement learning environments.},
author = {Kobayashi, Taisuke and Sugino, Toshiki},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Workshop and Special Sessions},
doi = {10.1007/978-3-030-30493-5_4},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30492-8 978-3-030-30493-5},
keywords = {[rnn],fractals,rc,reinforcement,reservoir computing},
language = {en},
mendeley-tags = {[rnn]},
pages = {35--47},
publisher = {Springer International Publishing},
title = {{Continual Learning Exploiting Structure of Fractal Reservoir Computing}},
url = {http://link.springer.com/10.1007/978-3-030-30493-5{\_}4},
volume = {11731},
year = {2019}
}
