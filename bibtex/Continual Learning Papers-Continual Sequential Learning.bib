@inproceedings{Chaudhry2019a,
abstract = {In lifelong learning, the learner is presented with a sequence of tasks, incrementally building a data-driven prior which may be leveraged to speed up learning of a new task. In this work, we investigate the efficiency of current lifelong approaches, in terms of sample complexity, computational and memory cost. Towards this end, we first introduce a new and a more realistic evaluation protocol, whereby learners observe each example only once and hyper-parameter selection is done on a small and disjoint set of tasks, which is not used for the actual learning experience and evaluation. Second, we introduce a new metric measuring how quickly a learner acquires a new skill. Third, we propose an improved version of GEM (Lopez-Paz & Ranzato, 2017), dubbed Averaged GEM (A-GEM), which enjoys the same or even better performance as GEM, while being almost as computationally and memory efficient as EWC (Kirkpatrick et al., 2016) and other regularization-based methods. Finally, we show that all algorithms including A-GEM can learn even more quickly if they are provided with task descriptors specifying the classification tasks under consideration. Our experiments on several standard lifelong learning benchmarks demonstrate that A-GEM has the best trade-off between accuracy and efficiency.},
annote = {Comment: Published as a conference paper at ICLR 2019
arXiv: 1812.00420},
author = {Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
booktitle = {ICLR},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,[cifar],[mnist]},
language = {en},
mendeley-tags = {[cifar],[mnist]},
title = {{Efficient Lifelong Learning with A-GEM}},
url = {http://arxiv.org/abs/1812.00420},
year = {2019}
}
@article{Parisi2020a,
abstract = {Online continual learning (OCL) refers to the ability of a system to learn over time from a continuous stream of data without having to revisit previously encountered training samples. Learning continually in a single data pass is crucial for agents and robots operating in changing environments and required to acquire, fine-tune, and transfer increasingly complex representations from non-i.i.d. input distributions. Machine learning models that address OCL must alleviate \textit{catastrophic forgetting} in which hidden representations are disrupted or completely overwritten when learning from streams of novel input. In this chapter, we summarize and discuss recent deep learning models that address OCL on sequential input through the use (and combination) of synaptic regularization, structural plasticity, and experience replay. Different implementations of replay have been proposed that alleviate catastrophic forgetting in connectionists architectures via the re-occurrence of (latent representations of) input sequences and that functionally resemble mechanisms of hippocampal replay in the mammalian brain. Empirical evidence shows that architectures endowed with experience replay typically outperform architectures without in (online) incremental learning tasks.},
annote = {Comment: L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies in Computational Intelligence 896
arXiv: 2003.09114},
author = {Parisi, German I and Lomonaco, Vincenzo},
doi = {10.1007/978-3-030-43883-8_8},
journal = {arXiv},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[framework]},
mendeley-tags = {[framework]},
month = {mar},
title = {{Online Continual Learning on Sequences}},
url = {http://arxiv.org/abs/2003.09114},
year = {2020}
}
@inproceedings{Rebuffi2017a,
author = {Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {[cifar]},
mendeley-tags = {[cifar]},
month = {jul},
title = {{iCaRL: Incremental Classifier and Representation Learning}},
year = {2017}
}
@inproceedings{xu2018a,
abstract = {Catastrophic interference has been a major roadblock in the research of continual learning. Here we propose a variant of the back-propagation algorithm, "conceptor-aided backprop" (CAB), in which gradients are shielded by concep-tors against degradation of previously learned tasks. Conceptors have their origin in reservoir computing, where they have been previously shown to overcome catastrophic forgetting. CAB extends these results to deep feedforward networks. On the disjoint and permuted MNIST tasks, CAB outperforms two other methods for coping with catastrophic interference that have recently been proposed.},
author = {He, Xu and Jaeger, Herbert},
booktitle = {ICLR},
file = {::},
keywords = {[mnist],[rnn]},
mendeley-tags = {[mnist],[rnn]},
title = {{Overcoming Catastrophic interference using conceptor-aided backpropagation}},
year = {2018}
}
@unpublished{Chaudhry2018a,
abstract = {Incremental learning (il) has received a lot of attention recently, however, the literature lacks a precise problem definition, proper evaluation settings, and metrics tailored specifically for the il problem. One of the main objectives of this work is to fill these gaps so as to provide a common ground for better understanding of il. The main challenge for an il algorithm is to update the classifier whilst preserving existing knowledge. We observe that, in addition to forgetting, a known issue while preserving knowledge, il also suffers from a problem we call intransigence, its inability to update knowledge. We introduce two metrics to quantify forgetting and intransigence that allow us to understand, analyse, and gain better insights into the behaviour of il algorithms. Furthermore, we present RWalk, a generalization of ewc++ (our efficient version of ewc [6]) and Path Integral [25] with a theoretically grounded KL-divergence based perspective. We provide a thorough analysis of various il algorithms on MNIST and CIFAR-100 datasets. In these experiments, RWalk obtains superior results in terms of accuracy, and also provides a better trade-off for forgetting and intransigence.},
archivePrefix = {arXiv},
arxivId = {1801.10112},
author = {Chaudhry, Arslan and Dokania, Puneet K and Ajanthan, Thalaiyasingam and Torr, Philip H.S.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01252-6_33},
eprint = {1801.10112},
isbn = {9783030012519},
issn = {16113349},
keywords = {[cifar],[mnist],ewc,ewc++,fisher,forgetting},
mendeley-tags = {[cifar],[mnist]},
pages = {556--572},
shorttitle = {Riemannian Walk for Incremental Learning},
title = {{Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence}},
url = {http://openaccess.thecvf.com/content_ECCV_2018/html/Arslan_Chaudhry__Riemannian_Walk_ECCV_2018_paper.html},
volume = {11215 LNCS},
year = {2018}
}
@inproceedings{Aljundi2019d,
author = {Aljundi, Rahaf and Lin, Min and Goujaud, Baptiste and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d\textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
pages = {11816--11825},
publisher = {Curran Associates, Inc.},
title = {{Gradient based sample selection for online continual learning}},
url = {http://papers.nips.cc/paper/9354-gradient-based-sample-selection-for-online-continual-learning.pdf},
year = {2019}
}
@inproceedings{Aljundi2019,
author = {Aljundi, Rahaf and Kelchtermans, Klaas and Tuytelaars, Tinne},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {[vision]},
mendeley-tags = {[vision]},
month = {jun},
publisher = {Aljundi2019b},
title = {{Task-Free Continual Learning}},
year = {2019}
}
@inproceedings{Lopez-Paz2017a,
abstract = {One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.},
annote = {Comment: Published at NIPS 2017
arXiv: 1706.08840},
author = {Lopez-Paz, David and Ranzato, Marc'Aurelio},
booktitle = {NIPS},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,[cifar],[mnist],gem},
mendeley-tags = {[cifar],[mnist]},
title = {{Gradient Episodic Memory for Continual Learning}},
url = {http://arxiv.org/abs/1706.08840},
year = {2017}
}
@inproceedings{Aljundi2019a,
author = {Aljundi, Rahaf and Belilovsky, Eugene and Tuytelaars, Tinne and Charlin, Laurent and Caccia, Massimo and Lin, Min and Page-Caccia, Lucas},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d\textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
pages = {11849--11860},
publisher = {Curran Associates, Inc.},
title = {{Online Continual Learning with Maximal Interfered Retrieval}},
url = {http://papers.nips.cc/paper/9357-online-continual-learning-with-maximal-interfered-retrieval.pdf},
year = {2019}
}
@article{Hayes2019b,
abstract = {When a robot acquires new information, ideally it would immediately be capable of using that information to understand its environment. While deep neural networks are now widely used by robots for inferring semantic information, conventional neural networks suffer from catastrophic forgetting when they are incrementally updated, with new knowledge overwriting established representations. While a variety of approaches have been developed that attempt to mitigate catastrophic forgetting in the incremental batch learning scenario, in which an agent learns a large collection of labeled samples at once, streaming learning has been much less studied in the robotics and deep learning communities. In streaming learning, an agent learns instances one-by-one and can be tested at any time. Here, we revisit streaming linear discriminant analysis, which has been widely used in the data mining research community. By combining streaming linear discriminant analysis with deep learning, we are able to outperform both incremental batch learning and streaming learning algorithms on both ImageNet-1K and CORe50.},
archivePrefix = {arXiv},
arxivId = {1909.01520},
author = {Hayes, Tyler L and Kanan, Christopher},
eprint = {1909.01520},
journal = {CLVision Workshop at CVPR 2020},
keywords = {[core50],[imagenet],deep learning,streaming learning},
mendeley-tags = {[core50],[imagenet]},
pages = {1--15},
title = {{Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis}},
url = {http://arxiv.org/abs/1909.01520},
year = {2020}
}
@inproceedings{Schak2019a,
abstract = {We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).},
address = {Cham},
author = {Schak, Monika and Gepperth, Alexander},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning},
doi = {10.1007/978-3-030-30484-3_56},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30484-3},
keywords = {Catastrophic Forgetting,LSTM,[rnn],sequential},
language = {en},
mendeley-tags = {[rnn]},
pages = {714--728},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{A Study on Catastrophic Forgetting in Deep LSTM Networks}},
year = {2019}
}
@misc{He2019a,
abstract = {While neural networks are powerful function approximators, they suffer from catastrophic forgetting when the data distribution is not stationary. One particular formalism that studies learning under non-stationary distribution is provided by continual learning, where the non-stationarity is imposed by a sequence of distinct tasks. Most methods in this space assume, however, the knowledge of task boundaries, and focus on alleviating catastrophic forgetting. In this work, we depart from this view and move the focus towards faster remembering – i.e measuring how quickly the network recovers performance rather than measuring the network's performance without any adaptation. We argue that in many settings this can be more effective and that it opens the door to combining meta-learning and continual learning techniques, leveraging their complementary advantages. We propose a framework specific for the scenario where no information about task boundaries or task identity is given. It relies on a separation of concerns into what task is being solved and how the task should be solved. This framework is implemented by differentiating task specific parameters from task agnostic parameters, where the latter are optimized in a continual meta learning fashion, without access to multiple tasks at the same time. We showcase this framework in a supervised learning scenario and discuss the implication of the proposed formalism.},
annote = {arXiv: 1906.05201},
author = {He, Xu and Sygnowski, Jakub and Galashov, Alexandre and Rusu, Andrei A and Teh, Yee Whye and Pascanu, Razvan},
booktitle = {arXiv:1906.05201 [cs, stat]},
keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,Statistics - Machine Learning,[mnist]},
mendeley-tags = {[mnist]},
month = {jun},
title = {{Task Agnostic Continual Learning via Meta Learning}},
url = {http://arxiv.org/abs/1906.05201},
year = {2019}
}
@article{smith2020a,
abstract = {We first pose the Unsupervised Progressive Learning (UPL) problem: an online representation learning problem in which the learner observes a non-stationary and unlabeled data stream, and identifies a growing number of features that persist over time even though the data is not stored or replayed. To solve the UPL problem we propose the Self-Taught Associative Memory (STAM) architecture. Layered hierarchies of STAM modules learn based on a combination of online clustering, novelty detection, forgetting outliers, and storing only prototypical features rather than specific examples. We evaluate STAM representations using classification and clustering tasks. Even though there are no prior approaches that are directly applicable to the UPL problem, we evaluate the STAM architecture in comparison to some unsupervised and self-supervised deep learning approaches adapted in the UPL context.},
archivePrefix = {arXiv},
arxivId = {1904.02021},
author = {Smith, James and Baer, Seth and Taylor, Cameron and Dovrolis, Constantine},
eprint = {1904.02021},
file = {::},
journal = {arXiv},
keywords = {[mnist]},
mendeley-tags = {[mnist]},
month = {apr},
title = {{Unsupervised Progressive Learning and the STAM Architecture}},
url = {http://arxiv.org/abs/1904.02021},
year = {2019}
}
@inproceedings{kemker2018b,
abstract = {Incremental class learning involves sequentially learning classes in bursts of examples from the same class. This violates the assumptions that underlie methods for training standard deep neural networks, and will cause them to suffer from catastrophic forgetting. Arguably, the best method for incremental class learning is iCaRL, but it requires storing training examples for each class, making it challenging to scale. Here, we propose FearNet for incremental class learning. FearNet is a generative model that does not store previous examples, making it memory efficient. FearNet uses a brain-inspired dual-memory system in which new memories are consolidated from a network for recent memories inspired by the mammalian hippocampal complex to a network for long-term storage inspired by medial prefrontal cortex. Memory consolidation is inspired by mechanisms that occur during sleep. FearNet also uses a module inspired by the basolateral amygdala for determining which memory system to use for recall. FearNet achieves state-of-the-art performance at incremental class learning on image (CIFAR-100, CUB-200) and audio classification (AudioSet) benchmarks.},
author = {Kemker, Ronald and Kanan, Christopher},
booktitle = {ICLR},
file = {::},
keywords = {[audio],[cifar],[generative]},
mendeley-tags = {[audio],[cifar],[generative]},
month = {feb},
title = {{FearNet: Brain-Inspired Model for Incremental Learning}},
url = {https://openreview.net/pdf?id=SJ1Xmf-Rb},
year = {2018}
}
