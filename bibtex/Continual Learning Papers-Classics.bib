@article{Robins1995a,
abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. We then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old informa tion.},
annote = {An in-depth overview of rehearsal techniques, including pseudo-rehearsal. Useful to combine these ideas in dual systems, in which one system learns a task while the other provides rehearsal of previous patterns.},
author = {Robins, Anthony},
doi = {10.1080/09540099550039318},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
keywords = {[dual]},
language = {en},
mendeley-tags = {[dual]},
month = {jun},
number = {2},
pages = {123--146},
title = {{Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},
volume = {7},
year = {1995}
}
@inproceedings{French1991a,
abstract = {In connectionist networks, newly-learned information destroys previously-learned information unless the network is continually retrained on the old information. This behavior, known as catastrophic forgetting, is unacceptable both for practical purposes and as a model of mind. This paper advances the claim that catastrophic forgetting is a direct consequence of the overlap of the system's distributed representations and can be reduced by reducing this overlap. A simple algorithm is presented that allows a standard feedforward backpropagation network to develop semi-distributed representations, thereby significantly reducing the problem of catastrophic forgetting. 1 Introduction Catastrophic forgetting is the inability of a neural network to retain old information in the presence of new. New information destroys old unless the old information is continually relearned by the net. McCloskey & Cohen [1990] and Ratcliff [1989] have demonstrated that this is a serious problem with c...},
author = {French, Robert M},
booktitle = {In Proceedings of the 13th Annual Cognitive Science Society Conference},
keywords = {[sparsity],activation sharpening},
mendeley-tags = {[sparsity]},
pages = {173--178},
publisher = {Erlbaum},
title = {{Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectionist Networks}},
year = {1991}
}
@article{Ring1997a,
abstract = {Continual learning is the constant development of increasingly complex behaviors; the process of building more complicated skills on top of those already developed. A continual-learning agent should therefore learn incrementally and hierarchically. This paper describes CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development. CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still.},
author = {Ring, Mark B},
doi = {10.1023/A:1007331723572},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {Continual learning,[rnn],cl,continual learner,definition,hierarchical neural networks,reinforcement learning,sequence learning,transfer},
language = {en},
mendeley-tags = {[rnn]},
month = {jul},
number = {1},
pages = {77--104},
shorttitle = {CHILD},
title = {{CHILD: A First Step Towards Continual Learning}},
url = {https://doi.org/10.1023/A:1007331723572},
volume = {28},
year = {1997}
}
@article{French1997a,
abstract = {In order to solve the “sensitivity-stability” problem — and its immediate correlate, the problem of sequential learning — it is crucial to develop connectionist architectures that are simultaneously sensitive to, but not excessively disrupted by, new input. French (1992) suggested that to alleviate a particularly severe form of this disruption, catastrophic forgetting, it was necessary for networks to dynamically separate their internal representations during learning. McClelland, McNaughton, & O'Reilly (1995) went even further. They suggested that nature's way of implementing this obligatory separation was the evolution of two separate areas of the brain, the hippocampus and the neocortex. In keeping with this idea of radical separation, a “pseudo-recurrent” memory model is presented here that partitions a connectionist network into two functionally distinct, but continually interacting areas. One area serves as a final-storage area for representations; the other is an early-processing area where new representations are first learned by the system. The final-storage area continually supplies internally generated patterns (pseudopatterns, Robins (1995)), which are approximations of its content, to the early-processing area, where they are interleaved with the new patterns to be learned. Transfer of the new learning is done either by weight-copying from the early-processing area to the final-storage area or by pseudopattern transfer. A number of experiments are presented that demonstrate the effectiveness of this approach, allowing, in particular, effective sequential learning with gradual forgetting in the presence of new input. Finally, it is shown that the two interacting areas automatically produce representational compaction and it is suggested that similar representational streamlining may exist in the brain.},
annote = {In this seminal paper the author introduces many different forms of rehearsal in order to mitigate the catastrophic forgetting phenomenon},
author = {French, Robert},
doi = {10.1080/095400997116595},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
keywords = {Catastrophic Interference,Dual Memory,Keywords: Pseudopatterns,Semi-distributed Representations,Sensitivity-stability Transfer,[dual],dilemma,plasticity,stability},
language = {en},
mendeley-tags = {[dual]},
month = {dec},
number = {4},
pages = {353--380},
shorttitle = {Pseudo-recurrent Connectionist Networks},
title = {{Pseudo-recurrent Connectionist Networks: An Approach to the 'Sensitivity-Stability' Dilemma}},
url = {http://www.tandfonline.com/doi/abs/10.1080/095400997116595},
volume = {9},
year = {1997}
}
@inproceedings{Thrun1996a,
author = {Thrun, Sebastian},
booktitle = {Advances in Neural Information Processing Systems 8},
editor = {Touretzky, D S and Mozer, M C and Hasselmo, M E},
keywords = {[vision],lifelong,lifelong learning},
mendeley-tags = {[vision]},
pages = {640--646},
publisher = {MIT Press},
title = {{Is Learning The n-th Thing Any Easier Than Learning The First?}},
url = {http://papers.nips.cc/paper/1034-is-learning-the-n-th-thing-any-easier-than-learning-the-first.pdf},
year = {1996}
}
@book{Hebb2005a,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology–a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
author = {Hebb, D O},
booktitle = {Lawrence Erlbaum},
isbn = {978-1-135-63191-8},
keywords = {Psychology / Cognitive Psychology & Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology,[hebbian]},
language = {en},
mendeley-tags = {[hebbian]},
month = {apr},
publisher = {Psychology Press},
shorttitle = {The Organization of Behavior},
title = {{The Organization of Behavior: A Neuropsychological Theory}},
year = {2002}
}
