Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Pfulb2018a,
abstract = {We present a large-scale empirical study of catastrophic forgetting (CF) in modern Deep Neural Network (DNN) models that perform sequential (or: incremen-tal) learning. A new experimental protocol is proposed that enforces typical constraints encountered in application scenarios. As the investigation is empirical, we evaluate CF behavior on the hitherto largest number of visual classification datasets, from each of which we construct a representative number of Sequential Learning Tasks (SLTs) in close alignment to previous works on CF. Our results clearly indicate that there is no model that avoids CF for all investigated datasets and SLTs under application conditions. We conclude with a discussion of potential solutions and workarounds to CF, notably for the EWC and IMM models.},
author = {Pf{\"{u}}lb, B and Gepperth, A},
booktitle = {ICLR},
file = {::},
keywords = {[fashion],[mnist]},
mendeley-tags = {[fashion],[mnist]},
month = {sep},
title = {{A comprehensive, application-oriented study of catastrophic forgetting in DNNs}},
url = {https://gitlab.informatik.hs-fulda.de/ML-Projects/CF{\_}in{\_}DNNs},
year = {2018}
}
@article{Abbott2000a,
abstract = {Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them—synaptic scaling, spike-timing dependent plasticity and synaptic redistribution—and discuss their functional implications.},
author = {Abbott, L F and Nelson, Sacha B},
doi = {10.1038/81453},
issn = {1546-1726},
journal = {Nature Neuroscience},
language = {en},
month = {nov},
number = {11},
pages = {1178--1183},
shorttitle = {Synaptic plasticity},
title = {{Synaptic plasticity: taming the beast}},
url = {https://www.nature.com/articles/nn1100{\_}1178},
volume = {3},
year = {2000}
}
@misc{Diaz-Rodriguez2018a,
abstract = {Continual learning consists of algorithms that learn from a stream of data/tasks continuously and adaptively thought time, enabling the incremental development of ever more complex knowledge and skills. The lack of consensus in evaluating continual learning algorithms and the almost exclusive focus on forgetting motivate us to propose a more comprehensive set of implementation independent metrics accounting for several factors we believe have practical implications worth considering in the deployment of real AI systems that learn continually: accuracy or performance over time, backward and forward knowledge transfer, memory overhead as well as computational efficiency. Drawing inspiration from the standard Multi-Attribute Value Theory (MAVT) we further propose to fuse these metrics into a single score for ranking purposes and we evaluate our proposal with five continual learning strategies on the iCIFAR-100 continual learning benchmark.},
annote = {arXiv: 1810.13166},
author = {D{\'{i}}az-Rodr{\'{i}}guez, Natalia and Lomonaco, Vincenzo and Filliat, David and Maltoni, Davide},
booktitle = {arXiv: 1810.13166 [cs]},
keywords = {68T05,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Rec,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computi,[cifar],[framework],cs.AI,cs.CV,cs.LG,cs.NE,stat.ML},
mendeley-tags = {[cifar],[framework]},
month = {oct},
shorttitle = {Don't forget, there is more than forgetting},
title = {{Don't forget, there is more than forgetting: new metrics for Continual Learning}},
url = {http://arxiv.org/abs/1810.13166},
year = {2018}
}
@inproceedings{Kemker2018a,
abstract = {Deep neural networks are used in many state-of-the-art systems for machine perception. Once a network is trained to do a specific task, e.g., bird classification, it cannot easily be trained to do new tasks, e.g., incrementally learning to recognize additional bird species or learning an entirely different task such as flower recognition. When new tasks are added, typical deep neural networks are prone to catastrophically forgetting previous tasks. Networks that are capable of assimilating new information incrementally, much like how humans form new memories over time, will be more efficient than re-training the model from scratch each time a new task needs to be learned. There have been multiple attempts to develop schemes that mitigate catastrophic forgetting, but these methods have not been directly compared, the tests used to evaluate them vary considerably, and these methods have only been evaluated on small-scale problems (e.g., MNIST). In this paper, we introduce new metrics and benchmarks for directly comparing five different mechanisms designed to mitigate catastrophic forgetting in neural networks: regularization, ensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on real-world images and sounds show that the mechanism(s) that are critical for optimal performance vary based on the incremental training paradigm and type of data being used, but they all demonstrate that the catastrophic forgetting problem is not yet solved.},
author = {Kemker, Ronald and McClure, Marc and Abitino, Angelina and Hayes, Tyler L and Kanan, Christopher},
booktitle = {Thirty-Second AAAI Conference on Artificial Intelligence},
keywords = {[mnist],audioset,kemker,review,survey},
language = {en},
mendeley-tags = {[mnist]},
month = {apr},
title = {{Measuring Catastrophic Forgetting in Neural Networks}},
url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16410},
year = {2018}
}
@book{Hebb2005a,
abstract = {Since its publication in 1949, D.O. Hebb's, The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However, the original edition has been unavailable since 1966, ensuring that Hebb's comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function, and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb, the Hebbian cell assembly, the Hebb synapse, and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering, robotics, and computer science, as well as neurophysiology, neuroscience, and psychology–a tribute to Hebb's foresight in developing a foundational neuropsychological theory of the organization of behavior.},
author = {Hebb, D O},
isbn = {978-1-135-63191-8},
keywords = {Psychology / Cognitive Psychology {\&} Cognition,Psychology / General,Psychology / Neuropsychology,Psychology / Physiological Psychology},
language = {en},
month = {apr},
publisher = {Psychology Press},
shorttitle = {The Organization of Behavior},
title = {{The Organization of Behavior: A Neuropsychological Theory}},
year = {2005}
}
@article{French1999a,
author = {French, Robert},
doi = {10.1016/S1364-6613(99)01294-2},
issn = {1364-6613, 1879-307X},
journal = {Trends in Cognitive Sciences},
keywords = {Catastrophic forgetting,Connectionism,Connectionist networks,Interference,Learning,Memory,Neuroscience,biology},
language = {English},
month = {apr},
number = {4},
pages = {128--135},
pmid = {10322466},
title = {{Catastrophic forgetting in connectionist networks}},
url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(99)01294-2},
volume = {3},
year = {1999}
}
@article{Robins1995a,
abstract = {This paper reviews the problem of catastrophic forgetting (the loss or disruption of previously learned information when new information is learned) in neural networks, and explores rehearsal mechanisms (the retraining of some of the previously learned information as the new information is added) as a potential solution. W e replicate some of the experiments described by Ratcliff (1990), including those relating to a simple `recency' based rehearsal regime. W e then develop further rehearsal regimes which are more effective than recency rehearsal. In particular, `sweep rehearsal' is very successful at minimizing catastrophic forgetting. One possible limitation of rehearsal in general, however, is that previously learned information may not be available for retraining. W e describe a solution to this problem, `pseudorehearsal' , a method which provides the advantages of rehearsal without actually requiring any access to the previously learned information (the original training population) itself. We then suggest an interpretation of these rehearsal mechanisms in the context of a function approximation based account of neural network learning. Both rehearsal and pseudorehearsal may have practical applications, allowing new information to be integrated into an existing network with minimum disruption of old inform a tion.},
author = {Robins, Anthony},
doi = {10.1080/09540099550039318},
issn = {0954-0091, 1360-0494},
journal = {Connection Science},
language = {en},
month = {jun},
number = {2},
pages = {123--146},
title = {{Catastrophic Forgetting; Catastrophic Interference; Stability; Plasticity; Rehearsal.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/09540099550039318},
volume = {7},
year = {1995}
}
@inproceedings{Toneva2018a,
abstract = {Inspired by the phenomenon of catastrophic forgetting, we investigate the learning dynamics of neural networks as they train on single classification tasks. Our goal is to understand whether a...},
annote = {An interesting aspect of this paper is related to the study of unforgettable patterns and how they influence performance in terms of forgetting.},
author = {Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
booktitle = {International Conference on Learning Representations},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {sep},
title = {{An Empirical Study of Example Forgetting during Deep Neural Network Learning}},
url = {https://openreview.net/forum?id=BJlxm30cKm},
year = {2019}
}
