@article{Saha2020a,
abstract = {Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.},
archivePrefix = {arXiv},
arxivId = {2001.08650},
author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},
eprint = {2001.08650},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Structured Compression and Sharing of Representational Space for Continual Learning}},
url = {http://arxiv.org/abs/2001.08650},
year = {2020}
}
@article{Li2020a,
abstract = {Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.},
archivePrefix = {arXiv},
arxivId = {2005.05080},
author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
eprint = {2005.05080},
file = {::},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {may},
title = {{Continual Learning Using Task Conditional Neural Networks}},
url = {http://arxiv.org/abs/2005.05080},
year = {2020}
}
@article{Ye2019a,
abstract = {With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.},
author = {Ye, Xin and Zhu, Qiuyu},
doi = {10.1109/ACCESS.2019.2904614},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {One-class classifier,[cifar],[mnist],feature extraction,incremental learning,loss function},
mendeley-tags = {[cifar],[mnist]},
pages = {42024--42031},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Class-Incremental Learning Based on Feature Extraction of CNN with Optimized Softmax and One-Class Classifiers}},
volume = {7},
year = {2019}
}
@article{Swaroop2018a,
abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach, achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an 'ideal' continual learning solution might be.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.02099v1},
author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},
eprint = {arXiv:1905.02099v1},
journal = {Continual Learning Workshop NeurIPS},
keywords = {[bayes],[mnist]},
mendeley-tags = {[bayes],[mnist]},
pages = {1--17},
title = {{Improving and Understanding Variational Continual Learning}},
url = {https://marcpickett.com/cl2018/CL-2018%7B%5C_%7Dpaper%7B%5C_%7D56.pdf},
year = {2018}
}
@article{Mancini2019a,
abstract = {Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge.},
archivePrefix = {arXiv},
arxivId = {1805.11119},
author = {Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Bul{\`{o}}, Samuel Rota},
doi = {10.1007/978-3-030-11012-3_14},
eprint = {1805.11119},
isbn = {9783030110116},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Incremental learning,Multi-task learning,[sparsity],[vision]},
mendeley-tags = {[sparsity],[vision]},
month = {may},
pages = {180--189},
title = {{Adding New Tasks to a Single Network with Weight Transformations using Binary Masks}},
url = {http://arxiv.org/abs/1805.11119},
volume = {11130 LNCS},
year = {2018}
}
@article{Teng2019a,
abstract = {In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1908.00355},
author = {Teng, Dan and Dasgupta, Sakyasingha},
eprint = {1908.00355},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {aug},
title = {{Continual Learning via Online Leverage Score Sampling}},
url = {http://arxiv.org/abs/1908.00355},
year = {2019}
}
@inproceedings{Rajasegaran2019a,
abstract = {Incremental lifelong learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
author = {Rajasegaran, Jathushan and Hayat, Munawar and Fahad, Salman Khan and Khan, Shahbaz and Shao, Ling},
booktitle = {NeurIPS},
file = {::},
keywords = {[cifar],[imagenet],[mnist]},
mendeley-tags = {[cifar],[imagenet],[mnist]},
pages = {12669--12679},
title = {{Random Path Selection for Incremental Learning}},
url = {http://papers.nips.cc/paper/9429-random-path-selection-for-continual-learning.pdf},
year = {2019}
}
