@article{Saha2020a,
abstract = {Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.},
archivePrefix = {arXiv},
arxivId = {2001.08650},
author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},
eprint = {2001.08650},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Structured Compression and Sharing of Representational Space for Continual Learning}},
url = {http://arxiv.org/abs/2001.08650},
year = {2020}
}
@article{Li2020a,
abstract = {Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.},
archivePrefix = {arXiv},
arxivId = {2005.05080},
author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
eprint = {2005.05080},
file = {::},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {may},
title = {{Continual Learning Using Task Conditional Neural Networks}},
url = {http://arxiv.org/abs/2005.05080},
year = {2020}
}
@article{Ye2019a,
abstract = {With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.},
author = {Ye, Xin and Zhu, Qiuyu},
doi = {10.1109/ACCESS.2019.2904614},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {One-class classifier,[cifar],[mnist],feature extraction,incremental learning,loss function},
mendeley-tags = {[cifar],[mnist]},
pages = {42024--42031},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Class-Incremental Learning Based on Feature Extraction of CNN with Optimized Softmax and One-Class Classifiers}},
volume = {7},
year = {2019}
}
@article{Swaroop2018a,
abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach, achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an 'ideal' continual learning solution might be.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.02099v1},
author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},
eprint = {arXiv:1905.02099v1},
journal = {Continual Learning Workshop NeurIPS},
keywords = {[bayes],[mnist]},
mendeley-tags = {[bayes],[mnist]},
pages = {1--17},
title = {{Improving and Understanding Variational Continual Learning}},
url = {https://marcpickett.com/cl2018/CL-2018%7B%5C_%7Dpaper%7B%5C_%7D56.pdf},
year = {2018}
}
