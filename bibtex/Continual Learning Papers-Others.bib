Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@inproceedings{Schak2019a,
abstract = {We present a systematic study of Catastrophic Forgetting (CF), i.e., the abrupt loss of previously acquired knowledge, when retraining deep recurrent LSTM networks with new samples. CF has recently received renewed attention in the case of feed-forward DNNs, and this article is the first work that aims to rigorously establish whether deep LSTM networks are afflicted by CF as well, and to what degree. In order to test this fully, training is conducted using a wide variety of high-dimensional image-based sequence classification tasks derived from established visual classification benchmarks (MNIST, Devanagari, FashionMNIST and EMNIST). We find that the CF effect occurs universally, without exception, for deep LSTM-based sequence classifiers, regardless of the construction and provenance of sequences. This leads us to conclude that LSTMs, just like DNNs, are fully affected by CF, and that further research work needs to be conducted in order to determine how to avoid this effect (which is not a goal of this study).},
address = {Cham},
author = {Schak, Monika and Gepperth, Alexander},
booktitle = {Artificial Neural Networks and Machine Learning – ICANN 2019: Deep Learning},
doi = {10.1007/978-3-030-30484-3_56},
editor = {Tetko, Igor V and Kůrkov{\'{a}}, V{\v{e}}ra and Karpov, Pavel and Theis, Fabian},
isbn = {978-3-030-30484-3},
keywords = {Catastrophic Forgetting,LSTM,[rnn],sequential},
language = {en},
mendeley-tags = {[rnn]},
pages = {714--728},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{A Study on Catastrophic Forgetting in Deep LSTM Networks}},
year = {2019}
}
