@inproceedings{Lee2019a,
abstract = {Lifelong learning with deep neural networks is well-known to suffer from catastrophic forgetting: The performance on previous tasks drastically degrades when learning a new task. To alleviate this effect, we propose to leverage a large stream of unlabeled data easily obtainable in the wild. In particular, we design a novel class-incremental learning scheme with (a) a new distillation loss, termed global distillation, (b) a learning strategy to avoid overfitting to the most recent task, and (c) a confidence-based sampling method to effectively leverage unlabeled external data. Our experimental results on various datasets, including CIFAR and ImageNet, demonstrate the superiority of the proposed methods over prior methods, particularly when a stream of unlabeled data is accessible: Our method shows up to 15.8{\%} higher accuracy and 46.5{\%} less forgetting compared to the state-of-the-art method. The code is available at https://github.com/kibok90/iccv2019-inc.},
archivePrefix = {arXiv},
arxivId = {1903.12648},
author = {Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2019.00040},
eprint = {1903.12648},
file = {::},
isbn = {9781728148038},
issn = {15505499},
month = {oct},
pages = {312--321},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Overcoming catastrophic forgetting with unlabeled data in the wild}},
url = {http://arxiv.org/abs/1903.12648},
volume = {2019-Octob},
year = {2019}
}
@article{Liu2020a,
abstract = {Object detection has improved significantly in recent years on multiple challenging benchmarks. However, most existing detectors are still domain-specific, where the models are trained and tested on a single domain. When adapting these detectors to new domains, they often suffer from catastrophic forgetting of previous knowledge. In this paper, we propose a continual object detector that can learn sequentially from different domains without forgetting. First, we explore learning the object detector continually in different scenarios across various domains and categories. Learning from the analysis, we propose attentive feature distillation leveraging both bottom-up and top-down attentions to mitigate forgetting. It takes advantage of attention to ignore the noisy background information and feature distillation to provide strong supervision. Finally, for the most challenging scenarios, we propose an adaptive exemplar sampling method to leverage exemplars from previous tasks for less forgetting effectively. The experimental results show the excellent performance of our proposed method in three different scenarios across seven different object detection datasets.},
archivePrefix = {arXiv},
arxivId = {2002.05347},
author = {Liu, Xialei and Yang, Hao and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
eprint = {2002.05347},
file = {::},
journal = {arXiv},
month = {feb},
title = {{Continual Universal Object Detection}},
url = {http://arxiv.org/abs/2002.05347},
year = {2020}
}
@inproceedings{Aljundi2016a,
abstract = {In this paper we introduce a model of lifelong learning, based on a Network of Experts. New tasks / experts are learned and added to the model sequentially, building on what was learned before. To ensure scalability of this process, data from previous tasks cannot be stored and hence is not available when learning a new task. A critical issue in such context, not addressed in the literature so far, relates to the decision of which expert to deploy at test time. We introduce a gating autoencoder that learns a representation for the task at hand, and is used at test time to automatically forward the test sample to the relevant expert. This has the added advantage of being memory efficient as only one expert network has to be loaded into memory at any given time. Further, the autoencoders inherently capture the relatedness of one task to another, based on which the most relevant prior model to be used for training a new expert with fine-tuning or learning-without-forgetting can be selected. We evaluate our system on image classification and video prediction problems.},
archivePrefix = {arXiv},
arxivId = {1611.06194},
author = {Aljundi, Rahaf and Chakravarty, Punarjay and Tuytelaars, Tinne},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eprint = {1611.06194},
keywords = {[vision]},
mendeley-tags = {[vision]},
title = {{Expert Gate: Lifelong Learning with a Network of Experts}},
url = {http://arxiv.org/abs/1611.06194},
year = {2017}
}
@inproceedings{Kading2017a,
abstract = {The revival of deep neural networks and the availability of ImageNet laid the foundation for recent success in highly complex recognition tasks. However, ImageNet does not cover all visual concepts of all possible application scenarios. Hence, application experts still record new data constantly and expect the data to be used upon its availability. In this paper, we follow this observation and apply the classical concept of fine-tuning deep neural networks to scenarios where data from known or completely new classes is continuously added. Besides a straightforward realization of continuous fine-tuning, we empirically analyze how computational burdens of training can be further reduced. Finally, we visualize how the network's attention maps evolve over time which allows for visually investigating what the network learned during continuous fine-tuning.},
author = {K{\"{a}}ding, Christoph and Rodner, Erik and Freytag, Alexander and Denzler, Joachim},
booktitle = {ACCV Workshop},
doi = {10.1007/978-3-319-54526-4_43},
isbn = {9783319545257},
issn = {16113349},
keywords = {[imagenet]},
mendeley-tags = {[imagenet]},
title = {{Fine-Tuning Deep Neural Networks in Continuous Learning Scenarios}},
url = {http://link.springer.com/10.1007/978-3-319-54526-4{\_}43},
year = {2016}
}
@inproceedings{amal2017a,
abstract = {This paper introduces a new lifelong learning solution where a single model is trained for a sequence of tasks. The main challenge that vision systems face in this context is catastrophic forgetting: as they tend to adapt to the most recently seen task, they lose performance on the tasks that were learned previously. Our method aims at preserving the knowledge of the previous tasks while learning a new one by using autoencoders. For each task, an under-complete autoencoder is learned, capturing the features that are crucial for its achievement. When a new task is presented to the system, we prevent the reconstructions of the features with these autoencoders from changing, which has the effect of preserving the information on which the previous tasks are mainly relying. At the same time, the features are given space to adjust to the most recent environment as only their projection into a low dimension submanifold is controlled. The proposed system is evaluated on image classification tasks and shows a reduction of forgetting over the state-ofthe-art.},
archivePrefix = {arXiv},
arxivId = {1704.01920},
author = {Rannen, Amal and Aljundi, Rahaf and Blaschko, Matthew B. and Tuytelaars, Tinne},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2017.148},
eprint = {1704.01920},
file = {::},
isbn = {9781538610329},
issn = {15505499},
keywords = {[imagenet],[vision]},
mendeley-tags = {[imagenet],[vision]},
month = {dec},
pages = {1329--1337},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Encoder Based Lifelong Learning}},
volume = {2017-Octob},
year = {2017}
}
@article{Liu2020a,
abstract = {Multi-Class Incremental Learning (MCIL) aims to learn new concepts by incrementally updating a model trained on previous concepts. However, there is an inherent trade-off to effectively learning new concepts without catastrophic forgetting of previous ones. To alleviate this issue, it has been proposed to keep around a few examples of the previous concepts but the effectiveness of this approach heavily depends on the representativeness of these examples. This paper proposes a novel and automatic framework we call mnemonics, where we parameterize exemplars and make them optimizable in an end-to-end manner. We train the framework through bilevel optimizations, i.e., model-level and exemplar-level. We conduct extensive experiments on three MCIL benchmarks, CIFAR-100, ImageNet-Subset and ImageNet, and show that using mnemonics exemplars can surpass the state-of-the-art by a large margin. Interestingly and quite intriguingly, the mnemonics exemplars tend to be on the boundaries between different classes.},
archivePrefix = {arXiv},
arxivId = {2002.10211},
author = {Liu, Yaoyao and Liu, An-An and Su, Yuting and Schiele, Bernt and Sun, Qianru},
eprint = {2002.10211},
journal = {arXiv},
keywords = {[cifar],[imagenet]},
mendeley-tags = {[cifar],[imagenet]},
month = {feb},
title = {{Mnemonics Training: Multi-Class Incremental Learning without Forgetting}},
url = {http://arxiv.org/abs/2002.10211},
year = {2020}
}
@inproceedings{Nguyen2019b,
abstract = {Given a labeled dataset that contains a rare (or minority) class of of-interest instances, as well as a large class of instances that are not of interest, how can we learn to recognize future of-interest instances over a continuous stream? We introduce RaRecognize, which (i) estimates a general decision boundary between the rare and the majority class, (ii) learns to recognize individual rare subclasses that exist within the training data, as well as (iii) flags instances from previously unseen rare subclasses as newly emerging. The learner in (i) is general in the sense that by construction it is dissimilar to the specialized learners in (ii), thus distinguishes minority from the majority without overly tuning to what is seen in the training data. Thanks to this generality, RaRecognize ignores all future instances that it labels as majority and recognizes the recurrent as well as emerging rare subclasses only. This saves effort at test time as well as ensures that the model size grows moderately over time as it only maintains specialized minority learners. Through extensive experiments, we show that RaRecognize outperforms state-of-the art baselines on three real-world datasets that contain corporate-risk and disaster documents as rare classes.},
archivePrefix = {arXiv},
arxivId = {1906.12218},
author = {Nguyen, Hung and Wang, Xuejian and Akoglu, Leman},
booktitle = {ECML},
eprint = {1906.12218},
keywords = {[nlp]},
mendeley-tags = {[nlp]},
month = {jun},
title = {{Continual Rare-Class Recognition with Emerging Novel Subclasses}},
url = {http://arxiv.org/abs/1906.12218},
year = {2019}
}
@article{Diethe2019a,
abstract = {This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.},
archivePrefix = {arXiv},
arxivId = {1903.05202},
author = {Diethe, Tom and Borchert, Tom and Thereska, Eno and Balle, Borja and Lawrence, Neil},
eprint = {1903.05202},
journal = {arXiv},
month = {mar},
title = {{Continual Learning in Practice}},
url = {http://arxiv.org/abs/1903.05202},
year = {2019}
}
@article{Kuzina2019a,
abstract = {Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences. From practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage. We address this problem by introducing a conceptually simple and scalable end-to-end approach of incorporating past knowledge by learning prior directly from the data. We consider scalable boosting-like approximation for intractable theoretical optimal prior. We provide empirical studies on two commonly used benchmarks, namely MNIST and Fashion MNIST on disjoint sequential image generation tasks. For each dataset proposed method delivers the best results or comparable to SOTA, avoiding catastrophic forgetting in a fully automatic way.},
archivePrefix = {arXiv},
arxivId = {1908.11853},
author = {Kuzina, Anna and Egorov, Evgenii and Burnaev, Evgeny},
eprint = {1908.11853},
journal = {arXiv},
keywords = {[bayes],[fashion],[mnist]},
mendeley-tags = {[bayes],[fashion],[mnist]},
title = {{BooVAE: A scalable framework for continual VAE learning under boosting approach}},
url = {http://arxiv.org/abs/1908.11853},
year = {2019}
}
@inproceedings{Mallya2018a,
abstract = {This work presents a method for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned tasks. By building upon ideas from network quantization and pruning, we learn binary masks that “piggyback” on an existing network, or are applied to unmodified weights of that network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask individual weights allows for the learning of a large number of filters. We show performance comparable to dedicated fine-tuned networks for a variety of classification tasks, including those with large domain shifts from the initial task (ImageNet), and a variety of network architectures. Our performance is agnostic to task ordering and we do not suffer from catastrophic forgetting or competition between tasks.},
archivePrefix = {arXiv},
arxivId = {1801.06519},
author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
booktitle = {ECCV},
doi = {10.1007/978-3-030-01225-0_5},
eprint = {1801.06519},
file = {::},
isbn = {9783030012243},
issn = {16113349},
keywords = {Binary networks,Incremental learning,[imagenet]},
mendeley-tags = {[imagenet]},
month = {sep},
pages = {72--88},
publisher = {Springer Verlag},
title = {{Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights}},
url = {https://doi.org/10.1007/978-3-030-01225-0{\_}5},
year = {2018}
}
@article{Cheung2019a,
abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
archivePrefix = {arXiv},
arxivId = {1902.05522},
author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
eprint = {1902.05522},
file = {::},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {feb},
title = {{Superposition of many models into one}},
url = {http://arxiv.org/abs/1902.05522},
year = {2019}
}
@inproceedings{Rajasegaran2019a,
abstract = {Incremental lifelong learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.},
author = {Rajasegaran, Jathushan and Hayat, Munawar and Fahad, Salman Khan and Khan, Shahbaz and Shao, Ling},
booktitle = {NeurIPS},
file = {::},
keywords = {[cifar],[imagenet],[mnist]},
mendeley-tags = {[cifar],[imagenet],[mnist]},
pages = {12669--12679},
title = {{Random Path Selection for Incremental Learning}},
url = {http://papers.nips.cc/paper/9429-random-path-selection-for-continual-learning.pdf},
year = {2019}
}
@article{Mancini2019a,
abstract = {Visual recognition algorithms are required today to exhibit adaptive abilities. Given a deep model trained on a specific, given task, it would be highly desirable to be able to adapt incrementally to new tasks, preserving scalability as the number of new tasks increases, while at the same time avoiding catastrophic forgetting issues. Recent work has shown that masking the internal weights of a given original conv-net through learned binary variables is a promising strategy. We build upon this intuition and take into account more elaborated affine transformations of the convolutional weights that include learned binary masks. We show that with our generalization it is possible to achieve significantly higher levels of adaptation to new tasks, enabling the approach to compete with fine tuning strategies by requiring slightly more than 1 bit per network parameter per additional task. Experiments on two popular benchmarks showcase the power of our approach, that achieves the new state of the art on the Visual Decathlon Challenge.},
archivePrefix = {arXiv},
arxivId = {1805.11119},
author = {Mancini, Massimiliano and Ricci, Elisa and Caputo, Barbara and Bul{\`{o}}, Samuel Rota},
doi = {10.1007/978-3-030-11012-3_14},
eprint = {1805.11119},
isbn = {9783030110116},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Incremental learning,Multi-task learning,[sparsity],[vision]},
mendeley-tags = {[sparsity],[vision]},
month = {may},
pages = {180--189},
title = {{Adding New Tasks to a Single Network with Weight Transformations using Binary Masks}},
url = {http://arxiv.org/abs/1805.11119},
volume = {11130 LNCS},
year = {2018}
}
@article{Teng2019a,
abstract = {In order to mimic the human ability of continual acquisition and transfer of knowledge across various tasks, a learning system needs the capability for continual learning, effectively utilizing the previously acquired skills. As such, the key challenge is to transfer and generalize the knowledge learned from one task to other tasks, avoiding forgetting and interference of previous knowledge and improving the overall performance. In this paper, within the continual learning paradigm, we introduce a method that effectively forgets the less useful data samples continuously and allows beneficial information to be kept for training of the subsequent tasks, in an online manner. The method uses statistical leverage score information to measure the importance of the data samples in every task and adopts frequent directions approach to enable a continual or life-long learning property. This effectively maintains a constant training size across all tasks. We first provide mathematical intuition for the method and then demonstrate its effectiveness in avoiding catastrophic forgetting and computational efficiency on continual learning of classification tasks when compared with the existing state-of-the-art techniques.},
archivePrefix = {arXiv},
arxivId = {1908.00355},
author = {Teng, Dan and Dasgupta, Sakyasingha},
eprint = {1908.00355},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {aug},
title = {{Continual Learning via Online Leverage Score Sampling}},
url = {http://arxiv.org/abs/1908.00355},
year = {2019}
}
@inproceedings{Swaroop2018a,
abstract = {In the continual learning setting, tasks are encountered sequentially. The goal is to learn whilst i) avoiding catastrophic forgetting, ii) efficiently using model capacity, and iii) employing forward and backward transfer. In this paper, we explore how the Variational Continual Learning (VCL) framework achieves these desiderata on two benchmarks in continual learning: split MNIST and permuted MNIST. We first report significantly improved results on what was already a competitive approach, achieved by establishing a new best practice approach to mean-field variational Bayesian neural networks. We then look at the solutions in detail. This allows us to obtain an understanding of why VCL performs as it does, and we compare the solution to what an 'ideal' continual learning solution might be.},
archivePrefix = {arXiv},
arxivId = {arXiv:1905.02099v1},
author = {Swaroop, Siddharth and Nguyen, Cuong V and Bui, Thang D and Turner, Richard E},
booktitle = {Continual Learning Workshop NeurIPS},
eprint = {arXiv:1905.02099v1},
keywords = {[bayes],[mnist]},
mendeley-tags = {[bayes],[mnist]},
pages = {1--17},
title = {{Improving and Understanding Variational Continual Learning}},
url = {https://marcpickett.com/cl2018/CL-2018{\%}7B{\%}5C{\_}{\%}7Dpaper{\%}7B{\%}5C{\_}{\%}7D56.pdf},
year = {2018}
}
@article{Ye2019a,
abstract = {With the development of deep convolutional neural networks in recent years, the network structure has become more and more complicated and varied, and there are very good results in pattern recognition, image classification, scene classification, and target tracking. This end-to-end learning model relies on the initial large dataset. However, many data are gradually obtained in practical situations, which contradict the deep learning of one-time batch learning. There is an urgent need for an incremental learning approach that can continuously learn new knowledge from new data while retaining what has already been learned. This paper proposes an incremental learning algorithm based on convolutional neural network and support vector data description. CNN and AM-Softmax loss function are used to represent and continuously learn image features. Support vector data description is used to construct multiple hyperspheres for new and old classes of images. Class-incremental learning is achieved by the increment of hyperspheres. The experimental results show that the incremental learning method proposed in this paper can effectively extract the latent features of the image and adapt it to the learning situation of the class-increment. The recognition accuracy is close to batch learning.},
author = {Ye, Xin and Zhu, Qiuyu},
doi = {10.1109/ACCESS.2019.2904614},
file = {::},
issn = {21693536},
journal = {IEEE Access},
keywords = {One-class classifier,[cifar],[mnist],feature extraction,incremental learning,loss function},
mendeley-tags = {[cifar],[mnist]},
pages = {42024--42031},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Class-Incremental Learning Based on Feature Extraction of CNN with Optimized Softmax and One-Class Classifiers}},
volume = {7},
year = {2019}
}
@article{Saha2020a,
abstract = {Humans are skilled at learning adaptively and efficiently throughout their lives, but learning tasks incrementally causes artificial neural networks to overwrite relevant information learned about older tasks, resulting in 'Catastrophic Forgetting'. Efforts to overcome this phenomenon suffer from poor utilization of resources in many ways, such as through the need to save older data or parametric importance scores, or to grow the network architecture. We propose an algorithm that enables a network to learn continually and efficiently by partitioning the representational space into a Core space, that contains the condensed information from previously learned tasks, and a Residual space, which is akin to a scratch space for learning the current task. The information in the Residual space is then compressed using Principal Component Analysis and added to the Core space, freeing up parameters for the next task. We evaluate our algorithm on P-MNIST, CIFAR-10 and CIFAR-100 datasets. We achieve comparable accuracy to state-of-the-art methods while overcoming the problem of catastrophic forgetting completely. Additionally, we get up to 4.5x improvement in energy efficiency during inference due to the structured nature of the resulting architecture.},
archivePrefix = {arXiv},
arxivId = {2001.08650},
author = {Saha, Gobinda and Garg, Isha and Ankit, Aayush and Roy, Kaushik},
eprint = {2001.08650},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
title = {{Structured Compression and Sharing of Representational Space for Continual Learning}},
url = {http://arxiv.org/abs/2001.08650},
year = {2020}
}
@article{Li2020a,
abstract = {Conventional deep learning models have limited capacity in learning multiple tasks sequentially. The issue of forgetting the previously learned tasks in continual learning is known as catastrophic forgetting or interference. When the input data or the goal of learning change, a continual model will learn and adapt to the new status. However, the model will not remember or recognise any revisits to the previous states. This causes performance reduction and re-training curves in dealing with periodic or irregularly reoccurring changes in the data or goals. The changes in goals or data are referred to as new tasks in a continual learning model. Most of the continual learning methods have a task-known setup in which the task identities are known in advance to the learning model. We propose Task Conditional Neural Networks (TCNN) that does not require to known the reoccurring tasks in advance. We evaluate our model on standard datasets using MNIST and CIFAR10, and also a real-world dataset that we have collected in a remote healthcare monitoring study (i.e. TIHM dataset). The proposed model outperforms the state-of-the-art solutions in continual learning and adapting to new tasks that are not defined in advance.},
archivePrefix = {arXiv},
arxivId = {2005.05080},
author = {Li, Honglin and Barnaghi, Payam and Enshaeifar, Shirin and Ganz, Frieder},
eprint = {2005.05080},
file = {::},
journal = {arXiv},
keywords = {[cifar],[mnist]},
mendeley-tags = {[cifar],[mnist]},
month = {may},
title = {{Continual Learning Using Task Conditional Neural Networks}},
url = {http://arxiv.org/abs/2005.05080},
year = {2020}
}
